<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.8.25">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Basic-Word Paradox: Is Basic Vocabulary Harder to Define?</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="https://cdn.jsdelivr.net/npm/jquery@3.5.1/dist/jquery.min.js" integrity="sha384-ZvpUoO/+PpLXR1lu4jmpXWu80pZlYUAfxl5NsBMWOEPSjUn/6Z/hRTt8+pR6L4N2" crossorigin="anonymous"></script><script src="basic_words_project_files/libs/clipboard/clipboard.min.js"></script>
<script src="basic_words_project_files/libs/quarto-html/quarto.js" type="module"></script>
<script src="basic_words_project_files/libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="basic_words_project_files/libs/quarto-html/axe/axe-check.js" type="module"></script>
<script src="basic_words_project_files/libs/quarto-html/popper.min.js"></script>
<script src="basic_words_project_files/libs/quarto-html/tippy.umd.min.js"></script>
<script src="basic_words_project_files/libs/quarto-html/anchor.min.js"></script>
<link href="basic_words_project_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="basic_words_project_files/libs/quarto-html/quarto-syntax-highlighting-7b89279ff1a6dce999919e0e67d4d9ec.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="basic_words_project_files/libs/bootstrap/bootstrap.min.js"></script>
<link href="basic_words_project_files/libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="basic_words_project_files/libs/bootstrap/bootstrap-d6a003b94517c951b2d65075d42fb01b.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script src="https://cdn.jsdelivr.net/npm/requirejs@2.3.6/require.min.js" integrity="sha384-c9c+LnTbwQ3aujuU7ULEPVvgLs+Fn6fJUvIGTsuu1ZcCf11fiEubah0ttpca4ntM sha384-6V1/AdqZRWk1KAlWbKBlGhN7VG4iE/yAZcO6NZPMF8od0vukrvr0tg4qY6NSrItx" crossorigin="anonymous"></script>

<script type="application/javascript">define('jquery', [],function() {return window.jQuery;})</script>


</head>

<body class="quarto-light">

<div id="quarto-content" class="page-columns page-rows-contents page-layout-article">
<div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
  <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#research-question" id="toc-research-question" class="nav-link active" data-scroll-target="#research-question"><span class="header-section-number">1</span> Research question</a></li>
  <li><a href="#theoretical-overview" id="toc-theoretical-overview" class="nav-link" data-scroll-target="#theoretical-overview"><span class="header-section-number">2</span> Theoretical overview</a></li>
  <li><a href="#methodology" id="toc-methodology" class="nav-link" data-scroll-target="#methodology"><span class="header-section-number">3</span> Methodology</a>
  <ul>
  <li><a href="#how-to-quantify-basicness-and-ease-of-definition" id="toc-how-to-quantify-basicness-and-ease-of-definition" class="nav-link" data-scroll-target="#how-to-quantify-basicness-and-ease-of-definition"><span class="header-section-number">3.1</span> How to quantify “basicness” and “ease of definition”?</a></li>
  <li><a href="#lexicographical-sources-for-obtaining-definitions" id="toc-lexicographical-sources-for-obtaining-definitions" class="nav-link" data-scroll-target="#lexicographical-sources-for-obtaining-definitions"><span class="header-section-number">3.2</span> Lexicographical sources for obtaining definitions</a></li>
  <li><a href="#preprocessing-and-tokenization" id="toc-preprocessing-and-tokenization" class="nav-link" data-scroll-target="#preprocessing-and-tokenization"><span class="header-section-number">3.3</span> Preprocessing and tokenization</a></li>
  <li><a href="#statistical-testing" id="toc-statistical-testing" class="nav-link" data-scroll-target="#statistical-testing"><span class="header-section-number">3.4</span> Statistical Testing</a></li>
  </ul></li>
  <li><a href="#preparation-collapse-this-section-for-better-overview-of-the-notebook-text" id="toc-preparation-collapse-this-section-for-better-overview-of-the-notebook-text" class="nav-link" data-scroll-target="#preparation-collapse-this-section-for-better-overview-of-the-notebook-text"><span class="header-section-number">4</span> Preparation (collapse this section for better overview of the notebook text)</a></li>
  <li><a href="#obtaining-and-analyzing-the-results" id="toc-obtaining-and-analyzing-the-results" class="nav-link" data-scroll-target="#obtaining-and-analyzing-the-results"><span class="header-section-number">5</span> Obtaining and analyzing the results</a>
  <ul>
  <li><a href="#initial-approach-wordnet-100-words-for-each-group-all-parts-of-speech" id="toc-initial-approach-wordnet-100-words-for-each-group-all-parts-of-speech" class="nav-link" data-scroll-target="#initial-approach-wordnet-100-words-for-each-group-all-parts-of-speech"><span class="header-section-number">5.1</span> Initial approach: WordNet, 100 words for each group, all parts of speech</a>
  <ul>
  <li><a href="#web-based-corpus" id="toc-web-based-corpus" class="nav-link" data-scroll-target="#web-based-corpus"><span class="header-section-number">5.1.1</span> Web-based corpus</a></li>
  <li><a href="#subtitles-based-corpus" id="toc-subtitles-based-corpus" class="nav-link" data-scroll-target="#subtitles-based-corpus"><span class="header-section-number">5.1.2</span> Subtitles-based corpus</a></li>
  <li><a href="#book-based-corpus" id="toc-book-based-corpus" class="nav-link" data-scroll-target="#book-based-corpus"><span class="header-section-number">5.1.3</span> Book-based corpus</a></li>
  <li><a href="#sanity-check-are-we-getting-plausible-basic-and-non-basic-words" id="toc-sanity-check-are-we-getting-plausible-basic-and-non-basic-words" class="nav-link" data-scroll-target="#sanity-check-are-we-getting-plausible-basic-and-non-basic-words"><span class="header-section-number">5.1.4</span> Sanity check: Are we getting plausible “basic” and “non-basic” words?</a></li>
  <li><a href="#analysis-token-frequencies-in-definitions" id="toc-analysis-token-frequencies-in-definitions" class="nav-link" data-scroll-target="#analysis-token-frequencies-in-definitions"><span class="header-section-number">5.1.5</span> Analysis: token frequencies in definitions</a></li>
  <li><a href="#analysis-length-of-definitions" id="toc-analysis-length-of-definitions" class="nav-link" data-scroll-target="#analysis-length-of-definitions"><span class="header-section-number">5.1.6</span> Analysis: length of definitions</a></li>
  <li><a href="#why-isnt-the-hypothesis-supported" id="toc-why-isnt-the-hypothesis-supported" class="nav-link" data-scroll-target="#why-isnt-the-hypothesis-supported"><span class="header-section-number">5.1.7</span> Why isn’t the hypothesis supported?</a></li>
  </ul></li>
  <li><a href="#alternarive-approach-nounsverbsadjectivesadverbs-analyzed-separately" id="toc-alternarive-approach-nounsverbsadjectivesadverbs-analyzed-separately" class="nav-link" data-scroll-target="#alternarive-approach-nounsverbsadjectivesadverbs-analyzed-separately"><span class="header-section-number">5.2</span> Alternarive approach: nouns/verbs/adjectives/adverbs analyzed separately</a>
  <ul>
  <li><a href="#analysis-token-frequencies-in-definitions-1" id="toc-analysis-token-frequencies-in-definitions-1" class="nav-link" data-scroll-target="#analysis-token-frequencies-in-definitions-1"><span class="header-section-number">5.2.1</span> Analysis: token frequencies in definitions</a></li>
  <li><a href="#analysis-length-of-definitions-1" id="toc-analysis-length-of-definitions-1" class="nav-link" data-scroll-target="#analysis-length-of-definitions-1"><span class="header-section-number">5.2.2</span> Analysis: length of definitions</a></li>
  </ul></li>
  <li><a href="#alternarive-approach-wiktionary-definitions" id="toc-alternarive-approach-wiktionary-definitions" class="nav-link" data-scroll-target="#alternarive-approach-wiktionary-definitions"><span class="header-section-number">5.3</span> Alternarive approach: Wiktionary definitions</a>
  <ul>
  <li><a href="#web-based-corpus-1" id="toc-web-based-corpus-1" class="nav-link" data-scroll-target="#web-based-corpus-1"><span class="header-section-number">5.3.1</span> Web-based corpus</a></li>
  <li><a href="#subtitles-based-corpus-1" id="toc-subtitles-based-corpus-1" class="nav-link" data-scroll-target="#subtitles-based-corpus-1"><span class="header-section-number">5.3.2</span> Subtitles-based corpus</a></li>
  <li><a href="#book-based-corpus-1" id="toc-book-based-corpus-1" class="nav-link" data-scroll-target="#book-based-corpus-1"><span class="header-section-number">5.3.3</span> Book-based corpus</a></li>
  <li><a href="#analysis-token-frequencies-in-definitions-2" id="toc-analysis-token-frequencies-in-definitions-2" class="nav-link" data-scroll-target="#analysis-token-frequencies-in-definitions-2"><span class="header-section-number">5.3.4</span> Analysis: token frequencies in definitions</a></li>
  <li><a href="#analysis-length-of-definitions-2" id="toc-analysis-length-of-definitions-2" class="nav-link" data-scroll-target="#analysis-length-of-definitions-2"><span class="header-section-number">5.3.5</span> Analysis: length of definitions</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#conclusion-and-future-work" id="toc-conclusion-and-future-work" class="nav-link" data-scroll-target="#conclusion-and-future-work"><span class="header-section-number">6</span> Conclusion and future work</a></li>
  <li><a href="#references" id="toc-references" class="nav-link" data-scroll-target="#references"><span class="header-section-number">7</span> References</a></li>
  </ul>
</nav>
</div>
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Basic-Word Paradox: Is Basic Vocabulary Harder to Define?</h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<p><em>NB: it is very important to install ipywidgets; without that, the notebook won’t render properly.</em></p>
<p>Dictionaries define words using other words, often simpler or more frequent ones. Yet some highly frequent, conceptually fundamental words – like water or house – may be defined using rarer or more specialized vocabulary. This project examines the idea of a “Basic-Word Paradox”: that words most central to human experience might paradoxically require more complex dictionary definitions. In this project, I aim to explore this paradox by analyzing dictionary and lexical database entries of basic and non-basic words in terms of “basicness” – with a frequency-based definition.</p>
<section id="research-question" class="level2" data-number="1">
<h2 data-number="1" class="anchored" data-anchor-id="research-question"><span class="header-section-number">1</span> Research question</h2>
<p><strong>Q: Do dictionary definitions of “basic” words differ systematically from those of “non-basic” words in terms of the frequency and length of the words used to define them?</strong></p>
<p>H: I hypothesize that definitions of “basic” concepts should be longer and at the same time contain less frequent (i.e., less basic) words, reflecting their conceptual breadth and the limits of simple paraphrase.</p>
</section>
<section id="theoretical-overview" class="level2" data-number="2">
<h2 data-number="2" class="anchored" data-anchor-id="theoretical-overview"><span class="header-section-number">2</span> Theoretical overview</h2>
<p>Early attempts to identify conceptually fundamental vocabulary are found in historical-comparative linguistics, in particular, the Swadesh list of “core” terms proposed for lexicostatistical comparison across languages (Swadesh, 1952). While the method aimed to date language divergence, it also revealed that certain words are resistant to replacement, tentatively encoding concepts central to human communication and cognition.</p>
<p>In lexicography, the notion of lexical basicness is found in the principle of the defining vocabulary, used in learner-oriented dictionaries such as the Longman Dictionary of Contemporary English and the Oxford Learner’s Dictionaries. Under this policy, entries are paraphrased using a limited set of high-frequency, semantically transparent words, improving accessibility for readers and learners who might otherwise struggle with complex or specialized metalanguage (Kamiński, 2021). This practice assumes a limited “definition” lexicon, where some words function as primitives for defining others.</p>
<p>From a cognitive perspective, the idea of basicness aligns with the concept of basic-level categories – the level of abstraction at which humans most readily identify and name objects (e.g., chair rather than furniture or rocking chair) (Rosch et al., 1976). Basic-level terms tend to be more frequent, imageable, and semantically rich, reflecting their privileged status in mental representation.</p>
<p>Recent computational and psycholinguistic research extends this by quantifying lexical “fundamentality” through measurable properties, such as word frequency, concreteness, imageability, and age of acquisition (e.g., Brysbaert et al., 2014; Mandera et al., 2017). Corpus-based approaches show that high-frequency words occupy central positions in semantic networks (Steyvers and Tenenbaum, 2005).</p>
<p>Building on the earlier research, this study operationalizes basicness through frequency-based word lists and examines how it manifests in dictionary definitions by comparing the length- and frequency-based ease of definitions for basic versus non-basic words across multiple corpora.</p>
</section>
<section id="methodology" class="level2" data-number="3">
<h2 data-number="3" class="anchored" data-anchor-id="methodology"><span class="header-section-number">3</span> Methodology</h2>
<section id="how-to-quantify-basicness-and-ease-of-definition" class="level3" data-number="3.1">
<h3 data-number="3.1" class="anchored" data-anchor-id="how-to-quantify-basicness-and-ease-of-definition"><span class="header-section-number">3.1</span> How to quantify “basicness” and “ease of definition”?</h3>
<p>As I have demonstrated in the previous section, here is no one common definition of vocabulary “basicness”. I opt to follow the frequency-based definition and treat more frequently used words as more “basic” and less frequently used words as less “basic”. As a medium for the ease of definition, I consider 1) the number of words used in a definition; 2) mean and median frequency of these words. Words that are easier to define will have shorter definitions and the words used in their definitions will be more basic, i.e.&nbsp;more frequent.</p>
<p>To define basic and non-basic words as well as basic and non-basic definitions, I need a list of word frequencies. I decide not to limit the analysis to one source of information and take a look at frequencies obtained from three different corpora: <a href="https://www.kaggle.com/datasets/rtatman/english-word-frequency/data">Google Web Trillion Word Corpus</a>, <a href="https://github.com/orgtre/top-open-subtitles-sentences">OpenSubtitles2018 corpus</a>, <a href="https://github.com/possibly-wrong/word-frequency">2020-02-17 Google Books Ngrams dataset</a> (100 datatpoints for each corpus, each type).</p>
</section>
<section id="lexicographical-sources-for-obtaining-definitions" class="level3" data-number="3.2">
<h3 data-number="3.2" class="anchored" data-anchor-id="lexicographical-sources-for-obtaining-definitions"><span class="header-section-number">3.2</span> Lexicographical sources for obtaining definitions</h3>
<p><a href="https://wordnet.princeton.edu">WordNet definitions</a> are obtained via NLTK WordNet subpackage. While this subpackage is no longer developed as a standalone component, it is stable, fully functional, integrated into NLTK, and widely used in corpus-based linguistic research.</p>
<p><a href="https://www.wiktionary.org">Wiktionary definitions</a> are retrieved by parsing the corresponding page markup from en.wiktionary.org. Specifically, I extract the first English definition section for each lemma using regular expressions that match ==English== headers and definition lines (e.g., those starting with #).</p>
</section>
<section id="preprocessing-and-tokenization" class="level3" data-number="3.3">
<h3 data-number="3.3" class="anchored" data-anchor-id="preprocessing-and-tokenization"><span class="header-section-number">3.3</span> Preprocessing and tokenization</h3>
<p>For each corpus, the words were lemmatized using NLTK’s WordNetLemmatizer and the frequencies for different wordforms (e.g., do, did, and does) of the same lemma were aggregated.</p>
<p>Word frequencies were normalized as smoothed log-probabilities: each lemma’s count was divided by the total corpus count (with +0.5 smoothing) and converted to base-10 logarithms. This reduces the skew (otherwise, a few dominating high-frequency tokens would affect the result disproportionately) and ensures comparability across corpora.</p>
<p>All definitions were lowercased, tokenized using NLTK’s word_tokenize, and stripped of punctuation and stopwords. Numerical tokens were removed. Mean and median token frequencies were computed after aligning each definition word to its corpus frequency list (with aggregated frequencies for lemmas).</p>
</section>
<section id="statistical-testing" class="level3" data-number="3.4">
<h3 data-number="3.4" class="anchored" data-anchor-id="statistical-testing"><span class="header-section-number">3.4</span> Statistical Testing</h3>
<p>For each corpus, I compared “basic” vs “non-basic” sets using Welch’s two-sample t-test (unequal variances) and Mann–Whitney U as a robustness check. Effect sizes were quantified with Cohen’s d, and 95% confidence intervals were computed for mean differences. As the significance threshold, I considered the standard p&lt;.05.</p>
</section>
</section>
<section id="preparation-collapse-this-section-for-better-overview-of-the-notebook-text" class="level2" data-number="4">
<h2 data-number="4" class="anchored" data-anchor-id="preparation-collapse-this-section-for-better-overview-of-the-notebook-text"><span class="header-section-number">4</span> Preparation (collapse this section for better overview of the notebook text)</h2>
<p>In the following section, I prepare everything necessary for the data analysis – imports, downloads, global variables and function definitions. I tried to document each function (see the docstring if there are any questions).</p>
<div id="cb368cda" class="cell" data-execution_count="264">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> re</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> os</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> nltk</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> sys</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> io</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> IPython.display <span class="im">import</span> display, HTML</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> nltk <span class="im">import</span> word_tokenize, pos_tag</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> nltk.corpus <span class="im">import</span> stopwords</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> nltk.corpus <span class="im">import</span> wordnet <span class="im">as</span> wn</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> nltk.stem <span class="im">import</span> WordNetLemmatizer</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> tqdm <span class="im">import</span> tqdm </span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> re</span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy <span class="im">import</span> stats</span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> matplotlib_venn <span class="im">import</span> venn3</span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> re</span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> html</span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> requests</span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a>_WIKI_API <span class="op">=</span> <span class="st">"https://en.wiktionary.org/w/api.php"</span></span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a>LMT <span class="op">=</span> WordNetLemmatizer()</span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a>nltk.download(<span class="st">"punkt"</span>, quiet<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a>nltk.download(<span class="st">"averaged_perceptron_tagger_eng"</span>, quiet<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a>nltk.download(<span class="st">"wordnet"</span>, quiet<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb1-28"><a href="#cb1-28" aria-hidden="true" tabindex="-1"></a>nltk.download(<span class="st">"omw-1.4"</span>, quiet<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb1-29"><a href="#cb1-29" aria-hidden="true" tabindex="-1"></a>nltk.download(<span class="st">"stopwords"</span>, quiet<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb1-30"><a href="#cb1-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-31"><a href="#cb1-31" aria-hidden="true" tabindex="-1"></a><span class="co"># manually defined stopwords</span></span>
<span id="cb1-32"><a href="#cb1-32" aria-hidden="true" tabindex="-1"></a>CUSTOM_STOP_FROM_LIST <span class="op">=</span> {</span>
<span id="cb1-33"><a href="#cb1-33" aria-hidden="true" tabindex="-1"></a>    <span class="co"># abbrev / shorthands</span></span>
<span id="cb1-34"><a href="#cb1-34" aria-hidden="true" tabindex="-1"></a>    <span class="st">"u"</span>,<span class="st">"de"</span>,<span class="st">"al"</span>,<span class="st">"co"</span>,<span class="st">"ed"</span>,<span class="st">"fig"</span>,<span class="st">"la"</span>,<span class="st">"wa"</span>,<span class="st">"ha"</span>,<span class="st">"xx"</span></span>
<span id="cb1-35"><a href="#cb1-35" aria-hidden="true" tabindex="-1"></a>    <span class="st">"xxiv"</span>, <span class="st">"lxx"</span>, <span class="st">"xxvii"</span>, <span class="st">"abcs"</span>, </span>
<span id="cb1-36"><a href="#cb1-36" aria-hidden="true" tabindex="-1"></a>    <span class="st">"romans"</span>, <span class="st">"teens"</span>, <span class="st">"fifties"</span>, <span class="st">"sevens"</span>, </span>
<span id="cb1-37"><a href="#cb1-37" aria-hidden="true" tabindex="-1"></a>    <span class="st">"thanks"</span>, <span class="st">"eats"</span>, <span class="st">"pursued"</span></span>
<span id="cb1-38"><a href="#cb1-38" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb1-39"><a href="#cb1-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-40"><a href="#cb1-40" aria-hidden="true" tabindex="-1"></a><span class="co"># stopwords for definitions</span></span>
<span id="cb1-41"><a href="#cb1-41" aria-hidden="true" tabindex="-1"></a>STOP <span class="op">=</span> <span class="bu">set</span>(stopwords.words(<span class="st">"english"</span>))</span>
<span id="cb1-42"><a href="#cb1-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-43"><a href="#cb1-43" aria-hidden="true" tabindex="-1"></a><span class="co"># custom stopwords for basic / non-basic wordlist, includes some words that I added manually (CUSTOM_STOP_FROM_LIST)</span></span>
<span id="cb1-44"><a href="#cb1-44" aria-hidden="true" tabindex="-1"></a>STOPWORDS <span class="op">=</span> STOP <span class="op">|</span> CUSTOM_STOP_FROM_LIST</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<div id="d345aeb3" class="cell" data-execution_count="282">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> has_pos(word: <span class="bu">str</span>, pos_tag) <span class="op">-&gt;</span> <span class="bu">bool</span>:</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a><span class="co">    True if the word only has at least one {POS} senses.</span></span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="bu">len</span>(wn.synsets(word, pos<span class="op">=</span>pos_tag)) <span class="op">&gt;</span> <span class="dv">0</span></span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> commonly_POS(word: <span class="bu">str</span>, pos<span class="op">=</span>wn.NOUN) <span class="op">-&gt;</span> <span class="bu">bool</span>:</span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a><span class="co">    True if the most-used SemCor sense is a {POS}.</span></span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a><span class="co">    If SemCor has zero counts for all senses (for rare words), optionally fall back to the old strict rule:</span></span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a><span class="co">      - has at least one {POS} sense</span></span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a><span class="co">      - and has NO senses other than {POS}</span></span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-16"><a href="#cb2-16" aria-hidden="true" tabindex="-1"></a>    w <span class="op">=</span> <span class="bu">str</span>(word).strip().lower()</span>
<span id="cb2-17"><a href="#cb2-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-18"><a href="#cb2-18" aria-hidden="true" tabindex="-1"></a>    lemma_objs <span class="op">=</span> wn.lemmas(w, pos<span class="op">=</span>wn.NOUN) <span class="op">+</span> wn.lemmas(w, pos<span class="op">=</span>wn.VERB) <span class="op">+</span> wn.lemmas(w, pos<span class="op">=</span>wn.ADJ) <span class="op">+</span> wn.lemmas(w, pos<span class="op">=</span>wn.ADV)</span>
<span id="cb2-19"><a href="#cb2-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-20"><a href="#cb2-20" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> <span class="kw">not</span> lemma_objs:</span>
<span id="cb2-21"><a href="#cb2-21" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">False</span></span>
<span id="cb2-22"><a href="#cb2-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-23"><a href="#cb2-23" aria-hidden="true" tabindex="-1"></a>    highest_count <span class="op">=</span> <span class="bu">max</span>(lemma_objs, key<span class="op">=</span><span class="kw">lambda</span> L: L.count() <span class="kw">or</span> <span class="dv">0</span>)</span>
<span id="cb2-24"><a href="#cb2-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-25"><a href="#cb2-25" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> highest_count:</span>
<span id="cb2-26"><a href="#cb2-26" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> highest_count.synset().pos() <span class="op">==</span> pos</span>
<span id="cb2-27"><a href="#cb2-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-28"><a href="#cb2-28" aria-hidden="true" tabindex="-1"></a>    <span class="co"># if SemCor has no counts, use pos-only rule</span></span>
<span id="cb2-29"><a href="#cb2-29" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb2-30"><a href="#cb2-30" aria-hidden="true" tabindex="-1"></a>        others <span class="op">=</span> {wn.VERB, wn.ADJ, wn.ADV, wn.NOUN} <span class="op">-</span> pos</span>
<span id="cb2-31"><a href="#cb2-31" aria-hidden="true" tabindex="-1"></a>        has_req <span class="op">=</span> <span class="bu">len</span>(wn.synsets(w, pos<span class="op">=</span>pos)) <span class="op">&gt;</span> <span class="dv">0</span></span>
<span id="cb2-32"><a href="#cb2-32" aria-hidden="true" tabindex="-1"></a>        has_other <span class="op">=</span> <span class="va">False</span></span>
<span id="cb2-33"><a href="#cb2-33" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> other_pos <span class="kw">in</span> others:</span>
<span id="cb2-34"><a href="#cb2-34" aria-hidden="true" tabindex="-1"></a>            has_other_check <span class="op">=</span> <span class="bu">len</span>(wn.synsets(w, pos<span class="op">=</span>other_pos)) <span class="op">&gt;</span> <span class="dv">0</span></span>
<span id="cb2-35"><a href="#cb2-35" aria-hidden="true" tabindex="-1"></a>            has_other <span class="op">=</span> has_other <span class="kw">or</span> has_other_check</span>
<span id="cb2-36"><a href="#cb2-36" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> has_req <span class="kw">and</span> <span class="kw">not</span> has_other</span>
<span id="cb2-37"><a href="#cb2-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-38"><a href="#cb2-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-39"><a href="#cb2-39" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> keep_token(token, pos<span class="op">=</span>wn.NOUN, source<span class="op">=</span><span class="st">"wordnet"</span>):</span>
<span id="cb2-40"><a href="#cb2-40" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb2-41"><a href="#cb2-41" aria-hidden="true" tabindex="-1"></a><span class="co">    Keep token if it passes {POS} check (most-used SemCor sense is a {POS}) and is not a stopword and is longer than one character.</span></span>
<span id="cb2-42"><a href="#cb2-42" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb2-43"><a href="#cb2-43" aria-hidden="true" tabindex="-1"></a>    <span class="cf">with</span> <span class="bu">open</span>(<span class="st">"logging.txt"</span>, <span class="st">"a"</span>, encoding<span class="op">=</span><span class="st">"utf-8"</span>) <span class="im">as</span> f:</span>
<span id="cb2-44"><a href="#cb2-44" aria-hidden="true" tabindex="-1"></a>        f.write(<span class="ss">f"</span><span class="sc">{</span>token<span class="sc">,</span> pos<span class="sc">,</span> source<span class="sc">}</span><span class="ch">\n</span><span class="ss">"</span>)</span>
<span id="cb2-45"><a href="#cb2-45" aria-hidden="true" tabindex="-1"></a>    w <span class="op">=</span> <span class="bu">str</span>(token).lower().strip()</span>
<span id="cb2-46"><a href="#cb2-46" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> w <span class="kw">in</span> STOPWORDS <span class="kw">or</span> <span class="bu">len</span>(w) <span class="op">==</span> <span class="dv">1</span>:</span>
<span id="cb2-47"><a href="#cb2-47" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">False</span></span>
<span id="cb2-48"><a href="#cb2-48" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> pos:</span>
<span id="cb2-49"><a href="#cb2-49" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> commonly_POS(w, pos<span class="op">=</span>pos)</span>
<span id="cb2-50"><a href="#cb2-50" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb2-51"><a href="#cb2-51" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> source<span class="op">==</span><span class="st">"wordnet"</span>:</span>
<span id="cb2-52"><a href="#cb2-52" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> has_pos(w, pos_tag<span class="op">=</span><span class="va">None</span>)</span>
<span id="cb2-53"><a href="#cb2-53" aria-hidden="true" tabindex="-1"></a>        <span class="cf">elif</span> source<span class="op">==</span><span class="st">"wiktionary"</span>:</span>
<span id="cb2-54"><a href="#cb2-54" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> has_pos(w, pos_tag<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb2-55"><a href="#cb2-55" aria-hidden="true" tabindex="-1"></a>                definition <span class="op">=</span> wiktionary_first_definition(token)</span>
<span id="cb2-56"><a href="#cb2-56" aria-hidden="true" tabindex="-1"></a>                <span class="cf">with</span> <span class="bu">open</span>(<span class="st">"logging.txt"</span>, <span class="st">"a"</span>, encoding<span class="op">=</span><span class="st">"utf-8"</span>) <span class="im">as</span> f:</span>
<span id="cb2-57"><a href="#cb2-57" aria-hidden="true" tabindex="-1"></a>                    f.write(<span class="ss">f"</span><span class="sc">{</span>definition<span class="sc">}</span><span class="ch">\n</span><span class="ss">"</span>)</span>
<span id="cb2-58"><a href="#cb2-58" aria-hidden="true" tabindex="-1"></a>                <span class="cf">if</span> definition:</span>
<span id="cb2-59"><a href="#cb2-59" aria-hidden="true" tabindex="-1"></a>                    <span class="cf">return</span> <span class="bu">len</span>(definition) <span class="op">&gt;</span> <span class="dv">3</span></span>
<span id="cb2-60"><a href="#cb2-60" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="va">False</span></span>
<span id="cb2-61"><a href="#cb2-61" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-62"><a href="#cb2-62" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-63"><a href="#cb2-63" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> get_lemma(word, pos<span class="op">=</span>wn.NOUN):</span>
<span id="cb2-64"><a href="#cb2-64" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb2-65"><a href="#cb2-65" aria-hidden="true" tabindex="-1"></a><span class="co">    Lemmatize to {POS} base form (singular) using WordNet.</span></span>
<span id="cb2-66"><a href="#cb2-66" aria-hidden="true" tabindex="-1"></a><span class="co">    If {POS} is not provided, use most common SemCor sense to determine part-of-speech.</span></span>
<span id="cb2-67"><a href="#cb2-67" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb2-68"><a href="#cb2-68" aria-hidden="true" tabindex="-1"></a>    w <span class="op">=</span> word.lower()</span>
<span id="cb2-69"><a href="#cb2-69" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> pos <span class="op">==</span> <span class="va">None</span>:</span>
<span id="cb2-70"><a href="#cb2-70" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> pos_tag <span class="kw">in</span> {wn.VERB, wn.ADJ, wn.ADV, wn.NOUN}:</span>
<span id="cb2-71"><a href="#cb2-71" aria-hidden="true" tabindex="-1"></a>            _is_pos <span class="op">=</span> commonly_POS(w, pos<span class="op">=</span>pos_tag)</span>
<span id="cb2-72"><a href="#cb2-72" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> _is_pos:</span>
<span id="cb2-73"><a href="#cb2-73" aria-hidden="true" tabindex="-1"></a>                pos <span class="op">=</span> pos_tag</span>
<span id="cb2-74"><a href="#cb2-74" aria-hidden="true" tabindex="-1"></a>                <span class="cf">break</span></span>
<span id="cb2-75"><a href="#cb2-75" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>: </span>
<span id="cb2-76"><a href="#cb2-76" aria-hidden="true" tabindex="-1"></a>            pos <span class="op">=</span> wn.NOUN</span>
<span id="cb2-77"><a href="#cb2-77" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-78"><a href="#cb2-78" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> LMT.lemmatize(w, pos<span class="op">=</span>pos)</span>
<span id="cb2-79"><a href="#cb2-79" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-80"><a href="#cb2-80" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-81"><a href="#cb2-81" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> get_basic_and_nonbasic_terms(file_name<span class="op">=</span><span class="st">"word_counts.csv"</span>,</span>
<span id="cb2-82"><a href="#cb2-82" aria-hidden="true" tabindex="-1"></a>         merged_lemma_count_csv<span class="op">=</span><span class="st">"merged_freqs.csv"</span>,</span>
<span id="cb2-83"><a href="#cb2-83" aria-hidden="true" tabindex="-1"></a>         output_top_csv<span class="op">=</span><span class="st">"top200_semcor.csv"</span>,</span>
<span id="cb2-84"><a href="#cb2-84" aria-hidden="true" tabindex="-1"></a>         output_bottom_csv<span class="op">=</span><span class="st">"bottom200_semcor.csv"</span>,</span>
<span id="cb2-85"><a href="#cb2-85" aria-hidden="true" tabindex="-1"></a>         min_count<span class="op">=</span><span class="dv">1</span>, </span>
<span id="cb2-86"><a href="#cb2-86" aria-hidden="true" tabindex="-1"></a>         top_n<span class="op">=</span><span class="dv">200</span>, </span>
<span id="cb2-87"><a href="#cb2-87" aria-hidden="true" tabindex="-1"></a>         bottom_n<span class="op">=</span><span class="dv">200</span>,</span>
<span id="cb2-88"><a href="#cb2-88" aria-hidden="true" tabindex="-1"></a>         pos<span class="op">=</span>wn.NOUN,</span>
<span id="cb2-89"><a href="#cb2-89" aria-hidden="true" tabindex="-1"></a>         source<span class="op">=</span><span class="st">"wordnet"</span>):</span>
<span id="cb2-90"><a href="#cb2-90" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb2-91"><a href="#cb2-91" aria-hidden="true" tabindex="-1"></a><span class="co">    Process the given frequency list to obtain {top_n} top and {bottom_n} bottom words that of part-of-speech {pos}.</span></span>
<span id="cb2-92"><a href="#cb2-92" aria-hidden="true" tabindex="-1"></a><span class="co">    Also, lemmatize words of given part-of-speech, merge their frequencies (e.g., sum frequencies for dog and dogs and report it as frequency for lemma "dog".</span></span>
<span id="cb2-93"><a href="#cb2-93" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb2-94"><a href="#cb2-94" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> <span class="st">"csv"</span> <span class="kw">in</span> file_name:</span>
<span id="cb2-95"><a href="#cb2-95" aria-hidden="true" tabindex="-1"></a>        df <span class="op">=</span> pd.read_csv(file_name)</span>
<span id="cb2-96"><a href="#cb2-96" aria-hidden="true" tabindex="-1"></a>    <span class="cf">elif</span> <span class="st">"txt"</span> <span class="kw">in</span> file_name:</span>
<span id="cb2-97"><a href="#cb2-97" aria-hidden="true" tabindex="-1"></a>        df <span class="op">=</span> pd.read_csv(file_name, sep<span class="op">=</span><span class="st">'</span><span class="ch">\t</span><span class="st">'</span>, header<span class="op">=</span><span class="va">None</span>, names<span class="op">=</span>[<span class="st">'word'</span>, <span class="st">'count_old'</span>, <span class="st">'count'</span>])</span>
<span id="cb2-98"><a href="#cb2-98" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-99"><a href="#cb2-99" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-100"><a href="#cb2-100" aria-hidden="true" tabindex="-1"></a>    df[<span class="st">"count"</span>] <span class="op">=</span> df[<span class="st">"count"</span>].astype(<span class="bu">int</span>)</span>
<span id="cb2-101"><a href="#cb2-101" aria-hidden="true" tabindex="-1"></a>    df[<span class="st">"word"</span>] <span class="op">=</span> df[<span class="st">"word"</span>].astype(<span class="bu">str</span>)</span>
<span id="cb2-102"><a href="#cb2-102" aria-hidden="true" tabindex="-1"></a>    df <span class="op">=</span> df[df[<span class="st">"count"</span>].astype(<span class="bu">int</span>) <span class="op">&gt;=</span> min_count].copy()</span>
<span id="cb2-103"><a href="#cb2-103" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-104"><a href="#cb2-104" aria-hidden="true" tabindex="-1"></a>    tqdm.pandas(desc<span class="op">=</span><span class="st">"Lemmatizing"</span>, dynamic_ncols<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb2-105"><a href="#cb2-105" aria-hidden="true" tabindex="-1"></a>    df[<span class="st">"lemma"</span>] <span class="op">=</span> df[<span class="st">"word"</span>].progress_map(<span class="kw">lambda</span> x: get_lemma(x, pos<span class="op">=</span>pos))</span>
<span id="cb2-106"><a href="#cb2-106" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-107"><a href="#cb2-107" aria-hidden="true" tabindex="-1"></a>    merged <span class="op">=</span> (df.groupby(<span class="st">"lemma"</span>, as_index<span class="op">=</span><span class="va">False</span>)[<span class="st">"count"</span>]</span>
<span id="cb2-108"><a href="#cb2-108" aria-hidden="true" tabindex="-1"></a>                .<span class="bu">sum</span>()</span>
<span id="cb2-109"><a href="#cb2-109" aria-hidden="true" tabindex="-1"></a>                .sort_values(<span class="st">"count"</span>, ascending<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb2-110"><a href="#cb2-110" aria-hidden="true" tabindex="-1"></a>                .reset_index(drop<span class="op">=</span><span class="va">True</span>))</span>
<span id="cb2-111"><a href="#cb2-111" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-112"><a href="#cb2-112" aria-hidden="true" tabindex="-1"></a>    merged.to_csv(merged_lemma_count_csv, index<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb2-113"><a href="#cb2-113" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb2-114"><a href="#cb2-114" aria-hidden="true" tabindex="-1"></a>    top_rows <span class="op">=</span> []</span>
<span id="cb2-115"><a href="#cb2-115" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> row <span class="kw">in</span> tqdm(merged.itertuples(index<span class="op">=</span><span class="va">False</span>), desc<span class="op">=</span><span class="st">"Getting top frequency words"</span>):</span>
<span id="cb2-116"><a href="#cb2-116" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> keep_token(row.lemma, pos<span class="op">=</span>pos, source<span class="op">=</span>source):</span>
<span id="cb2-117"><a href="#cb2-117" aria-hidden="true" tabindex="-1"></a>            top_rows.append(row)</span>
<span id="cb2-118"><a href="#cb2-118" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> <span class="bu">len</span>(top_rows) <span class="op">&gt;=</span> top_n:</span>
<span id="cb2-119"><a href="#cb2-119" aria-hidden="true" tabindex="-1"></a>                <span class="cf">break</span></span>
<span id="cb2-120"><a href="#cb2-120" aria-hidden="true" tabindex="-1"></a>    nouns_top <span class="op">=</span> pd.DataFrame(top_rows, columns<span class="op">=</span>merged.columns)</span>
<span id="cb2-121"><a href="#cb2-121" aria-hidden="true" tabindex="-1"></a>    nouns_top.to_csv(output_top_csv, index<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb2-122"><a href="#cb2-122" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-123"><a href="#cb2-123" aria-hidden="true" tabindex="-1"></a>    bottom_rows <span class="op">=</span> []</span>
<span id="cb2-124"><a href="#cb2-124" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> tqdm(<span class="bu">range</span>(<span class="bu">len</span>(merged) <span class="op">-</span> <span class="dv">1</span>, <span class="op">-</span><span class="dv">1</span>, <span class="op">-</span><span class="dv">1</span>), desc<span class="op">=</span><span class="st">"Getting bottom frequency words"</span>):</span>
<span id="cb2-125"><a href="#cb2-125" aria-hidden="true" tabindex="-1"></a>        lemma <span class="op">=</span> merged.at[i, <span class="st">"lemma"</span>]</span>
<span id="cb2-126"><a href="#cb2-126" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> keep_token(lemma, pos<span class="op">=</span>pos, source<span class="op">=</span>source):</span>
<span id="cb2-127"><a href="#cb2-127" aria-hidden="true" tabindex="-1"></a>            bottom_rows.append(merged.iloc[i])</span>
<span id="cb2-128"><a href="#cb2-128" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> <span class="bu">len</span>(bottom_rows) <span class="op">&gt;=</span> bottom_n:</span>
<span id="cb2-129"><a href="#cb2-129" aria-hidden="true" tabindex="-1"></a>                <span class="cf">break</span></span>
<span id="cb2-130"><a href="#cb2-130" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-131"><a href="#cb2-131" aria-hidden="true" tabindex="-1"></a>    nouns_bottom <span class="op">=</span> pd.DataFrame(bottom_rows[::<span class="op">-</span><span class="dv">1</span>])</span>
<span id="cb2-132"><a href="#cb2-132" aria-hidden="true" tabindex="-1"></a>    nouns_bottom.to_csv(output_bottom_csv, index<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb2-133"><a href="#cb2-133" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-134"><a href="#cb2-134" aria-hidden="true" tabindex="-1"></a>    <span class="co"># print(f"Collected top {len(nouns_top)} and bottom {len(nouns_bottom)} for {pos}")</span></span>
<span id="cb2-135"><a href="#cb2-135" aria-hidden="true" tabindex="-1"></a>    <span class="co"># tqdm.pandas(desc="Filtering with SemCor/WordNet/Wiktionary", dynamic_ncols=True)</span></span>
<span id="cb2-136"><a href="#cb2-136" aria-hidden="true" tabindex="-1"></a>    <span class="co"># mask = merged["lemma"].progress_map(lambda x: keep_token(x, pos=pos, source=source))</span></span>
<span id="cb2-137"><a href="#cb2-137" aria-hidden="true" tabindex="-1"></a>    <span class="co"># nouns_df = merged[mask].reset_index(drop=True)</span></span>
<span id="cb2-138"><a href="#cb2-138" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-139"><a href="#cb2-139" aria-hidden="true" tabindex="-1"></a>    <span class="co"># # top N most frequent nouns</span></span>
<span id="cb2-140"><a href="#cb2-140" aria-hidden="true" tabindex="-1"></a>    <span class="co"># nouns_top = nouns_df.head(top_n)</span></span>
<span id="cb2-141"><a href="#cb2-141" aria-hidden="true" tabindex="-1"></a>    <span class="co"># nouns_top.to_csv(output_top_csv, index=False)</span></span>
<span id="cb2-142"><a href="#cb2-142" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-143"><a href="#cb2-143" aria-hidden="true" tabindex="-1"></a>    <span class="co"># # bottom N most frequent nouns (= least frequent)</span></span>
<span id="cb2-144"><a href="#cb2-144" aria-hidden="true" tabindex="-1"></a>    <span class="co"># nouns_bottom = nouns_df.tail(bottom_n)</span></span>
<span id="cb2-145"><a href="#cb2-145" aria-hidden="true" tabindex="-1"></a>    <span class="co"># nouns_bottom.to_csv(output_bottom_csv, index=False)</span></span>
<span id="cb2-146"><a href="#cb2-146" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-147"><a href="#cb2-147" aria-hidden="true" tabindex="-1"></a>    <span class="co"># print(f"Kept {len(nouns_df)} total words of part-of-speech {pos}")</span></span>
<span id="cb2-148"><a href="#cb2-148" aria-hidden="true" tabindex="-1"></a>    <span class="co"># print(f"Merged lemma frequencies written to {merged_lemma_count_csv}")</span></span>
<span id="cb2-149"><a href="#cb2-149" aria-hidden="true" tabindex="-1"></a>    <span class="co"># print(f"Top {top_n} written to {output_top_csv}")</span></span>
<span id="cb2-150"><a href="#cb2-150" aria-hidden="true" tabindex="-1"></a>    <span class="co"># print(f"Bottom {bottom_n} written to {output_bottom_csv}")</span></span>
<span id="cb2-151"><a href="#cb2-151" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-152"><a href="#cb2-152" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-153"><a href="#cb2-153" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> most_used_semcor_definition(word: <span class="bu">str</span>, pos: <span class="bu">str</span> <span class="op">|</span> <span class="va">None</span><span class="op">=</span>wn.NOUN) <span class="op">-&gt;</span> <span class="bu">str</span> <span class="op">|</span> <span class="va">None</span>:</span>
<span id="cb2-154"><a href="#cb2-154" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb2-155"><a href="#cb2-155" aria-hidden="true" tabindex="-1"></a><span class="co">    Return the WordNet definition (gloss) for the most-used SemCor synset.</span></span>
<span id="cb2-156"><a href="#cb2-156" aria-hidden="true" tabindex="-1"></a><span class="co">    - pos: 'n' / 'v' / 'a' (adj) / 'r' (adv) / None (all POS).</span></span>
<span id="cb2-157"><a href="#cb2-157" aria-hidden="true" tabindex="-1"></a><span class="co">    - If SemCor counts are all zero for the chosen POS, take the first sense.</span></span>
<span id="cb2-158"><a href="#cb2-158" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb2-159"><a href="#cb2-159" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-160"><a href="#cb2-160" aria-hidden="true" tabindex="-1"></a>    w <span class="op">=</span> <span class="bu">str</span>(word).strip().lower()</span>
<span id="cb2-161"><a href="#cb2-161" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-162"><a href="#cb2-162" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> pos <span class="kw">in</span> {wn.NOUN, wn.VERB, wn.ADJ, wn.ADV}:</span>
<span id="cb2-163"><a href="#cb2-163" aria-hidden="true" tabindex="-1"></a>        lemma_objs <span class="op">=</span> wn.lemmas(w, pos<span class="op">=</span>pos)</span>
<span id="cb2-164"><a href="#cb2-164" aria-hidden="true" tabindex="-1"></a>        synsets <span class="op">=</span> wn.synsets(w, pos<span class="op">=</span>pos)</span>
<span id="cb2-165"><a href="#cb2-165" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb2-166"><a href="#cb2-166" aria-hidden="true" tabindex="-1"></a>        lemma_objs <span class="op">=</span> wn.lemmas(w, pos<span class="op">=</span>wn.NOUN) <span class="op">+</span> wn.lemmas(w, pos<span class="op">=</span>wn.VERB) <span class="op">+</span> wn.lemmas(w, pos<span class="op">=</span>wn.ADJ) <span class="op">+</span> wn.lemmas(w, pos<span class="op">=</span>wn.ADV)</span>
<span id="cb2-167"><a href="#cb2-167" aria-hidden="true" tabindex="-1"></a>        synsets <span class="op">=</span> wn.synsets(w, pos<span class="op">=</span>wn.NOUN) <span class="op">+</span> wn.synsets(w, pos<span class="op">=</span>wn.VERB) <span class="op">+</span> wn.synsets(w, pos<span class="op">=</span>wn.ADJ) <span class="op">+</span> wn.synsets(w, pos<span class="op">=</span>wn.ADV)</span>
<span id="cb2-168"><a href="#cb2-168" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-169"><a href="#cb2-169" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> <span class="kw">not</span> synsets:</span>
<span id="cb2-170"><a href="#cb2-170" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">None</span></span>
<span id="cb2-171"><a href="#cb2-171" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-172"><a href="#cb2-172" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> lemma_objs:</span>
<span id="cb2-173"><a href="#cb2-173" aria-hidden="true" tabindex="-1"></a>        best_lemma <span class="op">=</span> <span class="bu">max</span>(lemma_objs, key<span class="op">=</span><span class="kw">lambda</span> L: L.count() <span class="kw">or</span> <span class="dv">0</span>)</span>
<span id="cb2-174"><a href="#cb2-174" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="bu">any</span>((L.count() <span class="kw">or</span> <span class="dv">0</span>) <span class="op">&gt;</span> <span class="dv">0</span> <span class="cf">for</span> L <span class="kw">in</span> lemma_objs):</span>
<span id="cb2-175"><a href="#cb2-175" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> best_lemma.synset().definition()</span>
<span id="cb2-176"><a href="#cb2-176" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-177"><a href="#cb2-177" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> synsets[<span class="dv">0</span>].definition()</span>
<span id="cb2-178"><a href="#cb2-178" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-179"><a href="#cb2-179" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> wiktionary_first_definition(word):</span>
<span id="cb2-180"><a href="#cb2-180" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb2-181"><a href="#cb2-181" aria-hidden="true" tabindex="-1"></a><span class="co">    Return the first definition line ('# ...') from the English Wiktionary page for `word`.</span></span>
<span id="cb2-182"><a href="#cb2-182" aria-hidden="true" tabindex="-1"></a><span class="co">    Ignores part of speech and just takes the earliest definition found in the language section.</span></span>
<span id="cb2-183"><a href="#cb2-183" aria-hidden="true" tabindex="-1"></a><span class="co">    Returns None if no definition is found.</span></span>
<span id="cb2-184"><a href="#cb2-184" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb2-185"><a href="#cb2-185" aria-hidden="true" tabindex="-1"></a>    r <span class="op">=</span> requests.get(</span>
<span id="cb2-186"><a href="#cb2-186" aria-hidden="true" tabindex="-1"></a>        _WIKI_API,</span>
<span id="cb2-187"><a href="#cb2-187" aria-hidden="true" tabindex="-1"></a>        params<span class="op">=</span>{</span>
<span id="cb2-188"><a href="#cb2-188" aria-hidden="true" tabindex="-1"></a>            <span class="st">"action"</span>: <span class="st">"parse"</span>,</span>
<span id="cb2-189"><a href="#cb2-189" aria-hidden="true" tabindex="-1"></a>            <span class="st">"page"</span>: word.strip(),</span>
<span id="cb2-190"><a href="#cb2-190" aria-hidden="true" tabindex="-1"></a>            <span class="st">"prop"</span>: <span class="st">"wikitext"</span>,</span>
<span id="cb2-191"><a href="#cb2-191" aria-hidden="true" tabindex="-1"></a>            <span class="st">"format"</span>: <span class="st">"json"</span>,</span>
<span id="cb2-192"><a href="#cb2-192" aria-hidden="true" tabindex="-1"></a>            <span class="st">"redirects"</span>: <span class="dv">1</span>,</span>
<span id="cb2-193"><a href="#cb2-193" aria-hidden="true" tabindex="-1"></a>        },</span>
<span id="cb2-194"><a href="#cb2-194" aria-hidden="true" tabindex="-1"></a>        headers<span class="op">=</span>{<span class="st">"User-Agent"</span>: <span class="st">"basic-words-research/1.0"</span>},</span>
<span id="cb2-195"><a href="#cb2-195" aria-hidden="true" tabindex="-1"></a>        timeout<span class="op">=</span><span class="dv">15</span>,</span>
<span id="cb2-196"><a href="#cb2-196" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb2-197"><a href="#cb2-197" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> r.status_code <span class="op">!=</span> <span class="dv">200</span>:</span>
<span id="cb2-198"><a href="#cb2-198" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">None</span></span>
<span id="cb2-199"><a href="#cb2-199" aria-hidden="true" tabindex="-1"></a>    data <span class="op">=</span> r.json()</span>
<span id="cb2-200"><a href="#cb2-200" aria-hidden="true" tabindex="-1"></a>    wikitext <span class="op">=</span> data.get(<span class="st">"parse"</span>, {}).get(<span class="st">"wikitext"</span>, {}).get(<span class="st">"*"</span>, <span class="st">""</span>)</span>
<span id="cb2-201"><a href="#cb2-201" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> <span class="kw">not</span> wikitext:</span>
<span id="cb2-202"><a href="#cb2-202" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">None</span></span>
<span id="cb2-203"><a href="#cb2-203" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-204"><a href="#cb2-204" aria-hidden="true" tabindex="-1"></a>    m <span class="op">=</span> re.search(</span>
<span id="cb2-205"><a href="#cb2-205" aria-hidden="true" tabindex="-1"></a>        <span class="vs">rf"</span><span class="fu">(?ms)</span><span class="dv">^</span><span class="kw">(</span><span class="fu">?&lt;</span><span class="vs">!=</span><span class="kw">)</span><span class="vs">==</span><span class="dv">\s</span><span class="op">*</span><span class="vs">English</span><span class="dv">\s</span><span class="op">*</span><span class="vs">==</span><span class="ex">(</span><span class="fu">?!</span><span class="vs">=</span><span class="ex">)</span><span class="dv">\s</span><span class="op">*</span><span class="kw">(</span><span class="dv">.</span><span class="op">*?</span><span class="kw">)</span><span class="vs">"</span></span>
<span id="cb2-206"><a href="#cb2-206" aria-hidden="true" tabindex="-1"></a>        <span class="vs">rf"</span><span class="ex">(</span><span class="fu">?=</span><span class="dv">^</span><span class="kw">(</span><span class="fu">?&lt;</span><span class="vs">!=</span><span class="kw">)</span><span class="vs">==</span><span class="dv">\s</span><span class="op">*</span><span class="pp">[^=]</span><span class="dv">.</span><span class="op">*?</span><span class="vs">==</span><span class="ex">(</span><span class="fu">?!</span><span class="vs">=</span><span class="ex">)</span><span class="cf">|</span><span class="dv">\Z</span><span class="ex">)</span><span class="vs">"</span>,</span>
<span id="cb2-207"><a href="#cb2-207" aria-hidden="true" tabindex="-1"></a>        wikitext,</span>
<span id="cb2-208"><a href="#cb2-208" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb2-209"><a href="#cb2-209" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> <span class="kw">not</span> m:</span>
<span id="cb2-210"><a href="#cb2-210" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">None</span></span>
<span id="cb2-211"><a href="#cb2-211" aria-hidden="true" tabindex="-1"></a>    lang_text <span class="op">=</span> m.group(<span class="dv">1</span>)</span>
<span id="cb2-212"><a href="#cb2-212" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-213"><a href="#cb2-213" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> line <span class="kw">in</span> lang_text.splitlines():</span>
<span id="cb2-214"><a href="#cb2-214" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> re.match(<span class="vs">r"</span><span class="dv">^</span><span class="vs">#</span><span class="dv">\s</span><span class="ex">(</span><span class="fu">?!</span><span class="pp">[#*:;]</span><span class="ex">)</span><span class="vs">"</span>, line):</span>
<span id="cb2-215"><a href="#cb2-215" aria-hidden="true" tabindex="-1"></a>            text <span class="op">=</span> line.lstrip(<span class="st">"# "</span>).strip()</span>
<span id="cb2-216"><a href="#cb2-216" aria-hidden="true" tabindex="-1"></a>            text <span class="op">=</span> html.unescape(text)</span>
<span id="cb2-217"><a href="#cb2-217" aria-hidden="true" tabindex="-1"></a>            text <span class="op">=</span> re.sub(<span class="vs">r"</span><span class="ch">\{\{</span><span class="pp">[^{}]</span><span class="op">*</span><span class="ch">\}\}</span><span class="vs">"</span>, <span class="st">""</span>, text)</span>
<span id="cb2-218"><a href="#cb2-218" aria-hidden="true" tabindex="-1"></a>            text <span class="op">=</span> re.sub(<span class="vs">r"</span><span class="ch">\[\[</span><span class="kw">(</span><span class="pp">[^|</span><span class="ch">\]</span><span class="pp">]</span><span class="op">+</span><span class="kw">)</span><span class="ch">\|</span><span class="kw">(</span><span class="pp">[^</span><span class="ch">\]</span><span class="pp">]</span><span class="op">+</span><span class="kw">)</span><span class="ch">\]\]</span><span class="vs">"</span>, <span class="vs">r"</span><span class="ch">\2</span><span class="vs">"</span>, text)</span>
<span id="cb2-219"><a href="#cb2-219" aria-hidden="true" tabindex="-1"></a>            text <span class="op">=</span> re.sub(<span class="vs">r"</span><span class="ch">\[\[</span><span class="kw">(</span><span class="pp">[^</span><span class="ch">\]</span><span class="pp">]</span><span class="op">+</span><span class="kw">)</span><span class="ch">\]\]</span><span class="vs">"</span>, <span class="vs">r"</span><span class="ch">\1</span><span class="vs">"</span>, text)</span>
<span id="cb2-220"><a href="#cb2-220" aria-hidden="true" tabindex="-1"></a>            text <span class="op">=</span> text.replace(<span class="st">"''"</span>, <span class="st">""</span>)</span>
<span id="cb2-221"><a href="#cb2-221" aria-hidden="true" tabindex="-1"></a>            text <span class="op">=</span> re.sub(<span class="vs">r"&lt;</span><span class="pp">[^&gt;]</span><span class="op">+</span><span class="vs">&gt;"</span>, <span class="st">""</span>, text)</span>
<span id="cb2-222"><a href="#cb2-222" aria-hidden="true" tabindex="-1"></a>            text <span class="op">=</span> re.sub(<span class="vs">r"</span><span class="dv">\s</span><span class="op">+</span><span class="vs">"</span>, <span class="st">" "</span>, text).strip(<span class="st">" ;:—–-"</span>)</span>
<span id="cb2-223"><a href="#cb2-223" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> text</span>
<span id="cb2-224"><a href="#cb2-224" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-225"><a href="#cb2-225" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-226"><a href="#cb2-226" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> write_most_used_definitions_csv(input_csv: <span class="bu">str</span>,</span>
<span id="cb2-227"><a href="#cb2-227" aria-hidden="true" tabindex="-1"></a>                                    output_csv: <span class="bu">str</span>,</span>
<span id="cb2-228"><a href="#cb2-228" aria-hidden="true" tabindex="-1"></a>                                    pos: <span class="bu">str</span> <span class="op">|</span> <span class="va">None</span> <span class="op">=</span> wn.NOUN,</span>
<span id="cb2-229"><a href="#cb2-229" aria-hidden="true" tabindex="-1"></a>                                    source<span class="op">=</span><span class="st">"wordnet"</span>):</span>
<span id="cb2-230"><a href="#cb2-230" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb2-231"><a href="#cb2-231" aria-hidden="true" tabindex="-1"></a><span class="co">    Obtain the definition for the most-used SemCor synset (should correspond to the most widely used sense of the words) for each word in the file.</span></span>
<span id="cb2-232"><a href="#cb2-232" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb2-233"><a href="#cb2-233" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-234"><a href="#cb2-234" aria-hidden="true" tabindex="-1"></a>    df <span class="op">=</span> pd.read_csv(input_csv)</span>
<span id="cb2-235"><a href="#cb2-235" aria-hidden="true" tabindex="-1"></a>    lemmas <span class="op">=</span> (df[<span class="st">"lemma"</span>].astype(<span class="bu">str</span>))</span>
<span id="cb2-236"><a href="#cb2-236" aria-hidden="true" tabindex="-1"></a>    counts <span class="op">=</span> (df[<span class="st">"count"</span>].astype(<span class="bu">int</span>))</span>
<span id="cb2-237"><a href="#cb2-237" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-238"><a href="#cb2-238" aria-hidden="true" tabindex="-1"></a>    rows <span class="op">=</span> []</span>
<span id="cb2-239"><a href="#cb2-239" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> lemma, count <span class="kw">in</span> tqdm(<span class="bu">zip</span>(lemmas, counts), desc<span class="op">=</span><span class="st">"Obtaining definitions"</span>):</span>
<span id="cb2-240"><a href="#cb2-240" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> source <span class="op">==</span><span class="st">"wordnet"</span>:</span>
<span id="cb2-241"><a href="#cb2-241" aria-hidden="true" tabindex="-1"></a>            definition <span class="op">=</span> most_used_semcor_definition(lemma, pos<span class="op">=</span>pos)</span>
<span id="cb2-242"><a href="#cb2-242" aria-hidden="true" tabindex="-1"></a>        <span class="cf">elif</span> source <span class="op">==</span><span class="st">"wiktionary"</span>:</span>
<span id="cb2-243"><a href="#cb2-243" aria-hidden="true" tabindex="-1"></a>            definition <span class="op">=</span> wiktionary_first_definition(lemma)  </span>
<span id="cb2-244"><a href="#cb2-244" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>:</span>
<span id="cb2-245"><a href="#cb2-245" aria-hidden="true" tabindex="-1"></a>            definition <span class="op">=</span> <span class="va">None</span></span>
<span id="cb2-246"><a href="#cb2-246" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> definition <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb2-247"><a href="#cb2-247" aria-hidden="true" tabindex="-1"></a>            rows.append({<span class="st">"lemma"</span>: lemma, <span class="st">"definition"</span>: definition, <span class="st">"count"</span>: count})</span>
<span id="cb2-248"><a href="#cb2-248" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-249"><a href="#cb2-249" aria-hidden="true" tabindex="-1"></a>    pd.DataFrame(rows).to_csv(output_csv, index<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb2-250"><a href="#cb2-250" aria-hidden="true" tabindex="-1"></a>    <span class="co"># print(f"Wrote {len(rows)} rows to {output_csv}")</span></span>
<span id="cb2-251"><a href="#cb2-251" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-252"><a href="#cb2-252" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-253"><a href="#cb2-253" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> nltk_pos_to_wn(pos: <span class="bu">str</span>):</span>
<span id="cb2-254"><a href="#cb2-254" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb2-255"><a href="#cb2-255" aria-hidden="true" tabindex="-1"></a><span class="co">    Map NLTK parts of speech to WordNet ones.</span></span>
<span id="cb2-256"><a href="#cb2-256" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb2-257"><a href="#cb2-257" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> pos.startswith(<span class="st">"N"</span>): <span class="cf">return</span> wn.NOUN</span>
<span id="cb2-258"><a href="#cb2-258" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> pos.startswith(<span class="st">"V"</span>): <span class="cf">return</span> wn.VERB</span>
<span id="cb2-259"><a href="#cb2-259" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> pos.startswith(<span class="st">"J"</span>): <span class="cf">return</span> wn.ADJ</span>
<span id="cb2-260"><a href="#cb2-260" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> pos.startswith(<span class="st">"R"</span>): <span class="cf">return</span> wn.ADV</span>
<span id="cb2-261"><a href="#cb2-261" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="va">None</span></span>
<span id="cb2-262"><a href="#cb2-262" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-263"><a href="#cb2-263" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-264"><a href="#cb2-264" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> def_to_content_lemmas(def_text: <span class="bu">str</span>):</span>
<span id="cb2-265"><a href="#cb2-265" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb2-266"><a href="#cb2-266" aria-hidden="true" tabindex="-1"></a><span class="co">    Lemmatize word definitions; filter out stopwords and non-word tokens (that contain anything but alphabatecial characters and hyphen).</span></span>
<span id="cb2-267"><a href="#cb2-267" aria-hidden="true" tabindex="-1"></a><span class="co">    Lemmatization occurs according to the part-of-speech defined by NLTK POS-tagging.</span></span>
<span id="cb2-268"><a href="#cb2-268" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb2-269"><a href="#cb2-269" aria-hidden="true" tabindex="-1"></a>    toks <span class="op">=</span> [t.lower() <span class="cf">for</span> t <span class="kw">in</span> word_tokenize(<span class="bu">str</span>(def_text))]</span>
<span id="cb2-270"><a href="#cb2-270" aria-hidden="true" tabindex="-1"></a>    toks <span class="op">=</span> [t <span class="cf">for</span> t <span class="kw">in</span> toks <span class="cf">if</span> re.fullmatch(<span class="vs">r"</span><span class="pp">[a-z][a-z'-]</span><span class="op">*</span><span class="vs">"</span>, t)]</span>
<span id="cb2-271"><a href="#cb2-271" aria-hidden="true" tabindex="-1"></a>    toks <span class="op">=</span> [t <span class="cf">for</span> t <span class="kw">in</span> toks <span class="cf">if</span> t <span class="kw">not</span> <span class="kw">in</span> STOP <span class="kw">and</span> <span class="bu">len</span>(t) <span class="op">&gt;</span> <span class="dv">1</span>]</span>
<span id="cb2-272"><a href="#cb2-272" aria-hidden="true" tabindex="-1"></a>    <span class="co"># NLTK POS-tagging takes context into account</span></span>
<span id="cb2-273"><a href="#cb2-273" aria-hidden="true" tabindex="-1"></a>    tagged <span class="op">=</span> pos_tag(toks)</span>
<span id="cb2-274"><a href="#cb2-274" aria-hidden="true" tabindex="-1"></a>    lemmas <span class="op">=</span> []</span>
<span id="cb2-275"><a href="#cb2-275" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> w, p <span class="kw">in</span> tagged:</span>
<span id="cb2-276"><a href="#cb2-276" aria-hidden="true" tabindex="-1"></a>        wn_pos <span class="op">=</span> nltk_pos_to_wn(p)</span>
<span id="cb2-277"><a href="#cb2-277" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> wn_pos <span class="kw">in</span> {<span class="st">"n"</span>, <span class="st">"v"</span>, <span class="st">"a"</span>, <span class="st">"r"</span>}:</span>
<span id="cb2-278"><a href="#cb2-278" aria-hidden="true" tabindex="-1"></a>            lemmas.append(LMT.lemmatize(w, pos<span class="op">=</span>wn_pos))</span>
<span id="cb2-279"><a href="#cb2-279" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> lemmas</span>
<span id="cb2-280"><a href="#cb2-280" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-281"><a href="#cb2-281" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-282"><a href="#cb2-282" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> attach_definition_frequencies(input_csv: <span class="bu">str</span>,</span>
<span id="cb2-283"><a href="#cb2-283" aria-hidden="true" tabindex="-1"></a>                                  output_csv: <span class="bu">str</span> <span class="op">=</span> <span class="st">"defs_with_freq.csv"</span>,</span>
<span id="cb2-284"><a href="#cb2-284" aria-hidden="true" tabindex="-1"></a>                                  freq_csv: <span class="bu">str</span> <span class="op">=</span> <span class="st">"merged_freq.csv"</span>):</span>
<span id="cb2-285"><a href="#cb2-285" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb2-286"><a href="#cb2-286" aria-hidden="true" tabindex="-1"></a><span class="co">    Read input CSV with columns [lemma, definition, count], clean and lemmatize definitions,</span></span>
<span id="cb2-287"><a href="#cb2-287" aria-hidden="true" tabindex="-1"></a><span class="co">    map each token to its frequencies, and write metrics:</span></span>
<span id="cb2-288"><a href="#cb2-288" aria-hidden="true" tabindex="-1"></a><span class="co">      - len_tokens: number of content lemmas in the definition</span></span>
<span id="cb2-289"><a href="#cb2-289" aria-hidden="true" tabindex="-1"></a><span class="co">      - mean_log_freq: mean log10 probability based on counts from freq_csv</span></span>
<span id="cb2-290"><a href="#cb2-290" aria-hidden="true" tabindex="-1"></a><span class="co">      - median_log_freq: median log10 probability</span></span>
<span id="cb2-291"><a href="#cb2-291" aria-hidden="true" tabindex="-1"></a><span class="co">      - miss_rate: fraction of definition lemmas missing from freq table</span></span>
<span id="cb2-292"><a href="#cb2-292" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb2-293"><a href="#cb2-293" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-294"><a href="#cb2-294" aria-hidden="true" tabindex="-1"></a>    freq_df <span class="op">=</span> pd.read_csv(freq_csv)</span>
<span id="cb2-295"><a href="#cb2-295" aria-hidden="true" tabindex="-1"></a>    freq_df[<span class="st">"lemma"</span>] <span class="op">=</span> freq_df[<span class="st">"lemma"</span>].astype(<span class="bu">str</span>)</span>
<span id="cb2-296"><a href="#cb2-296" aria-hidden="true" tabindex="-1"></a>    agg <span class="op">=</span> freq_df.groupby(<span class="st">"lemma"</span>, as_index<span class="op">=</span><span class="va">False</span>)[<span class="st">"count"</span>].<span class="bu">sum</span>()</span>
<span id="cb2-297"><a href="#cb2-297" aria-hidden="true" tabindex="-1"></a>    total <span class="op">=</span> <span class="bu">float</span>(agg[<span class="st">"count"</span>].<span class="bu">sum</span>())</span>
<span id="cb2-298"><a href="#cb2-298" aria-hidden="true" tabindex="-1"></a>    agg[<span class="st">"log10p"</span>] <span class="op">=</span> np.log10((agg[<span class="st">"count"</span>] <span class="op">+</span> <span class="fl">0.5</span>) <span class="op">/</span> (total <span class="op">+</span> <span class="fl">0.5</span> <span class="op">*</span> <span class="bu">len</span>(agg)))</span>
<span id="cb2-299"><a href="#cb2-299" aria-hidden="true" tabindex="-1"></a>    freq_lookup <span class="op">=</span> <span class="bu">dict</span>(<span class="bu">zip</span>(agg[<span class="st">"lemma"</span>], agg[<span class="st">"log10p"</span>]))</span>
<span id="cb2-300"><a href="#cb2-300" aria-hidden="true" tabindex="-1"></a>    df <span class="op">=</span> pd.read_csv(input_csv)</span>
<span id="cb2-301"><a href="#cb2-301" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-302"><a href="#cb2-302" aria-hidden="true" tabindex="-1"></a>    out_rows <span class="op">=</span> []</span>
<span id="cb2-303"><a href="#cb2-303" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i, row <span class="kw">in</span> df.iterrows():</span>
<span id="cb2-304"><a href="#cb2-304" aria-hidden="true" tabindex="-1"></a>        definition <span class="op">=</span> <span class="bu">str</span>(row[<span class="st">"definition"</span>])</span>
<span id="cb2-305"><a href="#cb2-305" aria-hidden="true" tabindex="-1"></a>        lemmas <span class="op">=</span> def_to_content_lemmas(definition)</span>
<span id="cb2-306"><a href="#cb2-306" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-307"><a href="#cb2-307" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="bu">len</span>(lemmas) <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb2-308"><a href="#cb2-308" aria-hidden="true" tabindex="-1"></a>            out_rows.append({</span>
<span id="cb2-309"><a href="#cb2-309" aria-hidden="true" tabindex="-1"></a>                <span class="st">"lemma"</span>: row[<span class="st">"lemma"</span>],</span>
<span id="cb2-310"><a href="#cb2-310" aria-hidden="true" tabindex="-1"></a>                <span class="st">"definition"</span>: definition,</span>
<span id="cb2-311"><a href="#cb2-311" aria-hidden="true" tabindex="-1"></a>                <span class="st">"len_tokens"</span>: <span class="dv">0</span>,</span>
<span id="cb2-312"><a href="#cb2-312" aria-hidden="true" tabindex="-1"></a>                <span class="st">"mean_log_freq"</span>: np.nan,</span>
<span id="cb2-313"><a href="#cb2-313" aria-hidden="true" tabindex="-1"></a>                <span class="st">"median_log_freq"</span>: np.nan,</span>
<span id="cb2-314"><a href="#cb2-314" aria-hidden="true" tabindex="-1"></a>                <span class="st">"miss_rate"</span>: <span class="fl">1.0</span></span>
<span id="cb2-315"><a href="#cb2-315" aria-hidden="true" tabindex="-1"></a>            })</span>
<span id="cb2-316"><a href="#cb2-316" aria-hidden="true" tabindex="-1"></a>            <span class="cf">continue</span></span>
<span id="cb2-317"><a href="#cb2-317" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-318"><a href="#cb2-318" aria-hidden="true" tabindex="-1"></a>        vals <span class="op">=</span> [freq_lookup.get(l, np.nan) <span class="cf">for</span> l <span class="kw">in</span> lemmas]</span>
<span id="cb2-319"><a href="#cb2-319" aria-hidden="true" tabindex="-1"></a>        present <span class="op">=</span> [v <span class="cf">for</span> v <span class="kw">in</span> vals <span class="cf">if</span> <span class="kw">not</span> np.isnan(v)]</span>
<span id="cb2-320"><a href="#cb2-320" aria-hidden="true" tabindex="-1"></a>        miss_rate <span class="op">=</span> <span class="fl">1.0</span> <span class="op">-</span> (<span class="bu">len</span>(present) <span class="op">/</span> <span class="bu">len</span>(lemmas))</span>
<span id="cb2-321"><a href="#cb2-321" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-322"><a href="#cb2-322" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="kw">not</span> <span class="bu">len</span>(present):</span>
<span id="cb2-323"><a href="#cb2-323" aria-hidden="true" tabindex="-1"></a>            mean_log <span class="op">=</span> np.nan</span>
<span id="cb2-324"><a href="#cb2-324" aria-hidden="true" tabindex="-1"></a>            median_log <span class="op">=</span> np.nan</span>
<span id="cb2-325"><a href="#cb2-325" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>:</span>
<span id="cb2-326"><a href="#cb2-326" aria-hidden="true" tabindex="-1"></a>            arr <span class="op">=</span> np.array(present, dtype<span class="op">=</span><span class="bu">float</span>)</span>
<span id="cb2-327"><a href="#cb2-327" aria-hidden="true" tabindex="-1"></a>            mean_log <span class="op">=</span> <span class="bu">float</span>(arr.mean())</span>
<span id="cb2-328"><a href="#cb2-328" aria-hidden="true" tabindex="-1"></a>            median_log <span class="op">=</span> <span class="bu">float</span>(np.median(arr))</span>
<span id="cb2-329"><a href="#cb2-329" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-330"><a href="#cb2-330" aria-hidden="true" tabindex="-1"></a>        out_rows.append({</span>
<span id="cb2-331"><a href="#cb2-331" aria-hidden="true" tabindex="-1"></a>            <span class="st">"lemma"</span>: row[<span class="st">"lemma"</span>],</span>
<span id="cb2-332"><a href="#cb2-332" aria-hidden="true" tabindex="-1"></a>            <span class="st">"definition"</span>: definition,</span>
<span id="cb2-333"><a href="#cb2-333" aria-hidden="true" tabindex="-1"></a>            <span class="st">"len_tokens"</span>: <span class="bu">len</span>(lemmas),</span>
<span id="cb2-334"><a href="#cb2-334" aria-hidden="true" tabindex="-1"></a>            <span class="st">"mean_log_freq"</span>: mean_log,</span>
<span id="cb2-335"><a href="#cb2-335" aria-hidden="true" tabindex="-1"></a>            <span class="st">"median_log_freq"</span>: median_log,</span>
<span id="cb2-336"><a href="#cb2-336" aria-hidden="true" tabindex="-1"></a>            <span class="st">"miss_rate"</span>: miss_rate</span>
<span id="cb2-337"><a href="#cb2-337" aria-hidden="true" tabindex="-1"></a>        })</span>
<span id="cb2-338"><a href="#cb2-338" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-339"><a href="#cb2-339" aria-hidden="true" tabindex="-1"></a>    out_df <span class="op">=</span> pd.DataFrame(out_rows)</span>
<span id="cb2-340"><a href="#cb2-340" aria-hidden="true" tabindex="-1"></a>    out_df.to_csv(output_csv, index<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb2-341"><a href="#cb2-341" aria-hidden="true" tabindex="-1"></a>    <span class="co"># print(f"Wrote {len(out_df)} rows to {output_csv}")</span></span>
<span id="cb2-342"><a href="#cb2-342" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-343"><a href="#cb2-343" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-344"><a href="#cb2-344" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> t_tests_on_definition(basic_csv: <span class="bu">str</span>, nonbasic_csv: <span class="bu">str</span>, value_col: <span class="bu">str</span> <span class="op">=</span> <span class="st">"mean_log_freq"</span>):</span>
<span id="cb2-345"><a href="#cb2-345" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb2-346"><a href="#cb2-346" aria-hidden="true" tabindex="-1"></a><span class="co">    Performs statistical analysis on the results; determines whether the difference between results for basic and non-basic wordlists is statistically significant.</span></span>
<span id="cb2-347"><a href="#cb2-347" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb2-348"><a href="#cb2-348" aria-hidden="true" tabindex="-1"></a>    basic <span class="op">=</span> pd.read_csv(basic_csv)</span>
<span id="cb2-349"><a href="#cb2-349" aria-hidden="true" tabindex="-1"></a>    non   <span class="op">=</span> pd.read_csv(nonbasic_csv)</span>
<span id="cb2-350"><a href="#cb2-350" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-351"><a href="#cb2-351" aria-hidden="true" tabindex="-1"></a>    gb <span class="op">=</span> basic[value_col].astype(<span class="bu">float</span>).replace([np.inf, <span class="op">-</span>np.inf], np.nan).dropna()</span>
<span id="cb2-352"><a href="#cb2-352" aria-hidden="true" tabindex="-1"></a>    gn <span class="op">=</span> non[value_col].astype(<span class="bu">float</span>).replace([np.inf, <span class="op">-</span>np.inf], np.nan).dropna()</span>
<span id="cb2-353"><a href="#cb2-353" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-354"><a href="#cb2-354" aria-hidden="true" tabindex="-1"></a>    nb, nn <span class="op">=</span> <span class="bu">len</span>(gb), <span class="bu">len</span>(gn)</span>
<span id="cb2-355"><a href="#cb2-355" aria-hidden="true" tabindex="-1"></a>    mb, mn <span class="op">=</span> gb.mean(), gn.mean()</span>
<span id="cb2-356"><a href="#cb2-356" aria-hidden="true" tabindex="-1"></a>    sb, sn <span class="op">=</span> gb.std(ddof<span class="op">=</span><span class="dv">1</span>), gn.std(ddof<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb2-357"><a href="#cb2-357" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-358"><a href="#cb2-358" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Welch's t-test (unequal variances)</span></span>
<span id="cb2-359"><a href="#cb2-359" aria-hidden="true" tabindex="-1"></a>    tstat, pval <span class="op">=</span> stats.ttest_ind(gb, gn, equal_var<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb2-360"><a href="#cb2-360" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-361"><a href="#cb2-361" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Cohen's d (pooled SD with ddof=1)</span></span>
<span id="cb2-362"><a href="#cb2-362" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> cohens_d(a, b):</span>
<span id="cb2-363"><a href="#cb2-363" aria-hidden="true" tabindex="-1"></a>        a, b <span class="op">=</span> np.asarray(a), np.asarray(b)</span>
<span id="cb2-364"><a href="#cb2-364" aria-hidden="true" tabindex="-1"></a>        sa, sb <span class="op">=</span> a.std(ddof<span class="op">=</span><span class="dv">1</span>), b.std(ddof<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb2-365"><a href="#cb2-365" aria-hidden="true" tabindex="-1"></a>        sp <span class="op">=</span> np.sqrt(((<span class="bu">len</span>(a)<span class="op">-</span><span class="dv">1</span>)<span class="op">*</span>sa<span class="op">**</span><span class="dv">2</span> <span class="op">+</span> (<span class="bu">len</span>(b)<span class="op">-</span><span class="dv">1</span>)<span class="op">*</span>sb<span class="op">**</span><span class="dv">2</span>) <span class="op">/</span> (<span class="bu">len</span>(a)<span class="op">+</span><span class="bu">len</span>(b)<span class="op">-</span><span class="dv">2</span>))</span>
<span id="cb2-366"><a href="#cb2-366" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> (a.mean() <span class="op">-</span> b.mean()) <span class="op">/</span> sp</span>
<span id="cb2-367"><a href="#cb2-367" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-368"><a href="#cb2-368" aria-hidden="true" tabindex="-1"></a>    d <span class="op">=</span> cohens_d(gb, gn)</span>
<span id="cb2-369"><a href="#cb2-369" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-370"><a href="#cb2-370" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Mann–Whitney U </span></span>
<span id="cb2-371"><a href="#cb2-371" aria-hidden="true" tabindex="-1"></a>    u, p_u <span class="op">=</span> stats.mannwhitneyu(gb, gn, alternative<span class="op">=</span><span class="st">"two-sided"</span>)</span>
<span id="cb2-372"><a href="#cb2-372" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-373"><a href="#cb2-373" aria-hidden="true" tabindex="-1"></a>    <span class="co"># print(f"Samples: basic n={nb}, nonbasic n={nn}")</span></span>
<span id="cb2-374"><a href="#cb2-374" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Basic=</span><span class="sc">{</span>mb<span class="sc">:.4f}</span><span class="ss"> (+-</span><span class="sc">{</span>sb<span class="sc">:.4f}</span><span class="ss">), nonbasic=</span><span class="sc">{</span>mn<span class="sc">:.4f}</span><span class="ss">(+-</span><span class="sc">{</span>sn<span class="sc">:.4f}</span><span class="ss">)"</span>)</span>
<span id="cb2-375"><a href="#cb2-375" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Welch's t-test (unequal variances)"</span>)</span>
<span id="cb2-376"><a href="#cb2-376" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"t = </span><span class="sc">{</span>tstat<span class="sc">:.3f}</span><span class="ss">, p = </span><span class="sc">{</span>pval<span class="sc">:.3g}</span><span class="ss">"</span>)</span>
<span id="cb2-377"><a href="#cb2-377" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">Cohen's d = </span><span class="sc">{</span>d<span class="sc">:.3f}</span><span class="ss">  (positive ⇒ basic &gt; nonbasic)"</span>)</span>
<span id="cb2-378"><a href="#cb2-378" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Mann-Whitney U (robustness)"</span>)</span>
<span id="cb2-379"><a href="#cb2-379" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"U = </span><span class="sc">{</span>u<span class="sc">:.0f}</span><span class="ss">, p = </span><span class="sc">{</span>p_u<span class="sc">:.3g}</span><span class="ss">"</span>)</span>
<span id="cb2-380"><a href="#cb2-380" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-381"><a href="#cb2-381" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 95% CI</span></span>
<span id="cb2-382"><a href="#cb2-382" aria-hidden="true" tabindex="-1"></a>    diff <span class="op">=</span> mb <span class="op">-</span> mn</span>
<span id="cb2-383"><a href="#cb2-383" aria-hidden="true" tabindex="-1"></a>    se <span class="op">=</span> np.sqrt(sb<span class="op">**</span><span class="dv">2</span><span class="op">/</span>nb <span class="op">+</span> sn<span class="op">**</span><span class="dv">2</span><span class="op">/</span>nn)</span>
<span id="cb2-384"><a href="#cb2-384" aria-hidden="true" tabindex="-1"></a>    df <span class="op">=</span> (sb<span class="op">**</span><span class="dv">2</span><span class="op">/</span>nb <span class="op">+</span> sn<span class="op">**</span><span class="dv">2</span><span class="op">/</span>nn)<span class="op">**</span><span class="dv">2</span> <span class="op">/</span> ((sb<span class="op">**</span><span class="dv">2</span><span class="op">/</span>nb)<span class="op">**</span><span class="dv">2</span><span class="op">/</span>(nb<span class="op">-</span><span class="dv">1</span>) <span class="op">+</span> (sn<span class="op">**</span><span class="dv">2</span><span class="op">/</span>nn)<span class="op">**</span><span class="dv">2</span><span class="op">/</span>(nn<span class="op">-</span><span class="dv">1</span>))</span>
<span id="cb2-385"><a href="#cb2-385" aria-hidden="true" tabindex="-1"></a>    tcrit <span class="op">=</span> stats.t.ppf(<span class="fl">0.975</span>, df)</span>
<span id="cb2-386"><a href="#cb2-386" aria-hidden="true" tabindex="-1"></a>    lo, hi <span class="op">=</span> diff <span class="op">-</span> tcrit<span class="op">*</span>se, diff <span class="op">+</span> tcrit<span class="op">*</span>se</span>
<span id="cb2-387"><a href="#cb2-387" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">95% CI for (basic - nonbasic) mean difference: [</span><span class="sc">{</span>lo<span class="sc">:.4f}</span><span class="ss">, </span><span class="sc">{</span>hi<span class="sc">:.4f}</span><span class="ss">]"</span>)</span>
<span id="cb2-388"><a href="#cb2-388" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-389"><a href="#cb2-389" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> plot_violin_pairs(</span>
<span id="cb2-390"><a href="#cb2-390" aria-hidden="true" tabindex="-1"></a>    pairs,</span>
<span id="cb2-391"><a href="#cb2-391" aria-hidden="true" tabindex="-1"></a>    value_col<span class="op">=</span><span class="st">"mean_log_freq"</span>,</span>
<span id="cb2-392"><a href="#cb2-392" aria-hidden="true" tabindex="-1"></a>    pair_labels<span class="op">=</span><span class="va">None</span>,</span>
<span id="cb2-393"><a href="#cb2-393" aria-hidden="true" tabindex="-1"></a>    transform<span class="op">=</span><span class="st">"yes"</span>,</span>
<span id="cb2-394"><a href="#cb2-394" aria-hidden="true" tabindex="-1"></a>    yscale<span class="op">=</span><span class="st">"log"</span>,</span>
<span id="cb2-395"><a href="#cb2-395" aria-hidden="true" tabindex="-1"></a>    title<span class="op">=</span><span class="st">"Group comparison"</span>,</span>
<span id="cb2-396"><a href="#cb2-396" aria-hidden="true" tabindex="-1"></a>):</span>
<span id="cb2-397"><a href="#cb2-397" aria-hidden="true" tabindex="-1"></a>    data <span class="op">=</span> []</span>
<span id="cb2-398"><a href="#cb2-398" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> b_csv, n_csv <span class="kw">in</span> pairs:</span>
<span id="cb2-399"><a href="#cb2-399" aria-hidden="true" tabindex="-1"></a>        b <span class="op">=</span> pd.read_csv(b_csv)[value_col].astype(<span class="bu">float</span>).replace([np.inf,<span class="op">-</span>np.inf], np.nan).dropna().values</span>
<span id="cb2-400"><a href="#cb2-400" aria-hidden="true" tabindex="-1"></a>        n <span class="op">=</span> pd.read_csv(n_csv)[value_col].astype(<span class="bu">float</span>).replace([np.inf,<span class="op">-</span>np.inf], np.nan).dropna().values</span>
<span id="cb2-401"><a href="#cb2-401" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> transform <span class="op">==</span> <span class="st">"yes"</span>:</span>
<span id="cb2-402"><a href="#cb2-402" aria-hidden="true" tabindex="-1"></a>            b_plot <span class="op">=</span> (<span class="dv">10</span><span class="op">**</span>b) <span class="op">*</span> <span class="fl">1_000_000.0</span></span>
<span id="cb2-403"><a href="#cb2-403" aria-hidden="true" tabindex="-1"></a>            n_plot <span class="op">=</span> (<span class="dv">10</span><span class="op">**</span>n) <span class="op">*</span> <span class="fl">1_000_000.0</span></span>
<span id="cb2-404"><a href="#cb2-404" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>:</span>
<span id="cb2-405"><a href="#cb2-405" aria-hidden="true" tabindex="-1"></a>            b_plot, n_plot <span class="op">=</span> b, n</span>
<span id="cb2-406"><a href="#cb2-406" aria-hidden="true" tabindex="-1"></a>        data.append((b, n, b_plot, n_plot))</span>
<span id="cb2-407"><a href="#cb2-407" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-408"><a href="#cb2-408" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> pair_labels <span class="kw">is</span> <span class="va">None</span>:</span>
<span id="cb2-409"><a href="#cb2-409" aria-hidden="true" tabindex="-1"></a>        pair_labels <span class="op">=</span> [<span class="ss">f"Set </span><span class="sc">{</span>i<span class="op">+</span><span class="dv">1</span><span class="sc">}</span><span class="ss">"</span> <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(data))]</span>
<span id="cb2-410"><a href="#cb2-410" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-411"><a href="#cb2-411" aria-hidden="true" tabindex="-1"></a>    fig, axes <span class="op">=</span> plt.subplots(<span class="dv">1</span>, <span class="bu">len</span>(data), figsize<span class="op">=</span>(<span class="dv">4</span><span class="op">*</span><span class="bu">len</span>(data), <span class="fl">4.5</span>), dpi<span class="op">=</span><span class="dv">150</span>, sharey<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb2-412"><a href="#cb2-412" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> <span class="bu">len</span>(data) <span class="op">==</span> <span class="dv">1</span>:</span>
<span id="cb2-413"><a href="#cb2-413" aria-hidden="true" tabindex="-1"></a>        axes <span class="op">=</span> [axes]</span>
<span id="cb2-414"><a href="#cb2-414" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-415"><a href="#cb2-415" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> ax, (lbl, (b_log, n_log, b_plot, n_plot)) <span class="kw">in</span> <span class="bu">zip</span>(axes, <span class="bu">zip</span>(pair_labels, data)):</span>
<span id="cb2-416"><a href="#cb2-416" aria-hidden="true" tabindex="-1"></a>        ax.violinplot([b_plot, n_plot], positions<span class="op">=</span>[<span class="dv">1</span>,<span class="dv">2</span>], showmeans<span class="op">=</span><span class="va">False</span>, showmedians<span class="op">=</span><span class="va">False</span>, showextrema<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb2-417"><a href="#cb2-417" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-418"><a href="#cb2-418" aria-hidden="true" tabindex="-1"></a>        med_b, med_n <span class="op">=</span> np.median(b_plot), np.median(n_plot)</span>
<span id="cb2-419"><a href="#cb2-419" aria-hidden="true" tabindex="-1"></a>        ax.hlines([med_b, med_n], [<span class="fl">0.85</span>, <span class="fl">1.85</span>], [<span class="fl">1.15</span>, <span class="fl">2.15</span>], colors<span class="op">=</span><span class="st">"blue"</span>, linewidth<span class="op">=</span><span class="dv">1</span>, label<span class="op">=</span><span class="st">"Median"</span>)</span>
<span id="cb2-420"><a href="#cb2-420" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-421"><a href="#cb2-421" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> transform <span class="op">==</span> <span class="st">"yes"</span>:</span>
<span id="cb2-422"><a href="#cb2-422" aria-hidden="true" tabindex="-1"></a>            g_b <span class="op">=</span> <span class="dv">10</span><span class="op">**</span>(b_log.mean()) <span class="op">*</span> <span class="dv">1000000</span></span>
<span id="cb2-423"><a href="#cb2-423" aria-hidden="true" tabindex="-1"></a>            g_n <span class="op">=</span> <span class="dv">10</span><span class="op">**</span>(n_log.mean()) <span class="op">*</span> <span class="dv">1000000</span></span>
<span id="cb2-424"><a href="#cb2-424" aria-hidden="true" tabindex="-1"></a>            ax.hlines([g_b, g_n], [<span class="fl">0.85</span>, <span class="fl">1.85</span>], [<span class="fl">1.15</span>, <span class="fl">2.15</span>], colors<span class="op">=</span><span class="st">"orange"</span>, linewidth<span class="op">=</span><span class="dv">1</span>, label<span class="op">=</span><span class="st">"Geometric mean"</span>)</span>
<span id="cb2-425"><a href="#cb2-425" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-426"><a href="#cb2-426" aria-hidden="true" tabindex="-1"></a>        ax.set_yscale(yscale)</span>
<span id="cb2-427"><a href="#cb2-427" aria-hidden="true" tabindex="-1"></a>        ax.set_xticks([<span class="dv">1</span>,<span class="dv">2</span>])<span class="op">;</span> ax.set_xticklabels([<span class="st">"Basic"</span>,<span class="st">"Non-basic"</span>])</span>
<span id="cb2-428"><a href="#cb2-428" aria-hidden="true" tabindex="-1"></a>        ax.set_title(lbl)</span>
<span id="cb2-429"><a href="#cb2-429" aria-hidden="true" tabindex="-1"></a>        ax.grid(axis<span class="op">=</span><span class="st">'y'</span>, which<span class="op">=</span><span class="st">'both'</span>, linestyle<span class="op">=</span><span class="st">':'</span>, linewidth<span class="op">=</span><span class="fl">0.6</span>, alpha<span class="op">=</span><span class="fl">0.7</span>)</span>
<span id="cb2-430"><a href="#cb2-430" aria-hidden="true" tabindex="-1"></a>        a_b, a_n <span class="op">=</span> b_plot.mean(), n_plot.mean()</span>
<span id="cb2-431"><a href="#cb2-431" aria-hidden="true" tabindex="-1"></a>        <span class="co"># print(a_b, a_n)</span></span>
<span id="cb2-432"><a href="#cb2-432" aria-hidden="true" tabindex="-1"></a>        ax.hlines([a_b, a_n], [<span class="fl">0.85</span>, <span class="fl">1.85</span>], [<span class="fl">1.15</span>, <span class="fl">2.15</span>], colors<span class="op">=</span><span class="st">"black"</span>, linewidth<span class="op">=</span><span class="dv">1</span>, linestyle<span class="op">=</span><span class="st">"--"</span>, label<span class="op">=</span><span class="st">"Arithmetic mean"</span>)</span>
<span id="cb2-433"><a href="#cb2-433" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-434"><a href="#cb2-434" aria-hidden="true" tabindex="-1"></a>    fig.supylabel(<span class="st">"Mean length"</span> <span class="cf">if</span> transform <span class="kw">is</span> <span class="va">None</span> <span class="cf">else</span> (<span class="st">"Frequency per million"</span> <span class="op">+</span> (<span class="st">" (log scale)"</span> <span class="cf">if</span> yscale<span class="op">==</span><span class="st">"log"</span> <span class="cf">else</span> <span class="st">""</span>)))</span>
<span id="cb2-435"><a href="#cb2-435" aria-hidden="true" tabindex="-1"></a>    fig.suptitle(title, y<span class="op">=</span><span class="fl">1.02</span>, fontsize<span class="op">=</span><span class="dv">11</span>)</span>
<span id="cb2-436"><a href="#cb2-436" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-437"><a href="#cb2-437" aria-hidden="true" tabindex="-1"></a>    handles, labels <span class="op">=</span> axes[<span class="dv">0</span>].get_legend_handles_labels()</span>
<span id="cb2-438"><a href="#cb2-438" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> handles:</span>
<span id="cb2-439"><a href="#cb2-439" aria-hidden="true" tabindex="-1"></a>        fig.legend(handles, labels, loc<span class="op">=</span><span class="st">"upper right"</span>, frameon<span class="op">=</span><span class="va">False</span>, fontsize<span class="op">=</span><span class="dv">8</span>)</span>
<span id="cb2-440"><a href="#cb2-440" aria-hidden="true" tabindex="-1"></a>    fig.tight_layout()</span>
<span id="cb2-441"><a href="#cb2-441" aria-hidden="true" tabindex="-1"></a>    plt.show()</span>
<span id="cb2-442"><a href="#cb2-442" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-443"><a href="#cb2-443" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-444"><a href="#cb2-444" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> capture_function_output(func, <span class="op">*</span>args, <span class="op">**</span>kwargs):</span>
<span id="cb2-445"><a href="#cb2-445" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Capture print output from a function. This function is necessary for rendering pretty collapsible sections."""</span></span>
<span id="cb2-446"><a href="#cb2-446" aria-hidden="true" tabindex="-1"></a>    old_stdout <span class="op">=</span> sys.stdout</span>
<span id="cb2-447"><a href="#cb2-447" aria-hidden="true" tabindex="-1"></a>    sys.stdout <span class="op">=</span> mystdout <span class="op">=</span> io.StringIO()</span>
<span id="cb2-448"><a href="#cb2-448" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb2-449"><a href="#cb2-449" aria-hidden="true" tabindex="-1"></a>    <span class="cf">try</span>:</span>
<span id="cb2-450"><a href="#cb2-450" aria-hidden="true" tabindex="-1"></a>        func(<span class="op">*</span>args, <span class="op">**</span>kwargs)</span>
<span id="cb2-451"><a href="#cb2-451" aria-hidden="true" tabindex="-1"></a>        output <span class="op">=</span> mystdout.getvalue()</span>
<span id="cb2-452"><a href="#cb2-452" aria-hidden="true" tabindex="-1"></a>    <span class="cf">finally</span>:</span>
<span id="cb2-453"><a href="#cb2-453" aria-hidden="true" tabindex="-1"></a>        sys.stdout <span class="op">=</span> old_stdout</span>
<span id="cb2-454"><a href="#cb2-454" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb2-455"><a href="#cb2-455" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> output</span>
<span id="cb2-456"><a href="#cb2-456" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-457"><a href="#cb2-457" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-458"><a href="#cb2-458" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> create_venn_diagrams(base_path<span class="op">=</span><span class="st">"results/wordnet"</span>):</span>
<span id="cb2-459"><a href="#cb2-459" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Create Venn diagrams for word overlaps, also report the words for each section"""</span></span>
<span id="cb2-460"><a href="#cb2-460" aria-hidden="true" tabindex="-1"></a>    datasets <span class="op">=</span> [<span class="st">'books'</span>, <span class="st">'subtitles'</span>, <span class="st">'web'</span>]</span>
<span id="cb2-461"><a href="#cb2-461" aria-hidden="true" tabindex="-1"></a>    categories <span class="op">=</span> [<span class="st">'top100'</span>, <span class="st">'bottom100'</span>]</span>
<span id="cb2-462"><a href="#cb2-462" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb2-463"><a href="#cb2-463" aria-hidden="true" tabindex="-1"></a>    data <span class="op">=</span> {}</span>
<span id="cb2-464"><a href="#cb2-464" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> dataset <span class="kw">in</span> datasets:</span>
<span id="cb2-465"><a href="#cb2-465" aria-hidden="true" tabindex="-1"></a>        data[dataset] <span class="op">=</span> {}</span>
<span id="cb2-466"><a href="#cb2-466" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> category <span class="kw">in</span> categories:</span>
<span id="cb2-467"><a href="#cb2-467" aria-hidden="true" tabindex="-1"></a>            file_path <span class="op">=</span> os.path.join(base_path, dataset, <span class="st">'all_100'</span>, <span class="ss">f'</span><span class="sc">{</span>category<span class="sc">}</span><span class="ss">_semcor.csv'</span>)</span>
<span id="cb2-468"><a href="#cb2-468" aria-hidden="true" tabindex="-1"></a>            df <span class="op">=</span> pd.read_csv(file_path)</span>
<span id="cb2-469"><a href="#cb2-469" aria-hidden="true" tabindex="-1"></a>            data[dataset][category] <span class="op">=</span> <span class="bu">set</span>(df[<span class="st">'lemma'</span>].tolist())</span>
<span id="cb2-470"><a href="#cb2-470" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-471"><a href="#cb2-471" aria-hidden="true" tabindex="-1"></a>    fig, axes <span class="op">=</span> plt.subplots(<span class="dv">2</span>, <span class="dv">1</span>, figsize<span class="op">=</span>(<span class="dv">20</span>, <span class="dv">16</span>))</span>
<span id="cb2-472"><a href="#cb2-472" aria-hidden="true" tabindex="-1"></a>    <span class="co"># fig.suptitle('Word Overlap Analysis - Venn Diagrams', fontsize=16, fontweight='bold')</span></span>
<span id="cb2-473"><a href="#cb2-473" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb2-474"><a href="#cb2-474" aria-hidden="true" tabindex="-1"></a>    ax1 <span class="op">=</span> axes[<span class="dv">0</span>]</span>
<span id="cb2-475"><a href="#cb2-475" aria-hidden="true" tabindex="-1"></a>    books_top <span class="op">=</span> data[<span class="st">'books'</span>][<span class="st">'top100'</span>]</span>
<span id="cb2-476"><a href="#cb2-476" aria-hidden="true" tabindex="-1"></a>    subtitles_top <span class="op">=</span> data[<span class="st">'subtitles'</span>][<span class="st">'top100'</span>]</span>
<span id="cb2-477"><a href="#cb2-477" aria-hidden="true" tabindex="-1"></a>    web_top <span class="op">=</span> data[<span class="st">'web'</span>][<span class="st">'top100'</span>]</span>
<span id="cb2-478"><a href="#cb2-478" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb2-479"><a href="#cb2-479" aria-hidden="true" tabindex="-1"></a>    venn3([books_top, subtitles_top, web_top], </span>
<span id="cb2-480"><a href="#cb2-480" aria-hidden="true" tabindex="-1"></a>            (<span class="st">'Books'</span>, <span class="st">'Subtitles'</span>, <span class="st">'Web'</span>), ax<span class="op">=</span>ax1)</span>
<span id="cb2-481"><a href="#cb2-481" aria-hidden="true" tabindex="-1"></a>    ax1.set_title(<span class="st">'Top 100 Words'</span>, fontweight<span class="op">=</span><span class="st">'bold'</span>)</span>
<span id="cb2-482"><a href="#cb2-482" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb2-483"><a href="#cb2-483" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"TOP 100 WORDS:"</span>)</span>
<span id="cb2-484"><a href="#cb2-484" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb2-485"><a href="#cb2-485" aria-hidden="true" tabindex="-1"></a>    all_three <span class="op">=</span> books_top.intersection(subtitles_top).intersection(web_top)</span>
<span id="cb2-486"><a href="#cb2-486" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">Words in ALL THREE datasets (</span><span class="sc">{</span><span class="bu">len</span>(all_three)<span class="sc">}</span><span class="ss"> words):"</span>)</span>
<span id="cb2-487"><a href="#cb2-487" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> all_three:</span>
<span id="cb2-488"><a href="#cb2-488" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"  </span><span class="sc">{</span><span class="st">', '</span><span class="sc">.</span>join(<span class="bu">sorted</span>(<span class="bu">list</span>(all_three)))<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb2-489"><a href="#cb2-489" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb2-490"><a href="#cb2-490" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="st">"  None"</span>)</span>
<span id="cb2-491"><a href="#cb2-491" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb2-492"><a href="#cb2-492" aria-hidden="true" tabindex="-1"></a>    books_subtitles_only <span class="op">=</span> books_top.intersection(subtitles_top) <span class="op">-</span> web_top</span>
<span id="cb2-493"><a href="#cb2-493" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">Words in BOOKS &amp; SUBTITLES only (</span><span class="sc">{</span><span class="bu">len</span>(books_subtitles_only)<span class="sc">}</span><span class="ss"> words):"</span>)</span>
<span id="cb2-494"><a href="#cb2-494" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> books_subtitles_only:</span>
<span id="cb2-495"><a href="#cb2-495" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"  </span><span class="sc">{</span><span class="st">', '</span><span class="sc">.</span>join(<span class="bu">sorted</span>(<span class="bu">list</span>(books_subtitles_only)))<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb2-496"><a href="#cb2-496" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb2-497"><a href="#cb2-497" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="st">"  None"</span>)</span>
<span id="cb2-498"><a href="#cb2-498" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb2-499"><a href="#cb2-499" aria-hidden="true" tabindex="-1"></a>    books_web_only <span class="op">=</span> books_top.intersection(web_top) <span class="op">-</span> subtitles_top</span>
<span id="cb2-500"><a href="#cb2-500" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">Words in BOOKS &amp; WEB only (</span><span class="sc">{</span><span class="bu">len</span>(books_web_only)<span class="sc">}</span><span class="ss"> words):"</span>)</span>
<span id="cb2-501"><a href="#cb2-501" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> books_web_only:</span>
<span id="cb2-502"><a href="#cb2-502" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"  </span><span class="sc">{</span><span class="st">', '</span><span class="sc">.</span>join(<span class="bu">sorted</span>(<span class="bu">list</span>(books_web_only)))<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb2-503"><a href="#cb2-503" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb2-504"><a href="#cb2-504" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="st">"  None"</span>)</span>
<span id="cb2-505"><a href="#cb2-505" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb2-506"><a href="#cb2-506" aria-hidden="true" tabindex="-1"></a>    subtitles_web_only <span class="op">=</span> subtitles_top.intersection(web_top) <span class="op">-</span> books_top</span>
<span id="cb2-507"><a href="#cb2-507" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">Words in SUBTITLES &amp; WEB only (</span><span class="sc">{</span><span class="bu">len</span>(subtitles_web_only)<span class="sc">}</span><span class="ss"> words):"</span>)</span>
<span id="cb2-508"><a href="#cb2-508" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> subtitles_web_only:</span>
<span id="cb2-509"><a href="#cb2-509" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"  </span><span class="sc">{</span><span class="st">', '</span><span class="sc">.</span>join(<span class="bu">sorted</span>(<span class="bu">list</span>(subtitles_web_only)))<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb2-510"><a href="#cb2-510" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb2-511"><a href="#cb2-511" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="st">"  None"</span>)</span>
<span id="cb2-512"><a href="#cb2-512" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb2-513"><a href="#cb2-513" aria-hidden="true" tabindex="-1"></a>    books_only <span class="op">=</span> books_top <span class="op">-</span> subtitles_top <span class="op">-</span> web_top</span>
<span id="cb2-514"><a href="#cb2-514" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">Words in BOOKS only (</span><span class="sc">{</span><span class="bu">len</span>(books_only)<span class="sc">}</span><span class="ss"> words):"</span>)</span>
<span id="cb2-515"><a href="#cb2-515" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> books_only:</span>
<span id="cb2-516"><a href="#cb2-516" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"  </span><span class="sc">{</span><span class="st">', '</span><span class="sc">.</span>join(<span class="bu">sorted</span>(<span class="bu">list</span>(books_only)))<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb2-517"><a href="#cb2-517" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb2-518"><a href="#cb2-518" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="st">"  None"</span>)</span>
<span id="cb2-519"><a href="#cb2-519" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb2-520"><a href="#cb2-520" aria-hidden="true" tabindex="-1"></a>    subtitles_only <span class="op">=</span> subtitles_top <span class="op">-</span> books_top <span class="op">-</span> web_top</span>
<span id="cb2-521"><a href="#cb2-521" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">Words in SUBTITLES only (</span><span class="sc">{</span><span class="bu">len</span>(subtitles_only)<span class="sc">}</span><span class="ss"> words):"</span>)</span>
<span id="cb2-522"><a href="#cb2-522" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> subtitles_only:</span>
<span id="cb2-523"><a href="#cb2-523" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"  </span><span class="sc">{</span><span class="st">', '</span><span class="sc">.</span>join(<span class="bu">sorted</span>(<span class="bu">list</span>(subtitles_only)))<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb2-524"><a href="#cb2-524" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb2-525"><a href="#cb2-525" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="st">"  None"</span>)</span>
<span id="cb2-526"><a href="#cb2-526" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb2-527"><a href="#cb2-527" aria-hidden="true" tabindex="-1"></a>    web_only <span class="op">=</span> web_top <span class="op">-</span> books_top <span class="op">-</span> subtitles_top</span>
<span id="cb2-528"><a href="#cb2-528" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">Words in WEB only (</span><span class="sc">{</span><span class="bu">len</span>(web_only)<span class="sc">}</span><span class="ss"> words):"</span>)</span>
<span id="cb2-529"><a href="#cb2-529" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> web_only:</span>
<span id="cb2-530"><a href="#cb2-530" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"  </span><span class="sc">{</span><span class="st">', '</span><span class="sc">.</span>join(<span class="bu">sorted</span>(<span class="bu">list</span>(web_only)))<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb2-531"><a href="#cb2-531" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb2-532"><a href="#cb2-532" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="st">"  None"</span>)</span>
<span id="cb2-533"><a href="#cb2-533" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb2-534"><a href="#cb2-534" aria-hidden="true" tabindex="-1"></a>    ax2 <span class="op">=</span> axes[<span class="dv">1</span>]</span>
<span id="cb2-535"><a href="#cb2-535" aria-hidden="true" tabindex="-1"></a>    books_bottom <span class="op">=</span> data[<span class="st">'books'</span>][<span class="st">'bottom100'</span>]</span>
<span id="cb2-536"><a href="#cb2-536" aria-hidden="true" tabindex="-1"></a>    subtitles_bottom <span class="op">=</span> data[<span class="st">'subtitles'</span>][<span class="st">'bottom100'</span>]</span>
<span id="cb2-537"><a href="#cb2-537" aria-hidden="true" tabindex="-1"></a>    web_bottom <span class="op">=</span> data[<span class="st">'web'</span>][<span class="st">'bottom100'</span>]</span>
<span id="cb2-538"><a href="#cb2-538" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb2-539"><a href="#cb2-539" aria-hidden="true" tabindex="-1"></a>    venn3([books_bottom, subtitles_bottom, web_bottom], </span>
<span id="cb2-540"><a href="#cb2-540" aria-hidden="true" tabindex="-1"></a>            (<span class="st">'Books'</span>, <span class="st">'Subtitles'</span>, <span class="st">'Web'</span>), ax<span class="op">=</span>ax2)</span>
<span id="cb2-541"><a href="#cb2-541" aria-hidden="true" tabindex="-1"></a>    ax2.set_title(<span class="st">'Bottom 100 Words'</span>, fontweight<span class="op">=</span><span class="st">'bold'</span>)</span>
<span id="cb2-542"><a href="#cb2-542" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb2-543"><a href="#cb2-543" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"</span><span class="ch">\n\n</span><span class="st">BOTTOM 100 WORDS:"</span>)</span>
<span id="cb2-544"><a href="#cb2-544" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"="</span><span class="op">*</span><span class="dv">50</span>)</span>
<span id="cb2-545"><a href="#cb2-545" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb2-546"><a href="#cb2-546" aria-hidden="true" tabindex="-1"></a>    all_three <span class="op">=</span> books_bottom.intersection(subtitles_bottom).intersection(web_bottom)</span>
<span id="cb2-547"><a href="#cb2-547" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">Words in ALL THREE datasets (</span><span class="sc">{</span><span class="bu">len</span>(all_three)<span class="sc">}</span><span class="ss"> words):"</span>)</span>
<span id="cb2-548"><a href="#cb2-548" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> all_three:</span>
<span id="cb2-549"><a href="#cb2-549" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"  </span><span class="sc">{</span><span class="st">', '</span><span class="sc">.</span>join(<span class="bu">sorted</span>(<span class="bu">list</span>(all_three)))<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb2-550"><a href="#cb2-550" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb2-551"><a href="#cb2-551" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="st">"  None"</span>)</span>
<span id="cb2-552"><a href="#cb2-552" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb2-553"><a href="#cb2-553" aria-hidden="true" tabindex="-1"></a>    books_subtitles_only <span class="op">=</span> books_bottom.intersection(subtitles_bottom) <span class="op">-</span> web_bottom</span>
<span id="cb2-554"><a href="#cb2-554" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">Words in BOOKS &amp; SUBTITLES only (</span><span class="sc">{</span><span class="bu">len</span>(books_subtitles_only)<span class="sc">}</span><span class="ss"> words):"</span>)</span>
<span id="cb2-555"><a href="#cb2-555" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> books_subtitles_only:</span>
<span id="cb2-556"><a href="#cb2-556" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"  </span><span class="sc">{</span><span class="st">', '</span><span class="sc">.</span>join(<span class="bu">sorted</span>(<span class="bu">list</span>(books_subtitles_only)))<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb2-557"><a href="#cb2-557" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb2-558"><a href="#cb2-558" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="st">"  None"</span>)</span>
<span id="cb2-559"><a href="#cb2-559" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb2-560"><a href="#cb2-560" aria-hidden="true" tabindex="-1"></a>    books_web_only <span class="op">=</span> books_bottom.intersection(web_bottom) <span class="op">-</span> subtitles_bottom</span>
<span id="cb2-561"><a href="#cb2-561" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">Words in BOOKS &amp; WEB only (</span><span class="sc">{</span><span class="bu">len</span>(books_web_only)<span class="sc">}</span><span class="ss"> words):"</span>)</span>
<span id="cb2-562"><a href="#cb2-562" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> books_web_only:</span>
<span id="cb2-563"><a href="#cb2-563" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"  </span><span class="sc">{</span><span class="st">', '</span><span class="sc">.</span>join(<span class="bu">sorted</span>(<span class="bu">list</span>(books_web_only)))<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb2-564"><a href="#cb2-564" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb2-565"><a href="#cb2-565" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="st">"  None"</span>)</span>
<span id="cb2-566"><a href="#cb2-566" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb2-567"><a href="#cb2-567" aria-hidden="true" tabindex="-1"></a>    subtitles_web_only <span class="op">=</span> subtitles_bottom.intersection(web_bottom) <span class="op">-</span> books_bottom</span>
<span id="cb2-568"><a href="#cb2-568" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">Words in SUBTITLES &amp; WEB only (</span><span class="sc">{</span><span class="bu">len</span>(subtitles_web_only)<span class="sc">}</span><span class="ss"> words):"</span>)</span>
<span id="cb2-569"><a href="#cb2-569" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> subtitles_web_only:</span>
<span id="cb2-570"><a href="#cb2-570" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"  </span><span class="sc">{</span><span class="st">', '</span><span class="sc">.</span>join(<span class="bu">sorted</span>(<span class="bu">list</span>(subtitles_web_only)))<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb2-571"><a href="#cb2-571" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb2-572"><a href="#cb2-572" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="st">"  None"</span>)</span>
<span id="cb2-573"><a href="#cb2-573" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb2-574"><a href="#cb2-574" aria-hidden="true" tabindex="-1"></a>    books_only <span class="op">=</span> books_bottom <span class="op">-</span> subtitles_bottom <span class="op">-</span> web_bottom</span>
<span id="cb2-575"><a href="#cb2-575" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">Words in BOOKS only (</span><span class="sc">{</span><span class="bu">len</span>(books_only)<span class="sc">}</span><span class="ss"> words):"</span>)</span>
<span id="cb2-576"><a href="#cb2-576" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> books_only:</span>
<span id="cb2-577"><a href="#cb2-577" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"  </span><span class="sc">{</span><span class="st">', '</span><span class="sc">.</span>join(<span class="bu">sorted</span>(<span class="bu">list</span>(books_only)))<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb2-578"><a href="#cb2-578" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb2-579"><a href="#cb2-579" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="st">"  None"</span>)</span>
<span id="cb2-580"><a href="#cb2-580" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb2-581"><a href="#cb2-581" aria-hidden="true" tabindex="-1"></a>    subtitles_only <span class="op">=</span> subtitles_bottom <span class="op">-</span> books_bottom <span class="op">-</span> web_bottom</span>
<span id="cb2-582"><a href="#cb2-582" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">Words in SUBTITLES only (</span><span class="sc">{</span><span class="bu">len</span>(subtitles_only)<span class="sc">}</span><span class="ss"> words):"</span>)</span>
<span id="cb2-583"><a href="#cb2-583" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> subtitles_only:</span>
<span id="cb2-584"><a href="#cb2-584" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"  </span><span class="sc">{</span><span class="st">', '</span><span class="sc">.</span>join(<span class="bu">sorted</span>(<span class="bu">list</span>(subtitles_only)))<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb2-585"><a href="#cb2-585" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb2-586"><a href="#cb2-586" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="st">"  None"</span>)</span>
<span id="cb2-587"><a href="#cb2-587" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb2-588"><a href="#cb2-588" aria-hidden="true" tabindex="-1"></a>    web_only <span class="op">=</span> web_bottom <span class="op">-</span> books_bottom <span class="op">-</span> subtitles_bottom</span>
<span id="cb2-589"><a href="#cb2-589" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">Words in WEB only (</span><span class="sc">{</span><span class="bu">len</span>(web_only)<span class="sc">}</span><span class="ss"> words):"</span>)</span>
<span id="cb2-590"><a href="#cb2-590" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> web_only:</span>
<span id="cb2-591"><a href="#cb2-591" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"  </span><span class="sc">{</span><span class="st">', '</span><span class="sc">.</span>join(<span class="bu">sorted</span>(<span class="bu">list</span>(web_only)))<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb2-592"><a href="#cb2-592" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb2-593"><a href="#cb2-593" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="st">"  None"</span>)</span>
<span id="cb2-594"><a href="#cb2-594" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb2-595"><a href="#cb2-595" aria-hidden="true" tabindex="-1"></a>    plt.tight_layout()</span>
<span id="cb2-596"><a href="#cb2-596" aria-hidden="true" tabindex="-1"></a>    plt.show()</span>
<span id="cb2-597"><a href="#cb2-597" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-598"><a href="#cb2-598" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-599"><a href="#cb2-599" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> get_skews_basic_and_non_basic(basic_csv, nonbasic_csv, value_col<span class="op">=</span><span class="st">"mean_log_freq"</span>):</span>
<span id="cb2-600"><a href="#cb2-600" aria-hidden="true" tabindex="-1"></a>    basic <span class="op">=</span> pd.read_csv(basic_csv)</span>
<span id="cb2-601"><a href="#cb2-601" aria-hidden="true" tabindex="-1"></a>    non   <span class="op">=</span> pd.read_csv(nonbasic_csv)</span>
<span id="cb2-602"><a href="#cb2-602" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-603"><a href="#cb2-603" aria-hidden="true" tabindex="-1"></a>    gb <span class="op">=</span> basic[value_col].astype(<span class="bu">float</span>).replace([np.inf, <span class="op">-</span>np.inf], np.nan).dropna()</span>
<span id="cb2-604"><a href="#cb2-604" aria-hidden="true" tabindex="-1"></a>    gn <span class="op">=</span> non[value_col].astype(<span class="bu">float</span>).replace([np.inf, <span class="op">-</span>np.inf], np.nan).dropna()</span>
<span id="cb2-605"><a href="#cb2-605" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-606"><a href="#cb2-606" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Back-transform to per-million frequencies</span></span>
<span id="cb2-607"><a href="#cb2-607" aria-hidden="true" tabindex="-1"></a>    basic_fpm <span class="op">=</span> (<span class="dv">10</span> <span class="op">**</span> gb.values) <span class="op">*</span> <span class="fl">1_000_000.0</span></span>
<span id="cb2-608"><a href="#cb2-608" aria-hidden="true" tabindex="-1"></a>    non_fpm   <span class="op">=</span> (<span class="dv">10</span> <span class="op">**</span> gn.values) <span class="op">*</span> <span class="fl">1_000_000.0</span></span>
<span id="cb2-609"><a href="#cb2-609" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-610"><a href="#cb2-610" aria-hidden="true" tabindex="-1"></a>    skew_basic <span class="op">=</span> stats.skew(basic_fpm)</span>
<span id="cb2-611"><a href="#cb2-611" aria-hidden="true" tabindex="-1"></a>    skew_non   <span class="op">=</span> stats.skew(non_fpm)</span>
<span id="cb2-612"><a href="#cb2-612" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> skew_basic, skew_non</span>
<span id="cb2-613"><a href="#cb2-613" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-614"><a href="#cb2-614" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-615"><a href="#cb2-615" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> full_run(top_n, bottom_n, file_name<span class="op">=</span><span class="st">"./data/ngram_freq.csv"</span>, folder<span class="op">=</span><span class="st">"./results/wordnet"</span>, pos<span class="op">=</span>wn.NOUN, source<span class="op">=</span><span class="st">"wordnet"</span>):</span>
<span id="cb2-616"><a href="#cb2-616" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb2-617"><a href="#cb2-617" aria-hidden="true" tabindex="-1"></a><span class="co">    Perform end-to-end analysis of input file with frequencies.</span></span>
<span id="cb2-618"><a href="#cb2-618" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb2-619"><a href="#cb2-619" aria-hidden="true" tabindex="-1"></a>    os.makedirs(folder, exist_ok<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb2-620"><a href="#cb2-620" aria-hidden="true" tabindex="-1"></a>    get_basic_and_nonbasic_terms(</span>
<span id="cb2-621"><a href="#cb2-621" aria-hidden="true" tabindex="-1"></a>        file_name<span class="op">=</span>file_name,</span>
<span id="cb2-622"><a href="#cb2-622" aria-hidden="true" tabindex="-1"></a>        merged_lemma_count_csv<span class="op">=</span><span class="ss">f"</span><span class="sc">{</span>folder<span class="sc">}</span><span class="ss">/merged_freqs.csv"</span>,</span>
<span id="cb2-623"><a href="#cb2-623" aria-hidden="true" tabindex="-1"></a>        output_top_csv<span class="op">=</span><span class="ss">f"</span><span class="sc">{</span>folder<span class="sc">}</span><span class="ss">/top</span><span class="sc">{</span>top_n<span class="sc">}</span><span class="ss">_semcor.csv"</span>,</span>
<span id="cb2-624"><a href="#cb2-624" aria-hidden="true" tabindex="-1"></a>        output_bottom_csv<span class="op">=</span><span class="ss">f"</span><span class="sc">{</span>folder<span class="sc">}</span><span class="ss">/bottom</span><span class="sc">{</span>bottom_n<span class="sc">}</span><span class="ss">_semcor.csv"</span>,</span>
<span id="cb2-625"><a href="#cb2-625" aria-hidden="true" tabindex="-1"></a>        min_count<span class="op">=</span><span class="dv">1</span>,</span>
<span id="cb2-626"><a href="#cb2-626" aria-hidden="true" tabindex="-1"></a>        top_n<span class="op">=</span>top_n,</span>
<span id="cb2-627"><a href="#cb2-627" aria-hidden="true" tabindex="-1"></a>        bottom_n<span class="op">=</span>bottom_n,</span>
<span id="cb2-628"><a href="#cb2-628" aria-hidden="true" tabindex="-1"></a>        pos<span class="op">=</span>pos,</span>
<span id="cb2-629"><a href="#cb2-629" aria-hidden="true" tabindex="-1"></a>        source<span class="op">=</span>source</span>
<span id="cb2-630"><a href="#cb2-630" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb2-631"><a href="#cb2-631" aria-hidden="true" tabindex="-1"></a>    write_most_used_definitions_csv(</span>
<span id="cb2-632"><a href="#cb2-632" aria-hidden="true" tabindex="-1"></a>        <span class="ss">f"</span><span class="sc">{</span>folder<span class="sc">}</span><span class="ss">/bottom</span><span class="sc">{</span>bottom_n<span class="sc">}</span><span class="ss">_semcor.csv"</span>,</span>
<span id="cb2-633"><a href="#cb2-633" aria-hidden="true" tabindex="-1"></a>        <span class="ss">f"</span><span class="sc">{</span>folder<span class="sc">}</span><span class="ss">/bottom</span><span class="sc">{</span>bottom_n<span class="sc">}</span><span class="ss">_semcor_defs.csv"</span>,</span>
<span id="cb2-634"><a href="#cb2-634" aria-hidden="true" tabindex="-1"></a>        pos<span class="op">=</span>pos,</span>
<span id="cb2-635"><a href="#cb2-635" aria-hidden="true" tabindex="-1"></a>        source<span class="op">=</span>source</span>
<span id="cb2-636"><a href="#cb2-636" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb2-637"><a href="#cb2-637" aria-hidden="true" tabindex="-1"></a>    write_most_used_definitions_csv(</span>
<span id="cb2-638"><a href="#cb2-638" aria-hidden="true" tabindex="-1"></a>        <span class="ss">f"</span><span class="sc">{</span>folder<span class="sc">}</span><span class="ss">/top</span><span class="sc">{</span>top_n<span class="sc">}</span><span class="ss">_semcor.csv"</span>,</span>
<span id="cb2-639"><a href="#cb2-639" aria-hidden="true" tabindex="-1"></a>        <span class="ss">f"</span><span class="sc">{</span>folder<span class="sc">}</span><span class="ss">/top</span><span class="sc">{</span>top_n<span class="sc">}</span><span class="ss">_semcor_defs.csv"</span>,</span>
<span id="cb2-640"><a href="#cb2-640" aria-hidden="true" tabindex="-1"></a>        pos<span class="op">=</span>pos,</span>
<span id="cb2-641"><a href="#cb2-641" aria-hidden="true" tabindex="-1"></a>        source<span class="op">=</span>source</span>
<span id="cb2-642"><a href="#cb2-642" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb2-643"><a href="#cb2-643" aria-hidden="true" tabindex="-1"></a>    attach_definition_frequencies(</span>
<span id="cb2-644"><a href="#cb2-644" aria-hidden="true" tabindex="-1"></a>        <span class="ss">f"</span><span class="sc">{</span>folder<span class="sc">}</span><span class="ss">/top</span><span class="sc">{</span>top_n<span class="sc">}</span><span class="ss">_semcor_defs.csv"</span>,</span>
<span id="cb2-645"><a href="#cb2-645" aria-hidden="true" tabindex="-1"></a>        output_csv<span class="op">=</span><span class="ss">f"</span><span class="sc">{</span>folder<span class="sc">}</span><span class="ss">/top</span><span class="sc">{</span>top_n<span class="sc">}</span><span class="ss">_defs_with_freq.csv"</span>,</span>
<span id="cb2-646"><a href="#cb2-646" aria-hidden="true" tabindex="-1"></a>        freq_csv<span class="op">=</span><span class="ss">f"</span><span class="sc">{</span>folder<span class="sc">}</span><span class="ss">/merged_freqs.csv"</span></span>
<span id="cb2-647"><a href="#cb2-647" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb2-648"><a href="#cb2-648" aria-hidden="true" tabindex="-1"></a>    attach_definition_frequencies(</span>
<span id="cb2-649"><a href="#cb2-649" aria-hidden="true" tabindex="-1"></a>        <span class="ss">f"</span><span class="sc">{</span>folder<span class="sc">}</span><span class="ss">/bottom</span><span class="sc">{</span>bottom_n<span class="sc">}</span><span class="ss">_semcor_defs.csv"</span>,</span>
<span id="cb2-650"><a href="#cb2-650" aria-hidden="true" tabindex="-1"></a>        output_csv<span class="op">=</span><span class="ss">f"</span><span class="sc">{</span>folder<span class="sc">}</span><span class="ss">/bottom</span><span class="sc">{</span>bottom_n<span class="sc">}</span><span class="ss">_defs_with_freq.csv"</span>,</span>
<span id="cb2-651"><a href="#cb2-651" aria-hidden="true" tabindex="-1"></a>        freq_csv<span class="op">=</span><span class="ss">f"</span><span class="sc">{</span>folder<span class="sc">}</span><span class="ss">/merged_freqs.csv"</span></span>
<span id="cb2-652"><a href="#cb2-652" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb2-653"><a href="#cb2-653" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"</span><span class="ch">\n\n</span><span class="st">Log-10 mean frequency of tokens in definitions"</span>)</span>
<span id="cb2-654"><a href="#cb2-654" aria-hidden="true" tabindex="-1"></a>    t_tests_on_definition(</span>
<span id="cb2-655"><a href="#cb2-655" aria-hidden="true" tabindex="-1"></a>        <span class="ss">f"</span><span class="sc">{</span>folder<span class="sc">}</span><span class="ss">/top</span><span class="sc">{</span>top_n<span class="sc">}</span><span class="ss">_defs_with_freq.csv"</span>,</span>
<span id="cb2-656"><a href="#cb2-656" aria-hidden="true" tabindex="-1"></a>        <span class="ss">f"</span><span class="sc">{</span>folder<span class="sc">}</span><span class="ss">/bottom</span><span class="sc">{</span>bottom_n<span class="sc">}</span><span class="ss">_defs_with_freq.csv"</span></span>
<span id="cb2-657"><a href="#cb2-657" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb2-658"><a href="#cb2-658" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"</span><span class="ch">\n\n</span><span class="st">Log-10 median frequency of tokens in definitions"</span>)</span>
<span id="cb2-659"><a href="#cb2-659" aria-hidden="true" tabindex="-1"></a>    t_tests_on_definition(</span>
<span id="cb2-660"><a href="#cb2-660" aria-hidden="true" tabindex="-1"></a>        <span class="ss">f"</span><span class="sc">{</span>folder<span class="sc">}</span><span class="ss">/top</span><span class="sc">{</span>top_n<span class="sc">}</span><span class="ss">_defs_with_freq.csv"</span>,</span>
<span id="cb2-661"><a href="#cb2-661" aria-hidden="true" tabindex="-1"></a>        <span class="ss">f"</span><span class="sc">{</span>folder<span class="sc">}</span><span class="ss">/bottom</span><span class="sc">{</span>bottom_n<span class="sc">}</span><span class="ss">_defs_with_freq.csv"</span>,</span>
<span id="cb2-662"><a href="#cb2-662" aria-hidden="true" tabindex="-1"></a>        value_col<span class="op">=</span><span class="st">"median_log_freq"</span></span>
<span id="cb2-663"><a href="#cb2-663" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb2-664"><a href="#cb2-664" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"</span><span class="ch">\n\n</span><span class="st">Length of definition in tokens"</span>)</span>
<span id="cb2-665"><a href="#cb2-665" aria-hidden="true" tabindex="-1"></a>    t_tests_on_definition(</span>
<span id="cb2-666"><a href="#cb2-666" aria-hidden="true" tabindex="-1"></a>        <span class="ss">f"</span><span class="sc">{</span>folder<span class="sc">}</span><span class="ss">/top</span><span class="sc">{</span>top_n<span class="sc">}</span><span class="ss">_defs_with_freq.csv"</span>,</span>
<span id="cb2-667"><a href="#cb2-667" aria-hidden="true" tabindex="-1"></a>        <span class="ss">f"</span><span class="sc">{</span>folder<span class="sc">}</span><span class="ss">/bottom</span><span class="sc">{</span>bottom_n<span class="sc">}</span><span class="ss">_defs_with_freq.csv"</span>,</span>
<span id="cb2-668"><a href="#cb2-668" aria-hidden="true" tabindex="-1"></a>        value_col<span class="op">=</span><span class="st">"len_tokens"</span></span>
<span id="cb2-669"><a href="#cb2-669" aria-hidden="true" tabindex="-1"></a>    )</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
</section>
<section id="obtaining-and-analyzing-the-results" class="level2" data-number="5">
<h2 data-number="5" class="anchored" data-anchor-id="obtaining-and-analyzing-the-results"><span class="header-section-number">5</span> Obtaining and analyzing the results</h2>
<p>To keep the report readable, I keep the details (some numerical values and results of statistical tests) collapsed by default. I will mention the most important of them in my textual analysis; you can see the rest by expanding collapsible sections.</p>
<p>All CSV files with results (basic and non-basic words, their definitions, metrics) can be found in the results/ folder in the root of this repository.</p>
<section id="initial-approach-wordnet-100-words-for-each-group-all-parts-of-speech" class="level3" data-number="5.1">
<h3 data-number="5.1" class="anchored" data-anchor-id="initial-approach-wordnet-100-words-for-each-group-all-parts-of-speech"><span class="header-section-number">5.1</span> Initial approach: WordNet, 100 words for each group, all parts of speech</h3>
<p>As suggested in the project proposal, I start with analyzing 100 “basic” and 100 “non-basic” words with no part-of-speech differentiation. Let’s obtain and analyze the results separately for the Web, Subtitle, and Book corpora.</p>
<div id="90ae39fb" class="cell" data-execution_count="13">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a>top_n <span class="op">=</span> <span class="dv">100</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>bottom_n <span class="op">=</span> <span class="dv">100</span></span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>collapsible_content <span class="op">=</span> <span class="st">"""</span></span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a><span class="st">&lt;details&gt;</span></span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a><span class="st">&lt;summary&gt;Click to view analysis output&lt;/summary&gt;</span></span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a><span class="st">&lt;pre&gt;</span><span class="sc">{captured_output}</span><span class="st">&lt;/pre&gt;</span></span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a><span class="st">&lt;/details&gt;</span></span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a><span class="st">"""</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<section id="web-based-corpus" class="level4" data-number="5.1.1">
<h4 data-number="5.1.1" class="anchored" data-anchor-id="web-based-corpus"><span class="header-section-number">5.1.1</span> Web-based corpus</h4>
<div id="64b1dced" class="cell" data-execution_count="246">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a>captured_output <span class="op">=</span> capture_function_output(full_run, top_n, bottom_n, file_name<span class="op">=</span><span class="st">"./data/unigram_freq_web.csv"</span>, folder<span class="op">=</span><span class="st">"./results/wordnet/web/all_100"</span>, pos<span class="op">=</span><span class="va">None</span>)</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>display(HTML(collapsible_content.replace(<span class="st">"</span><span class="sc">{captured_output}</span><span class="st">"</span>, captured_output)))</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stderr">
<pre><code>Lemmatizing: 100%|██████████| 333333/333333 [00:18&lt;00:00, 17931.03it/s]
Getting top frequency words: 188it [00:00, 28571.97it/s]
Getting bottom frequency words:   1%|          | 3816/315568 [00:00&lt;00:09, 31814.32it/s]
Obtaining definitions: 100it [00:00, 9445.35it/s]
Obtaining definitions: 100it [00:00, 2945.52it/s]</code></pre>
</div>
<div class="cell-output cell-output-display">

<details>
<summary>Click to view analysis output</summary>
<pre>
Log-10 mean frequency of tokens in definitions
Basic=-3.9265 (+-0.3314), nonbasic=-4.5579(+-0.6170)

Welch's t-test (unequal variances)
t = 9.004, p = 8.4e-16

Cohen's d = 1.273  (positive ⇒ basic &gt; nonbasic)

Mann-Whitney U (robustness)
U = 8008, p = 5.19e-14

95% CI for (basic - nonbasic) mean difference: [0.4928, 0.7699]


Log-10 median frequency of tokens in definitions
Basic=-3.8325 (+-0.3659), nonbasic=-4.4685(+-0.6886)

Welch's t-test (unequal variances)
t = 8.147, p = 1.31e-13

Cohen's d = 1.152  (positive ⇒ basic &gt; nonbasic)

Mann-Whitney U (robustness)
U = 7838, p = 1.17e-12

95% CI for (basic - nonbasic) mean difference: [0.4818, 0.7903]


Length of definition in tokens
Basic=5.8400 (+-3.0342), nonbasic=4.5300(+-2.9006)

Welch's t-test (unequal variances)
t = 3.121, p = 0.00207

Cohen's d = 0.441  (positive ⇒ basic &gt; nonbasic)

Mann-Whitney U (robustness)
U = 6548, p = 0.000137

95% CI for (basic - nonbasic) mean difference: [0.4822, 2.1378]
</pre>
</details>
</div>
</div>
</section>
<section id="subtitles-based-corpus" class="level4" data-number="5.1.2">
<h4 data-number="5.1.2" class="anchored" data-anchor-id="subtitles-based-corpus"><span class="header-section-number">5.1.2</span> Subtitles-based corpus</h4>
<div id="94af6034" class="cell" data-execution_count="247">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a>captured_output <span class="op">=</span> capture_function_output(full_run, top_n, bottom_n, file_name<span class="op">=</span><span class="st">"./data/unigram_freq_subtitles.csv"</span>, folder<span class="op">=</span><span class="st">"./results/wordnet/subtitles/all_100"</span>, pos<span class="op">=</span><span class="va">None</span>)</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a>display(HTML(collapsible_content.replace(<span class="st">"</span><span class="sc">{captured_output}</span><span class="st">"</span>, captured_output)))</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stderr">
<pre><code>Lemmatizing: 100%|██████████| 30000/30000 [00:05&lt;00:00, 5193.53it/s]
Getting top frequency words: 203it [00:00, 25611.18it/s]
Getting bottom frequency words:   1%|          | 154/26052 [00:00&lt;00:01, 17703.31it/s]
Obtaining definitions: 100it [00:00, 6984.22it/s]
Obtaining definitions: 100it [00:00, 2847.55it/s]</code></pre>
</div>
<div class="cell-output cell-output-display">

<details>
<summary>Click to view analysis output</summary>
<pre>
Log-10 mean frequency of tokens in definitions
Basic=-4.1607 (+-0.5421), nonbasic=-4.4563(+-0.5175)

Welch's t-test (unequal variances)
t = 3.923, p = 0.000121

Cohen's d = 0.558  (positive ⇒ basic &gt; nonbasic)

Mann-Whitney U (robustness)
U = 6400, p = 0.000201

95% CI for (basic - nonbasic) mean difference: [0.1470, 0.4442]


Log-10 median frequency of tokens in definitions
Basic=-4.1711 (+-0.6084), nonbasic=-4.4615(+-0.5409)

Welch's t-test (unequal variances)
t = 3.547, p = 0.00049

Cohen's d = 0.505  (positive ⇒ basic &gt; nonbasic)

Mann-Whitney U (robustness)
U = 6258, p = 0.000762

95% CI for (basic - nonbasic) mean difference: [0.1289, 0.4518]


Length of definition in tokens
Basic=5.0200 (+-3.6514), nonbasic=5.1700(+-2.9096)

Welch's t-test (unequal variances)
t = -0.321, p = 0.748

Cohen's d = -0.045  (positive ⇒ basic &gt; nonbasic)

Mann-Whitney U (robustness)
U = 4669, p = 0.414

95% CI for (basic - nonbasic) mean difference: [-1.0710, 0.7710]
</pre>
</details>
</div>
</div>
</section>
<section id="book-based-corpus" class="level4" data-number="5.1.3">
<h4 data-number="5.1.3" class="anchored" data-anchor-id="book-based-corpus"><span class="header-section-number">5.1.3</span> Book-based corpus</h4>
<div id="d5be3e1e" class="cell" data-execution_count="248">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a>captured_output <span class="op">=</span> capture_function_output(full_run, top_n, bottom_n, file_name<span class="op">=</span><span class="st">"./data/unigram_freq_books.txt"</span>, folder<span class="op">=</span><span class="st">"./results/wordnet/books/all_100"</span>, pos<span class="op">=</span><span class="va">None</span>)</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a>display(HTML(collapsible_content.replace(<span class="st">"</span><span class="sc">{captured_output}</span><span class="st">"</span>, captured_output)))</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stderr">
<pre><code>Lemmatizing: 100%|██████████| 397430/397430 [00:23&lt;00:00, 17144.91it/s]
Getting top frequency words: 205it [00:00, 29714.97it/s]
Getting bottom frequency words:   2%|▏         | 6090/360685 [00:00&lt;00:10, 33587.43it/s]
Obtaining definitions: 100it [00:00, 11177.06it/s]
Obtaining definitions: 100it [00:00, 2654.35it/s]</code></pre>
</div>
<div class="cell-output cell-output-display">

<details>
<summary>Click to view analysis output</summary>
<pre>
Log-10 mean frequency of tokens in definitions
Basic=-3.9112 (+-0.3224), nonbasic=-4.5963(+-0.6323)

Welch's t-test (unequal variances)
t = 9.622, p = 2.51e-17

Cohen's d = 1.359  (positive ⇒ basic &gt; nonbasic)

Mann-Whitney U (robustness)
U = 8474, p = 1.34e-19

95% CI for (basic - nonbasic) mean difference: [0.5444, 0.8258]


Log-10 median frequency of tokens in definitions
Basic=-3.8594 (+-0.3483), nonbasic=-4.5103(+-0.7003)

Welch's t-test (unequal variances)
t = 8.297, p = 6.49e-14

Cohen's d = 1.171  (positive ⇒ basic &gt; nonbasic)

Mann-Whitney U (robustness)
U = 8025, p = 2.1e-15

95% CI for (basic - nonbasic) mean difference: [0.4958, 0.8059]


Length of definition in tokens
Basic=5.2900 (+-3.2139), nonbasic=5.1200(+-3.9371)

Welch's t-test (unequal variances)
t = 0.334, p = 0.738

Cohen's d = 0.047  (positive ⇒ basic &gt; nonbasic)

Mann-Whitney U (robustness)
U = 5694, p = 0.0872

95% CI for (basic - nonbasic) mean difference: [-0.8325, 1.1725]
</pre>
</details>
</div>
</div>
</section>
<section id="sanity-check-are-we-getting-plausible-basic-and-non-basic-words" class="level4" data-number="5.1.4">
<h4 data-number="5.1.4" class="anchored" data-anchor-id="sanity-check-are-we-getting-plausible-basic-and-non-basic-words"><span class="header-section-number">5.1.4</span> Sanity check: Are we getting plausible “basic” and “non-basic” words?</h4>
<p>Before analyzing the results, let’s take a look at basic and non-basic words for each corpus to get a better understanding of our data. As I preprocessed the data, removing stopwords and words that are not present in WordNet, it is important to make sure that the results make sense. I provide Venn diagrams for top and bottom 100 words; the exact words can be found in my analysis or by clicking on the collapsible section under the diagram.</p>
<div id="b07a3e8a" class="cell" data-execution_count="48">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a>captured_output <span class="op">=</span> capture_function_output(create_venn_diagrams)</span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a>display(HTML(collapsible_content.replace(<span class="st">"</span><span class="sc">{captured_output}</span><span class="st">"</span>, captured_output)))</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="basic_words_project_files/figure-html/cell-8-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">

<details>
<summary>Click to view analysis output</summary>
<pre>TOP 100 WORDS:

Words in ALL THREE datasets (19 words):
  back, day, find, first, good, great, like, make, new, one, people, right, see, time, two, way, well, work, year

Words in BOOKS &amp; SUBTITLES only (18 words):
  come, doe, even, give, know, life, little, long, made, man, mean, much, must, place, said, say, take, woman

Words in BOOKS &amp; WEB only (22 words):
  also, area, city, company, group, information, line, may, member, number, order, part, program, report, result, school, service, state, system, use, used, world

Words in SUBTITLES &amp; WEB only (7 words):
  get, go, help, home, last, name, please

Words in BOOKS only (41 words):
  act, american, another, case, change, child, country, court, development, effect, every, fact, far, form, general, given, government, hand, high, house, however, interest, law, many, men, national, person, point, power, present, problem, public, question, rate, section, study, three, united, value, water, within

Words in SUBTITLES only (56 words):
  always, around, away, bad, believe, big, boy, ca, call, done, ever, father, feel, fine, friend, girl, god, going, got, guy, keep, leave, let, look, lot, love, maybe, money, mr., na, need, never, night, nothing, oh, okay, old, put, really, sir, sorry, stay, still, stop, sure, talk, tell, thank, thing, think, thought, told, wait, want, yeah, yes

Words in WEB only (52 words):
  add, available, best, book, business, buy, car, click, comment, contact, copyright, data, date, email, file, forum, free, game, health, hotel, info, item, jan, job, link, list, map, message, music, news, next, online, page, photo, pm, policy, post, price, product, review, search, set, site, software, store, support, term, top, user, video, view, web


BOTTOM 100 WORDS:
==================================================

Words in ALL THREE datasets (0 words):
  None

Words in BOOKS &amp; SUBTITLES only (0 words):
  None

Words in BOOKS &amp; WEB only (0 words):
  None

Words in SUBTITLES &amp; WEB only (0 words):
  None

Words in BOOKS only (100 words):
  anagrammatises, aquilege, arterialises, atrophedema, basifies, bedighting, bespeckles, burglarises, candyweed, capsulises, carnalises, carposporous, chichiest, clareting, clinocephalism, colourises, corakan, creashak, crocolite, demasculinise, detribalises, diabolises, discasing, discerping, disforests, displumes, eightvo, embrangles, etherises, exbibyte, floodhead, fluoridising, footslogs, frontstall, gynaeolatry, harlequining, holeyest, humidest, hygienise, hyperlipoidaemia, imminentness, imperfectest, inhumanest, iridoncus, jaggiest, jewbush, jocundest, jupaty, keratonosus, mailsorter, mayidism, meerestone, mongrelises, mundanest, mussitated, mussitating, nauseatingness, obeser, onomancer, ortygan, phlebotomizes, polyplacophore, psilotatae, putrider, quetching, religiousism, rotundest, sailplaned, scranches, seasicker, secernate, spicemill, straightforwarder, synercus, tabularises, taraktagenos, telepathising, telepathizes, terrietia, theologiser, tichodrome, tribulates, triostium, tunaburger, unappetizingness, unevenest, unforethoughtful, unitises, unpermissiveness, unsubtlest, uptighter, vetchworm, vitriolled, voltarean, waggonwright, wedeled, winteriest, wrawls, yanquapin, yauped

Words in SUBTITLES only (100 words):
  adoration, aiken, alerting, amazement, applesauce, beckons, bestest, brownstone, bruges, busier, callback, carpentry, characterize, christen, converging, corsica, crumbled, debit, decryption, directional, endures, evaporated, flintstone, forsaking, gooder, guardianship, gulag, hounded, impunity, inconspicuous, invoked, jumble, justifiable, khaki, knapsack, labrador, laguna, lally, lansing, lind, locus, loosing, memorandum, methodist, miri, molding, naga, nativity, nauseating, nicker, nullah, nunnery, oni, overzealous, p.e., parasitic, pituitary, playhouse, pollute, pomegranate, pondering, pragmatic, pram, predicts, printout, reiterate, retaining, retires, rotor, scrambler, shebang, sifting, slowest, southerner, spatula, splutter, squabbling, stamping, steeple, straightening, sussex, swerved, synergy, texan, traverse, tricycle, underlined, undisclosed, unknowingly, unsupervised, v.p., vaporize, varys, vas, ventricle, winkle, wisp, workhouse, wormwood, wylie

Words in WEB only (100 words):
  accessorial, aimlessness, airburst, amytal, antedated, antemortem, areolar, asilidae, asklepios, australopithecine, avows, barkes, catasetum, caustically, characin, choreographs, churchillian, cinclus, clupeidae, cocain, coelom, combativeness, commiserated, complots, concussed, containes, contentiousness, counterattacked, crookedly, deckled, deferentially, defoliate, dekko, demulcent, deodorized, deutzia, disconnectedness, distributary, dostoevski, enciphering, enthral, erithacus, expurgated, feminize, flouring, flurried, forfend, fragmental, gloominess, greyer, griever, hellishly, hinderer, hunchbacked, insatiably, jelled, jerkily, jobholder, jocose, lagune, leting, lither, litigates, loadstar, lobsterman, louisianan, mournes, moussorgsky, noctua, noncom, nybble, overborne, overflying, paramountcy, passiveness, pleochroism, rebating, retreaded, rusticated, saleroom, scomberomorus, skiving, steradian, suavely, sumach, synchronises, tackiest, tameness, thryothorus, topknot, topmast, triose, trochilidae, underselling, undiplomatic, urga, urmia, vambrace, vaporisation, wanter
</pre>
</details>
</div>
</div>
<p>For the “basic” words, there is substantial overlap between the datasets, with 19 words appearing across all three (“back, day, find, first, good, great, like, make, new, one, people, right, see, time, two, way, well, work, year”). That points to core vocabulary that transcends domain boundaries. When analysing the results for each dataset more closely, one may notice that:</p>
<ul>
<li>Books demonstrate formal institutional language (41 unique words like “act, american, case, government, law, public”)</li>
<li>Subtitles are full of conversational markers and basic verbs (56 unique words like “okay, oh, sorry, sure, thank, yes, yeah; believe, go, feel, look, love, need, wait, want”)</li>
<li>Web content is dominated by web-related terms (52 unique words like “add, available, book, business, click, comment, data, email, file, software, copyright”).</li>
</ul>
<p>In contrast, the “non-basic” words (expectedly) show no overlap between datasets, with each corpus containing completely different sets of specialized terms.</p>
<p>Additionally, let us take a look at several definitions from the book corpus:</p>
<div id="b04cc150" class="cell" data-execution_count="280">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a>df_basic_web <span class="op">=</span> pd.read_csv(<span class="st">"results/wordnet/books/all_100/top100_defs_with_freq.csv"</span>)</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>df_basic_web[:<span class="dv">5</span>]</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display" data-execution_count="280">
<div>


<table class="dataframe caption-top table table-sm table-striped small" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">lemma</th>
<th data-quarto-table-cell-role="th">definition</th>
<th data-quarto-table-cell-role="th">len_tokens</th>
<th data-quarto-table-cell-role="th">mean_log_freq</th>
<th data-quarto-table-cell-role="th">median_log_freq</th>
<th data-quarto-table-cell-role="th">miss_rate</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<th data-quarto-table-cell-role="th">0</th>
<td>one</td>
<td>used of a single unit or thing; not two or more</td>
<td>4</td>
<td>-3.420481</td>
<td>-3.455693</td>
<td>0.0</td>
</tr>
<tr class="even">
<th data-quarto-table-cell-role="th">1</th>
<td>may</td>
<td>the month following April and preceding June</td>
<td>5</td>
<td>-4.044908</td>
<td>-3.743638</td>
<td>0.0</td>
</tr>
<tr class="odd">
<th data-quarto-table-cell-role="th">2</th>
<td>time</td>
<td>an instance or single occasion for some event</td>
<td>4</td>
<td>-3.822569</td>
<td>-3.778365</td>
<td>0.0</td>
</tr>
<tr class="even">
<th data-quarto-table-cell-role="th">3</th>
<td>state</td>
<td>the territory occupied by one of the constitue...</td>
<td>6</td>
<td>-4.116025</td>
<td>-4.071440</td>
<td>0.0</td>
</tr>
<tr class="odd">
<th data-quarto-table-cell-role="th">4</th>
<td>year</td>
<td>a period of time containing 365 (or 366) days</td>
<td>4</td>
<td>-3.321808</td>
<td>-3.196169</td>
<td>0.0</td>
</tr>
</tbody>
</table>

</div>
</div>
</div>
<div id="5cd4a3c3" class="cell" data-execution_count="281">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a>df_nonbasic_web <span class="op">=</span> pd.read_csv(<span class="st">"results/wordnet/books/all_100/bottom100_defs_with_freq.csv"</span>)</span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a>df_nonbasic_web[:<span class="dv">5</span>]</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display" data-execution_count="281">
<div>


<table class="dataframe caption-top table table-sm table-striped small" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">lemma</th>
<th data-quarto-table-cell-role="th">definition</th>
<th data-quarto-table-cell-role="th">len_tokens</th>
<th data-quarto-table-cell-role="th">mean_log_freq</th>
<th data-quarto-table-cell-role="th">median_log_freq</th>
<th data-quarto-table-cell-role="th">miss_rate</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<th data-quarto-table-cell-role="th">0</th>
<td>fluoridising</td>
<td>subject to fluoridation; treat with fluoride</td>
<td>4</td>
<td>-4.832069</td>
<td>-4.833181</td>
<td>0.000000</td>
</tr>
<tr class="even">
<th data-quarto-table-cell-role="th">1</th>
<td>imperfectest</td>
<td>not perfect; defective or inadequate</td>
<td>3</td>
<td>-4.547468</td>
<td>-4.631890</td>
<td>0.000000</td>
</tr>
<tr class="odd">
<th data-quarto-table-cell-role="th">2</th>
<td>candyweed</td>
<td>bog plant of pine barrens of southeastern Unit...</td>
<td>11</td>
<td>-4.252058</td>
<td>-4.476592</td>
<td>0.181818</td>
</tr>
<tr class="even">
<th data-quarto-table-cell-role="th">3</th>
<td>jupaty</td>
<td>a tall Brazilian feather palm with a terminal ...</td>
<td>17</td>
<td>-4.390772</td>
<td>-4.289991</td>
<td>0.000000</td>
</tr>
<tr class="odd">
<th data-quarto-table-cell-role="th">4</th>
<td>unevenest</td>
<td>not even or uniform as e.g. in shape or texture</td>
<td>4</td>
<td>-4.070057</td>
<td>-4.151500</td>
<td>0.000000</td>
</tr>
</tbody>
</table>

</div>
</div>
</div>
<p>The words, the definitions, the length (after stopword removal), and log frequencies of the words used in the definitions are as expected. Lower mean log frequency corresponds to more rare (i.e., less basic) words being used in the definitions. The basic word list features words such as “one”, “may”, and “time”, while the non-basic one includes specialised concepts such as “fluoridising” and uncommon wordforms such as “imperfectest”.</p>
</section>
<section id="analysis-token-frequencies-in-definitions" class="level4" data-number="5.1.5">
<h4 data-number="5.1.5" class="anchored" data-anchor-id="analysis-token-frequencies-in-definitions"><span class="header-section-number">5.1.5</span> Analysis: token frequencies in definitions</h4>
<p>Now that I have shown that the obtained words and definitions seem to be as expected (basic words between datasets overlap, but for each dataset distinct patterns are identifiable; non-basic words are indeed rare and do not overlap between datasets; more basic definitions get higher log frequency score), let’s analyze the results I obtained earlier, when looking into “basicness” of the definitions on the previous step.</p>
<p>In addition to the numbers (that you can see by going up to <a href="#web-based-corpus">the Initial approach section</a> and clicking to expand the analysis results for each corpus), let’s look at the violin plots for mean log10 frequency of words in definitions and mean length of definitions for each corpus.</p>
<div id="be8f1024" class="cell" data-execution_count="95">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a>plot_violin_pairs(</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a>    pairs<span class="op">=</span>[</span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a>        (<span class="st">"./results/wordnet/web/all_100/top100_defs_with_freq.csv"</span>, <span class="ss">f"./results/wordnet/web/all_100/bottom100_defs_with_freq.csv"</span>),</span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a>        (<span class="st">"./results/wordnet/books/all_100/top100_defs_with_freq.csv"</span>, <span class="ss">f"./results/wordnet/books/all_100/bottom100_defs_with_freq.csv"</span>),</span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a>        (<span class="st">"./results/wordnet/subtitles/all_100/top100_defs_with_freq.csv"</span>, <span class="ss">f"./results/wordnet/subtitles/all_100/bottom100_defs_with_freq.csv"</span>),</span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a>    ],    </span>
<span id="cb13-7"><a href="#cb13-7" aria-hidden="true" tabindex="-1"></a>    value_col<span class="op">=</span><span class="st">"mean_log_freq"</span>,</span>
<span id="cb13-8"><a href="#cb13-8" aria-hidden="true" tabindex="-1"></a>    pair_labels<span class="op">=</span>[<span class="st">"Web"</span>,<span class="st">"Books"</span>,<span class="st">"Subs"</span>],</span>
<span id="cb13-9"><a href="#cb13-9" aria-hidden="true" tabindex="-1"></a>    transform<span class="op">=</span><span class="st">"yes"</span>,</span>
<span id="cb13-10"><a href="#cb13-10" aria-hidden="true" tabindex="-1"></a>    yscale<span class="op">=</span><span class="st">"log"</span>,</span>
<span id="cb13-11"><a href="#cb13-11" aria-hidden="true" tabindex="-1"></a>    title<span class="op">=</span><span class="st">"Token frequencies per million"</span></span>
<span id="cb13-12"><a href="#cb13-12" aria-hidden="true" tabindex="-1"></a>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="basic_words_project_files/figure-html/cell-11-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>Across all three corpora, definitions of the “basic” words systematically employ higher-frequency (i.e., more “basic”) vocabulary than those of “non-basic” words. In the Web corpus, the mean log-frequency gap is 0.63 (−3.93 vs −4.56; p&lt;.001; d=1.27), with a 95 % CI [0.49, 0.77], corresponding to roughly 117 vs 27.5 occurrences per million tokens—about 4.5× more frequent wording. In the Subtitles corpus, the gap is 0.30 (−4.16 vs −4.46; p&lt;.001; d=0.56; 95 % CI [0.15, 0.44]), equating to approximately 69 vs 35 per million, or about 2× higher frequency. In the Books corpus, the gap is 0.69 (−3.91 vs −4.60; p&lt;.001; d=1.36; 95 % CI [0.54, 0.83]), corresponding to 123 vs 25 per million, or roughly 5× more frequent wording. Across all three datasets, these differences are statistically significant (p&lt;.001) and consistently in the same direction, with more “basic” words featuring more “basic” defitions as well. The effect sizes indicate large differences for Web and Books (d&gt;.8) and moderate difference for Subtitles (.5&gt;d&gt;.2).</p>
<p>The violin plots tell the same story: for every corpus, the distributions for “basic” words are shifted upward on the log scale, and their geometric-mean lines (orange) consistently lie above those for “non-basic”. As for the shapes of the curves: in Web and Books, “basic” distributions are compact and concentrated at higher frequencies, while “non-basic” distributions are broader with longer rare-word tails; and Subtitles show greater overlap and lower overall mass, consistent with the smaller effect size.</p>
<p>The above-described patterns directly contradict the supposed “basic-word paradox”. Instead, they seems to align with defining-vocabulary practices: most “basic” words are explained with other “basic” words.</p>
</section>
<section id="analysis-length-of-definitions" class="level4" data-number="5.1.6">
<h4 data-number="5.1.6" class="anchored" data-anchor-id="analysis-length-of-definitions"><span class="header-section-number">5.1.6</span> Analysis: length of definitions</h4>
<div id="1e6c4fa1" class="cell" data-execution_count="98">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb14"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a>plot_violin_pairs(</span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a>    pairs<span class="op">=</span>[</span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a>        (<span class="st">"./results/wordnet/web/all_100/top100_defs_with_freq.csv"</span>, <span class="ss">f"./results/wordnet/web/all_100/bottom100_defs_with_freq.csv"</span>),</span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a>        (<span class="st">"./results/wordnet/books/all_100/top100_defs_with_freq.csv"</span>, <span class="ss">f"./results/wordnet/books/all_100/bottom100_defs_with_freq.csv"</span>),</span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a>        (<span class="st">"./results/wordnet/subtitles/all_100/top100_defs_with_freq.csv"</span>, <span class="ss">f"./results/wordnet/subtitles/all_100/bottom100_defs_with_freq.csv"</span>),</span>
<span id="cb14-6"><a href="#cb14-6" aria-hidden="true" tabindex="-1"></a>    ],</span>
<span id="cb14-7"><a href="#cb14-7" aria-hidden="true" tabindex="-1"></a>    value_col<span class="op">=</span><span class="st">"len_tokens"</span>,</span>
<span id="cb14-8"><a href="#cb14-8" aria-hidden="true" tabindex="-1"></a>    pair_labels<span class="op">=</span>[<span class="st">"Web"</span>,<span class="st">"Books"</span>,<span class="st">"Subs"</span>],</span>
<span id="cb14-9"><a href="#cb14-9" aria-hidden="true" tabindex="-1"></a>    transform<span class="op">=</span><span class="va">None</span>,</span>
<span id="cb14-10"><a href="#cb14-10" aria-hidden="true" tabindex="-1"></a>    yscale<span class="op">=</span><span class="st">"linear"</span>,</span>
<span id="cb14-11"><a href="#cb14-11" aria-hidden="true" tabindex="-1"></a>    title<span class="op">=</span><span class="st">"Definition length"</span></span>
<span id="cb14-12"><a href="#cb14-12" aria-hidden="true" tabindex="-1"></a>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="basic_words_project_files/figure-html/cell-12-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>For definition length, basic terms are modestly longer on Web (+1.31 tokens; 5.84 vs 4.53; p=.002; d=0.44) and Books (+0.58; 5.09 vs 4.51; p&lt;.001; d=0.20), but not in Subtitles (−0.23; 4.97 vs 5.20; p=.093, d=-0.045). For Web and Books, the difference is statistically significant with p-value smaller than .05 and the Cohen’s d points to small difference(.2&lt;d&lt;.5). For Subtitles, both Cohen’s d (&lt;.2) and p-value (&gt;.05) point to a negligible difference that is not statistically significant.</p>
<p>The violin plots show that the distributions of definition length are broad and strongly right-skewed, with long tails of long definitions across all corpora. The “basic” violins are slightly shifted upward (comared to “non-basic” ones) in Web and Books, indicating modestly longer definitions on average. At the same time, the large overlap between groups (especially in Subtitles) shows that the difference is minor relative to within-group variability. The wide middle portion of each violin (4–6 token) indicates that most definitions cluster tightly around similar short lengths, confirming that most definitions are short and compact.</p>
<p>This suggests that while explanations remain lexically simple, basic concepts may require slightly more wording (e.g., multiple senses or usage notes). That is consistent with my expectation that basic terms are conceptually rich, describing broad, general categories (and thus requiring longer definitions).</p>
<p>NB-1: In some “basic” cases, definition length equals 0, which can be explained by stopword removal, i.e.&nbsp;definitions for some “basic” words consisted entirely of stopwords.</p>
<p>NB-2: It is important to note that I only looked at one, most commonly used (according to SemCor) sense of each word. It would also be interesting to look into polysemy of “basic” words, but that is outside of the scope of this project.</p>
</section>
<section id="why-isnt-the-hypothesis-supported" class="level4" data-number="5.1.7">
<h4 data-number="5.1.7" class="anchored" data-anchor-id="why-isnt-the-hypothesis-supported"><span class="header-section-number">5.1.7</span> Why isn’t the hypothesis supported?</h4>
<p>In this section, I review two possible issues with the project setup. Later in the notebook, I address both of them to see whether the hypothesis is supported in an alternative setting.</p>
<ol type="1">
<li><p>When reviewing the list of target words, I noticed that the dataset mixes different parts of speech. These parts of speech differ systematically in how their definitions are written, which, in its turn, may affect there frequency profile.</p>
<p>For example, noun definitions in WordNet often include domain-specific or technical terms (“an assembly … to conduct judicial business” for “court”), leading to lower mean log frequencies. Verb definitions, however, tend to use very frequent auxiliary or generic words such as do, cause, make, which may raise their mean definition frequencies. Adjectives are defined either with longer explanatory phrases (“having desirable or positive qualities especially those suitable for a thing specified” for “good”) or with standardized repretitive construtions (“of or relating to or involving light or optics” for “optical”, and “relating or belonging to the class of chemical compounds having a carbon basis” for “organic”), again producing a distinct frequency profile and a higher mean length of explanation.</p>
<p>These systematic stylistic differences can distort the contrast between “basic” and “non-basic” words, so I decided to break the analysis down by part of speech. This way I can check whether the “basic-word paradox” (that basic words are defined using rarer vocabulary) holds within any of the given categoties or fails to hold for all of them.</p></li>
<li><p>Another problematic point is my choice of the source for word definitions. WordNet definitions are highly standardized and formulaic as they are supposed to serve as machine-readable entries in a lexical database rather than human-oriented dictionary definitions.</p>
<p>Each definition is written to describe a synset, or a specific conceptual sense. Often, definitions follow “hypernym + differentia” template and use repetitive constructions such as “of or relating to …” or “having …”. For example, “organization” is defined as “a group of people who work together”, patient as “a person who requires medical care”, and toy as “an artifact designed to be played with”, each mapping the concept to a high-level class (group/person/artifact) with short additional information to differentiate between subclasses. Even for very broad classes, the definitions are taxonomic and and concise. For example, “animal” is “a living organism characterized by voluntary movement”, and “device” is “an instrumentality invented for a particular purpose”.</p>
<p>This uniformity results in definitions that sound unnatural and lack the variation and descriptiveness of human-written lexicographic entries.</p></li>
</ol>
</section>
</section>
<section id="alternarive-approach-nounsverbsadjectivesadverbs-analyzed-separately" class="level3" data-number="5.2">
<h3 data-number="5.2" class="anchored" data-anchor-id="alternarive-approach-nounsverbsadjectivesadverbs-analyzed-separately"><span class="header-section-number">5.2</span> Alternarive approach: nouns/verbs/adjectives/adverbs analyzed separately</h3>
<p>In this part of the project, I also analyse 100 words for each group with definitions obtained from WordNet. However, in this case, different parts of speech are analyzed separately to see whether the word’s part of speech affects the complexity of its definition. I hypothesize that some parts of speech may follow the “Basic-Word Paradox” and some may not.</p>
<div id="2873950f" class="cell" data-execution_count="249">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb15"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> corpus_type <span class="kw">in</span> {<span class="st">"books"</span>, <span class="st">"web"</span>, <span class="st">"subtitles"</span>}:</span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> pos <span class="kw">in</span> {wn.NOUN, wn.VERB, wn.ADJ, wn.ADV}:</span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n\n</span><span class="ss">Analyzing for </span><span class="sc">{</span>corpus_type<span class="sc">}</span><span class="ss"> corpus, `</span><span class="sc">{</span>pos<span class="sc">}</span><span class="ss">` part-of-speech"</span>, flush<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> corpus_type <span class="op">==</span> <span class="st">"books"</span>:</span>
<span id="cb15-5"><a href="#cb15-5" aria-hidden="true" tabindex="-1"></a>            captured_output <span class="op">=</span> capture_function_output(full_run, <span class="dv">100</span>, <span class="dv">100</span>, file_name<span class="op">=</span><span class="ss">f"./data/unigram_freq_</span><span class="sc">{</span>corpus_type<span class="sc">}</span><span class="ss">.txt"</span>, folder<span class="op">=</span><span class="ss">f"./results/wordnet/</span><span class="sc">{</span>corpus_type<span class="sc">}</span><span class="ss">/</span><span class="sc">{</span>pos<span class="sc">}</span><span class="ss">"</span>, pos<span class="op">=</span>pos)</span>
<span id="cb15-6"><a href="#cb15-6" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>:</span>
<span id="cb15-7"><a href="#cb15-7" aria-hidden="true" tabindex="-1"></a>            captured_output <span class="op">=</span> capture_function_output(full_run, <span class="dv">100</span>, <span class="dv">100</span>, file_name<span class="op">=</span><span class="ss">f"./data/unigram_freq_</span><span class="sc">{</span>corpus_type<span class="sc">}</span><span class="ss">.csv"</span>, folder<span class="op">=</span><span class="ss">f"./results/wordnet/</span><span class="sc">{</span>corpus_type<span class="sc">}</span><span class="ss">/</span><span class="sc">{</span>pos<span class="sc">}</span><span class="ss">"</span>, pos<span class="op">=</span>pos)</span>
<span id="cb15-8"><a href="#cb15-8" aria-hidden="true" tabindex="-1"></a>        display(HTML(collapsible_content.replace(<span class="st">"</span><span class="sc">{captured_output}</span><span class="st">"</span>, captured_output)))</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>

Analyzing for subtitles corpus, `a` part-of-speech</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Lemmatizing: 100%|██████████| 30000/30000 [00:00&lt;00:00, 172670.95it/s]
Getting top frequency words: 958it [00:00, 3690.68it/s]
Getting bottom frequency words:  10%|█         | 3100/29793 [00:00&lt;00:02, 11669.50it/s]
Obtaining definitions: 100it [00:00, 3602.05it/s]
Obtaining definitions: 100it [00:00, 4476.12it/s]</code></pre>
</div>
<div class="cell-output cell-output-display">

<details>
<summary>Click to view analysis output</summary>
<pre>
Log-10 mean frequency of tokens in definitions
Basic=-4.4466 (+-0.4481), nonbasic=-4.7875(+-0.5560)

Welch's t-test (unequal variances)
t = 4.774, p = 3.6e-06

Cohen's d = 0.675  (positive ⇒ basic &gt; nonbasic)

Mann-Whitney U (robustness)
U = 6964, p = 1.61e-06

95% CI for (basic - nonbasic) mean difference: [0.2001, 0.4818]


Log-10 median frequency of tokens in definitions
Basic=-4.4940 (+-0.4696), nonbasic=-4.8303(+-0.6004)

Welch's t-test (unequal variances)
t = 4.411, p = 1.73e-05

Cohen's d = 0.624  (positive ⇒ basic &gt; nonbasic)

Mann-Whitney U (robustness)
U = 6798, p = 1.12e-05

95% CI for (basic - nonbasic) mean difference: [0.1859, 0.4866]


Length of definition in tokens
Basic=5.6400 (+-2.5167), nonbasic=4.0000(+-1.9540)

Welch's t-test (unequal variances)
t = 5.147, p = 6.67e-07

Cohen's d = 0.728  (positive ⇒ basic &gt; nonbasic)

Mann-Whitney U (robustness)
U = 6992, p = 8.8e-07

95% CI for (basic - nonbasic) mean difference: [1.0114, 2.2686]
</pre>
</details>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>

Analyzing for subtitles corpus, `v` part-of-speech</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Lemmatizing: 100%|██████████| 30000/30000 [00:00&lt;00:00, 336719.70it/s]
Getting top frequency words: 381it [00:00, 4270.34it/s]
Getting bottom frequency words:   9%|▊         | 2087/24370 [00:00&lt;00:01, 15583.21it/s]
Obtaining definitions: 100it [00:00, 7894.42it/s]
Obtaining definitions: 100it [00:00, 2861.23it/s]</code></pre>
</div>
<div class="cell-output cell-output-display">

<details>
<summary>Click to view analysis output</summary>
<pre>
Log-10 mean frequency of tokens in definitions
Basic=-4.0525 (+-0.5156), nonbasic=-4.3371(+-0.6069)

Welch's t-test (unequal variances)
t = 3.575, p = 0.000443

Cohen's d = 0.506  (positive ⇒ basic &gt; nonbasic)

Mann-Whitney U (robustness)
U = 6431, p = 0.000474

95% CI for (basic - nonbasic) mean difference: [0.1276, 0.4417]


Log-10 median frequency of tokens in definitions
Basic=-4.0619 (+-0.5490), nonbasic=-4.3432(+-0.6566)

Welch's t-test (unequal variances)
t = 3.287, p = 0.00121

Cohen's d = 0.465  (positive ⇒ basic &gt; nonbasic)

Mann-Whitney U (robustness)
U = 6310, p = 0.00137

95% CI for (basic - nonbasic) mean difference: [0.1125, 0.4501]


Length of definition in tokens
Basic=4.0900 (+-1.9441), nonbasic=3.8500(+-1.9918)

Welch's t-test (unequal variances)
t = 0.862, p = 0.39

Cohen's d = 0.122  (positive ⇒ basic &gt; nonbasic)

Mann-Whitney U (robustness)
U = 5506, p = 0.209

95% CI for (basic - nonbasic) mean difference: [-0.3089, 0.7889]
</pre>
</details>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>

Analyzing for subtitles corpus, `n` part-of-speech</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Lemmatizing: 100%|██████████| 30000/30000 [00:00&lt;00:00, 224402.73it/s]
Getting top frequency words: 383it [00:00, 3363.96it/s]
Getting bottom frequency words:   1%|          | 255/26060 [00:00&lt;00:02, 10333.89it/s]
Obtaining definitions: 100it [00:00, 9758.05it/s]
Obtaining definitions: 100it [00:00, 5887.24it/s]</code></pre>
</div>
<div class="cell-output cell-output-display">

<details>
<summary>Click to view analysis output</summary>
<pre>
Log-10 mean frequency of tokens in definitions
Basic=-4.2765 (+-0.5239), nonbasic=-4.5308(+-0.3929)

Welch's t-test (unequal variances)
t = 3.882, p = 0.000144

Cohen's d = 0.549  (positive ⇒ basic &gt; nonbasic)

Mann-Whitney U (robustness)
U = 6570, p = 0.000126

95% CI for (basic - nonbasic) mean difference: [0.1250, 0.3835]


Log-10 median frequency of tokens in definitions
Basic=-4.2899 (+-0.5661), nonbasic=-4.4932(+-0.4373)

Welch's t-test (unequal variances)
t = 2.842, p = 0.00498

Cohen's d = 0.402  (positive ⇒ basic &gt; nonbasic)

Mann-Whitney U (robustness)
U = 6185, p = 0.0038

95% CI for (basic - nonbasic) mean difference: [0.0622, 0.3444]


Length of definition in tokens
Basic=5.2600 (+-3.7809), nonbasic=6.0800(+-3.1676)

Welch's t-test (unequal variances)
t = -1.662, p = 0.0981

Cohen's d = -0.235  (positive ⇒ basic &gt; nonbasic)

Mann-Whitney U (robustness)
U = 3828, p = 0.00386

95% CI for (basic - nonbasic) mean difference: [-1.7929, 0.1529]
</pre>
</details>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>

Analyzing for subtitles corpus, `r` part-of-speech</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Lemmatizing: 100%|██████████| 30000/30000 [00:00&lt;00:00, 533789.45it/s]
Getting top frequency words: 1829it [00:00, 4592.28it/s]
Getting bottom frequency words:  19%|█▊        | 5564/29993 [00:00&lt;00:01, 13503.28it/s]
Obtaining definitions: 100it [00:00, 13367.02it/s]
Obtaining definitions: 100it [00:00, 7302.95it/s]</code></pre>
</div>
<div class="cell-output cell-output-display">

<details>
<summary>Click to view analysis output</summary>
<pre>
Log-10 mean frequency of tokens in definitions
Basic=-4.1245 (+-0.5742), nonbasic=-4.7327(+-0.6626)

Welch's t-test (unequal variances)
t = 6.887, p = 7.92e-11

Cohen's d = 0.981  (positive ⇒ basic &gt; nonbasic)

Mann-Whitney U (robustness)
U = 7592, p = 7.34e-12

95% CI for (basic - nonbasic) mean difference: [0.4340, 0.7824]


Log-10 median frequency of tokens in definitions
Basic=-4.1050 (+-0.6364), nonbasic=-4.7378(+-0.6758)

Welch's t-test (unequal variances)
t = 6.766, p = 1.51e-10

Cohen's d = 0.964  (positive ⇒ basic &gt; nonbasic)

Mann-Whitney U (robustness)
U = 7464, p = 6.58e-11

95% CI for (basic - nonbasic) mean difference: [0.4483, 0.8172]


Length of definition in tokens
Basic=3.4500 (+-2.3969), nonbasic=2.2500(+-0.9143)

Welch's t-test (unequal variances)
t = 4.678, p = 7.3e-06

Cohen's d = 0.662  (positive ⇒ basic &gt; nonbasic)

Mann-Whitney U (robustness)
U = 6414, p = 0.000323

95% CI for (basic - nonbasic) mean difference: [0.6924, 1.7076]
</pre>
</details>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>

Analyzing for books corpus, `a` part-of-speech</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Lemmatizing: 100%|██████████| 397430/397430 [00:01&lt;00:00, 221739.18it/s]
Getting top frequency words: 782it [00:00, 4504.84it/s]
Getting bottom frequency words:  18%|█▊        | 72371/394946 [00:02&lt;00:10, 29367.46it/s]
Obtaining definitions: 100it [00:00, 11394.47it/s]
Obtaining definitions: 100it [00:00, 3857.43it/s]</code></pre>
</div>
<div class="cell-output cell-output-display">

<details>
<summary>Click to view analysis output</summary>
<pre>
Log-10 mean frequency of tokens in definitions
Basic=-4.0004 (+-0.3236), nonbasic=-5.1688(+-0.9526)

Welch's t-test (unequal variances)
t = 11.562, p = 3.06e-21

Cohen's d = 1.646  (positive ⇒ basic &gt; nonbasic)

Mann-Whitney U (robustness)
U = 8865, p = 5.59e-22

95% CI for (basic - nonbasic) mean difference: [0.9683, 1.3685]


Log-10 median frequency of tokens in definitions
Basic=-3.9622 (+-0.3583), nonbasic=-5.0902(+-1.0302)

Welch's t-test (unequal variances)
t = 10.295, p = 3.06e-18

Cohen's d = 1.465  (positive ⇒ basic &gt; nonbasic)

Mann-Whitney U (robustness)
U = 8755, p = 7.51e-21

95% CI for (basic - nonbasic) mean difference: [0.9111, 1.3449]


Length of definition in tokens
Basic=5.7200 (+-2.3315), nonbasic=3.3300(+-2.0940)

Welch's t-test (unequal variances)
t = 7.626, p = 1.02e-12

Cohen's d = 1.079  (positive ⇒ basic &gt; nonbasic)

Mann-Whitney U (robustness)
U = 8050, p = 5.31e-14

95% CI for (basic - nonbasic) mean difference: [1.7720, 3.0080]
</pre>
</details>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>

Analyzing for books corpus, `v` part-of-speech</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Lemmatizing: 100%|██████████| 397430/397430 [00:01&lt;00:00, 341401.90it/s]
Getting top frequency words: 435it [00:00, 2908.28it/s]
Getting bottom frequency words:  26%|██▌       | 96019/372294 [00:03&lt;00:10, 26421.41it/s]
Obtaining definitions: 100it [00:00, 6615.93it/s]
Obtaining definitions: 100it [00:00, 2485.22it/s]</code></pre>
</div>
<div class="cell-output cell-output-display">

<details>
<summary>Click to view analysis output</summary>
<pre>
Log-10 mean frequency of tokens in definitions
Basic=-3.6676 (+-0.3455), nonbasic=-4.5182(+-1.1397)

Welch's t-test (unequal variances)
t = 7.139, p = 8.48e-11

Cohen's d = 1.008  (positive ⇒ basic &gt; nonbasic)

Mann-Whitney U (robustness)
U = 8264, p = 3.42e-16

95% CI for (basic - nonbasic) mean difference: [0.6146, 1.0865]


Log-10 median frequency of tokens in definitions
Basic=-3.6502 (+-0.3980), nonbasic=-4.4600(+-1.1592)

Welch's t-test (unequal variances)
t = 6.604, p = 1.1e-09

Cohen's d = 0.933  (positive ⇒ basic &gt; nonbasic)

Mann-Whitney U (robustness)
U = 7987, p = 7.68e-14

95% CI for (basic - nonbasic) mean difference: [0.5671, 1.0526]


Length of definition in tokens
Basic=4.3700 (+-2.1256), nonbasic=3.3600(+-1.5604)

Welch's t-test (unequal variances)
t = 3.830, p = 0.000176

Cohen's d = 0.542  (positive ⇒ basic &gt; nonbasic)

Mann-Whitney U (robustness)
U = 6538, p = 0.000136

95% CI for (basic - nonbasic) mean difference: [0.4897, 1.5303]
</pre>
</details>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>

Analyzing for books corpus, `n` part-of-speech</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Lemmatizing: 100%|██████████| 397430/397430 [00:01&lt;00:00, 394381.43it/s]
Getting top frequency words: 289it [00:00, 2714.56it/s]
Getting bottom frequency words:   4%|▍         | 14069/360676 [00:00&lt;00:20, 17011.78it/s]
Obtaining definitions: 100it [00:00, 7481.15it/s]
Obtaining definitions: 100it [00:00, 3996.94it/s]</code></pre>
</div>
<div class="cell-output cell-output-display">

<details>
<summary>Click to view analysis output</summary>
<pre>
Log-10 mean frequency of tokens in definitions
Basic=-3.8994 (+-0.3450), nonbasic=-4.5402(+-0.5794)

Welch's t-test (unequal variances)
t = 9.503, p = 2.77e-17

Cohen's d = 1.344  (positive ⇒ basic &gt; nonbasic)

Mann-Whitney U (robustness)
U = 8612, p = 1.1e-18

95% CI for (basic - nonbasic) mean difference: [0.5076, 0.7739]


Log-10 median frequency of tokens in definitions
Basic=-3.8074 (+-0.3555), nonbasic=-4.4167(+-0.6454)

Welch's t-test (unequal variances)
t = 8.268, p = 5.91e-14

Cohen's d = 1.169  (positive ⇒ basic &gt; nonbasic)

Mann-Whitney U (robustness)
U = 8256, p = 1.82e-15

95% CI for (basic - nonbasic) mean difference: [0.4637, 0.7548]


Length of definition in tokens
Basic=5.4700 (+-2.8086), nonbasic=6.8200(+-4.3190)

Welch's t-test (unequal variances)
t = -2.620, p = 0.00958

Cohen's d = -0.371  (positive ⇒ basic &gt; nonbasic)

Mann-Whitney U (robustness)
U = 4254, p = 0.0664

95% CI for (basic - nonbasic) mean difference: [-2.3670, -0.3330]
</pre>
</details>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>

Analyzing for books corpus, `r` part-of-speech</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Lemmatizing: 100%|██████████| 397430/397430 [00:00&lt;00:00, 603125.23it/s]
Getting top frequency words: 2069it [00:00, 5201.64it/s]
Getting bottom frequency words:  40%|███▉      | 157626/397423 [00:06&lt;00:09, 25198.88it/s]
Obtaining definitions: 100it [00:00, 16542.97it/s]
Obtaining definitions: 100it [00:00, 10876.78it/s]</code></pre>
</div>
<div class="cell-output cell-output-display">

<details>
<summary>Click to view analysis output</summary>
<pre>
Log-10 mean frequency of tokens in definitions
Basic=-3.8885 (+-0.5012), nonbasic=-5.0662(+-0.6684)

Welch's t-test (unequal variances)
t = 14.071, p = 5.55e-31

Cohen's d = 1.992  (positive ⇒ basic &gt; nonbasic)

Mann-Whitney U (robustness)
U = 9117, p = 1.1e-24

95% CI for (basic - nonbasic) mean difference: [1.0125, 1.3428]


Log-10 median frequency of tokens in definitions
Basic=-3.8508 (+-0.5354), nonbasic=-5.0801(+-0.7853)

Welch's t-test (unequal variances)
t = 12.912, p = 3.22e-27

Cohen's d = 1.827  (positive ⇒ basic &gt; nonbasic)

Mann-Whitney U (robustness)
U = 8952, p = 6.76e-23

95% CI for (basic - nonbasic) mean difference: [1.0414, 1.4171]


Length of definition in tokens
Basic=3.5900 (+-2.4499), nonbasic=2.2700(+-0.7502)

Welch's t-test (unequal variances)
t = 5.152, p = 1.05e-06

Cohen's d = 0.729  (positive ⇒ basic &gt; nonbasic)

Mann-Whitney U (robustness)
U = 6551, p = 7.39e-05

95% CI for (basic - nonbasic) mean difference: [0.8126, 1.8274]
</pre>
</details>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>

Analyzing for web corpus, `a` part-of-speech</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Lemmatizing: 100%|██████████| 333333/333333 [00:00&lt;00:00, 511348.13it/s]
Getting top frequency words: 990it [00:00, 4879.01it/s]
Getting bottom frequency words:  12%|█▏        | 39063/332546 [00:01&lt;00:11, 24815.75it/s]
Obtaining definitions: 100it [00:00, 12817.60it/s]
Obtaining definitions: 100it [00:00, 6222.08it/s]</code></pre>
</div>
<div class="cell-output cell-output-display">

<details>
<summary>Click to view analysis output</summary>
<pre>
Log-10 mean frequency of tokens in definitions
Basic=-4.1695 (+-0.4135), nonbasic=-4.7509(+-0.6221)

Welch's t-test (unequal variances)
t = 7.756, p = 7.68e-13

Cohen's d = 1.102  (positive ⇒ basic &gt; nonbasic)

Mann-Whitney U (robustness)
U = 7788, p = 2.84e-12

95% CI for (basic - nonbasic) mean difference: [0.4334, 0.7293]


Log-10 median frequency of tokens in definitions
Basic=-4.1058 (+-0.4544), nonbasic=-4.6779(+-0.6336)

Welch's t-test (unequal variances)
t = 7.313, p = 8.6e-12

Cohen's d = 1.039  (positive ⇒ basic &gt; nonbasic)

Mann-Whitney U (robustness)
U = 7708, p = 1.12e-11

95% CI for (basic - nonbasic) mean difference: [0.4177, 0.7265]


Length of definition in tokens
Basic=5.7600 (+-2.5151), nonbasic=3.8100(+-2.1588)

Welch's t-test (unequal variances)
t = 5.883, p = 1.74e-08

Cohen's d = 0.832  (positive ⇒ basic &gt; nonbasic)

Mann-Whitney U (robustness)
U = 7519, p = 4.9e-10

95% CI for (basic - nonbasic) mean difference: [1.2963, 2.6037]
</pre>
</details>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>

Analyzing for web corpus, `v` part-of-speech</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Lemmatizing: 100%|██████████| 333333/333333 [00:00&lt;00:00, 453051.87it/s]
Getting top frequency words: 482it [00:00, 4367.49it/s]
Getting bottom frequency words:  15%|█▌        | 47838/317003 [00:01&lt;00:10, 26409.91it/s]
Obtaining definitions: 100it [00:00, 9486.80it/s]
Obtaining definitions: 100it [00:00, 3385.59it/s]</code></pre>
</div>
<div class="cell-output cell-output-display">

<details>
<summary>Click to view analysis output</summary>
<pre>
Log-10 mean frequency of tokens in definitions
Basic=-3.7822 (+-0.4010), nonbasic=-4.3861(+-0.5701)

Welch's t-test (unequal variances)
t = 8.665, p = 2.73e-15

Cohen's d = 1.225  (positive ⇒ basic &gt; nonbasic)

Mann-Whitney U (robustness)
U = 8185, p = 7.2e-15

95% CI for (basic - nonbasic) mean difference: [0.4663, 0.7414]


Log-10 median frequency of tokens in definitions
Basic=-3.7421 (+-0.4669), nonbasic=-4.3231(+-0.6227)

Welch's t-test (unequal variances)
t = 7.465, p = 3.24e-12

Cohen's d = 1.056  (positive ⇒ basic &gt; nonbasic)

Mann-Whitney U (robustness)
U = 7808, p = 6.89e-12

95% CI for (basic - nonbasic) mean difference: [0.4275, 0.7346]


Length of definition in tokens
Basic=4.3200 (+-1.9893), nonbasic=3.5200(+-1.7493)

Welch's t-test (unequal variances)
t = 3.020, p = 0.00287

Cohen's d = 0.427  (positive ⇒ basic &gt; nonbasic)

Mann-Whitney U (robustness)
U = 6346, p = 0.000817

95% CI for (basic - nonbasic) mean difference: [0.2776, 1.3224]
</pre>
</details>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>

Analyzing for web corpus, `n` part-of-speech</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Lemmatizing: 100%|██████████| 333333/333333 [00:00&lt;00:00, 379162.92it/s]
Getting top frequency words: 249it [00:00, 4338.43it/s]
Getting bottom frequency words:   2%|▏         | 6172/315566 [00:00&lt;00:13, 22246.49it/s]
Obtaining definitions: 100it [00:00, 12545.40it/s]
Obtaining definitions: 100it [00:00, 4800.79it/s]</code></pre>
</div>
<div class="cell-output cell-output-display">

<details>
<summary>Click to view analysis output</summary>
<pre>
Log-10 mean frequency of tokens in definitions
Basic=-3.9085 (+-0.3203), nonbasic=-4.5695(+-0.6389)

Welch's t-test (unequal variances)
t = 9.250, p = 2.58e-16

Cohen's d = 1.308  (positive ⇒ basic &gt; nonbasic)

Mann-Whitney U (robustness)
U = 8317, p = 5.34e-16

95% CI for (basic - nonbasic) mean difference: [0.5198, 0.8023]


Log-10 median frequency of tokens in definitions
Basic=-3.7949 (+-0.3194), nonbasic=-4.4735(+-0.7330)

Welch's t-test (unequal variances)
t = 8.488, p = 3.28e-14

Cohen's d = 1.200  (positive ⇒ basic &gt; nonbasic)

Mann-Whitney U (robustness)
U = 8106, p = 3.22e-14

95% CI for (basic - nonbasic) mean difference: [0.5205, 0.8368]


Length of definition in tokens
Basic=6.0800 (+-2.8839), nonbasic=5.6800(+-3.3751)

Welch's t-test (unequal variances)
t = 0.901, p = 0.369

Cohen's d = 0.127  (positive ⇒ basic &gt; nonbasic)

Mann-Whitney U (robustness)
U = 5664, p = 0.102

95% CI for (basic - nonbasic) mean difference: [-0.4756, 1.2756]
</pre>
</details>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>

Analyzing for web corpus, `r` part-of-speech</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Lemmatizing: 100%|██████████| 333333/333333 [00:00&lt;00:00, 576241.16it/s]
Getting top frequency words: 3609it [00:00, 6128.96it/s]
Getting bottom frequency words:  13%|█▎        | 43585/333326 [00:01&lt;00:11, 25077.18it/s]
Obtaining definitions: 100it [00:00, 15147.36it/s]
Obtaining definitions: 100it [00:00, 9547.48it/s]</code></pre>
</div>
<div class="cell-output cell-output-display">

<details>
<summary>Click to view analysis output</summary>
<pre>
Log-10 mean frequency of tokens in definitions
Basic=-4.0334 (+-0.4925), nonbasic=-5.1014(+-0.5456)

Welch's t-test (unequal variances)
t = 14.465, p = 1.1e-32

Cohen's d = 2.054  (positive ⇒ basic &gt; nonbasic)

Mann-Whitney U (robustness)
U = 9011, p = 2.06e-24

95% CI for (basic - nonbasic) mean difference: [0.9224, 1.2136]


Log-10 median frequency of tokens in definitions
Basic=-3.9843 (+-0.5238), nonbasic=-5.0804(+-0.6193)

Welch's t-test (unequal variances)
t = 13.457, p = 1.66e-29

Cohen's d = 1.910  (positive ⇒ basic &gt; nonbasic)

Mann-Whitney U (robustness)
U = 8878, p = 5.88e-23

95% CI for (basic - nonbasic) mean difference: [0.9354, 1.2568]


Length of definition in tokens
Basic=3.5800 (+-2.5313), nonbasic=2.4100(+-0.9545)

Welch's t-test (unequal variances)
t = 4.325, p = 3.07e-05

Cohen's d = 0.612  (positive ⇒ basic &gt; nonbasic)

Mann-Whitney U (robustness)
U = 6194, p = 0.00257

95% CI for (basic - nonbasic) mean difference: [0.6347, 1.7053]
</pre>
</details>
</div>
</div>
<section id="analysis-token-frequencies-in-definitions-1" class="level4" data-number="5.2.1">
<h4 data-number="5.2.1" class="anchored" data-anchor-id="analysis-token-frequencies-in-definitions-1"><span class="header-section-number">5.2.1</span> Analysis: token frequencies in definitions</h4>
<div id="39a54291" class="cell" data-execution_count="263">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb40"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb40-1"><a href="#cb40-1" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> pos <span class="kw">in</span> {wn.NOUN, wn.VERB, wn.ADJ, wn.ADV}:</span>
<span id="cb40-2"><a href="#cb40-2" aria-hidden="true" tabindex="-1"></a>    pos_full <span class="op">=</span> <span class="st">"Nouns"</span> <span class="cf">if</span> pos <span class="op">==</span> <span class="st">'n'</span> <span class="cf">else</span> <span class="st">"Verbs"</span> <span class="cf">if</span> pos <span class="op">==</span> <span class="st">'v'</span> <span class="cf">else</span> <span class="st">"Adjectives"</span> <span class="cf">if</span> pos <span class="op">==</span> <span class="st">'a'</span> <span class="cf">else</span> <span class="st">"Adverbs"</span> <span class="cf">if</span> pos <span class="op">==</span> <span class="st">'r'</span> <span class="cf">else</span> <span class="va">None</span></span>
<span id="cb40-3"><a href="#cb40-3" aria-hidden="true" tabindex="-1"></a>    plot_violin_pairs(</span>
<span id="cb40-4"><a href="#cb40-4" aria-hidden="true" tabindex="-1"></a>        pairs<span class="op">=</span>[</span>
<span id="cb40-5"><a href="#cb40-5" aria-hidden="true" tabindex="-1"></a>            (<span class="ss">f"./results/wordnet/web/</span><span class="sc">{</span>pos<span class="sc">}</span><span class="ss">/top100_defs_with_freq.csv"</span>, <span class="ss">f"./results/wordnet/web/</span><span class="sc">{</span>pos<span class="sc">}</span><span class="ss">/bottom100_defs_with_freq.csv"</span>),</span>
<span id="cb40-6"><a href="#cb40-6" aria-hidden="true" tabindex="-1"></a>            (<span class="ss">f"./results/wordnet/books/</span><span class="sc">{</span>pos<span class="sc">}</span><span class="ss">/top100_defs_with_freq.csv"</span>, <span class="ss">f"./results/wordnet/books/</span><span class="sc">{</span>pos<span class="sc">}</span><span class="ss">/bottom100_defs_with_freq.csv"</span>),</span>
<span id="cb40-7"><a href="#cb40-7" aria-hidden="true" tabindex="-1"></a>            (<span class="ss">f"./results/wordnet/subtitles/</span><span class="sc">{</span>pos<span class="sc">}</span><span class="ss">/top100_defs_with_freq.csv"</span>, <span class="ss">f"./results/wordnet/subtitles/</span><span class="sc">{</span>pos<span class="sc">}</span><span class="ss">/bottom100_defs_with_freq.csv"</span>),</span>
<span id="cb40-8"><a href="#cb40-8" aria-hidden="true" tabindex="-1"></a>        ],    </span>
<span id="cb40-9"><a href="#cb40-9" aria-hidden="true" tabindex="-1"></a>        value_col<span class="op">=</span><span class="st">"mean_log_freq"</span>,</span>
<span id="cb40-10"><a href="#cb40-10" aria-hidden="true" tabindex="-1"></a>        pair_labels<span class="op">=</span>[<span class="st">"Web"</span>,<span class="st">"Books"</span>,<span class="st">"Subs"</span>],</span>
<span id="cb40-11"><a href="#cb40-11" aria-hidden="true" tabindex="-1"></a>        transform<span class="op">=</span><span class="st">"yes"</span>,</span>
<span id="cb40-12"><a href="#cb40-12" aria-hidden="true" tabindex="-1"></a>        yscale<span class="op">=</span><span class="st">"log"</span>,</span>
<span id="cb40-13"><a href="#cb40-13" aria-hidden="true" tabindex="-1"></a>        title<span class="op">=</span><span class="ss">f"Token frequencies per million, </span><span class="sc">{</span>pos_full<span class="sc">}</span><span class="ss">"</span></span>
<span id="cb40-14"><a href="#cb40-14" aria-hidden="true" tabindex="-1"></a>    )</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="basic_words_project_files/figure-html/cell-14-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="basic_words_project_files/figure-html/cell-14-output-2.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="basic_words_project_files/figure-html/cell-14-output-3.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="basic_words_project_files/figure-html/cell-14-output-4.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>Basic adjectives consistently favor higher-frequency (more basic) defining vocabulary. In Web the gap is 0.58 (−4.17 vs −4.75; p&lt;.001; d=1.10; 95% CI [0.43, 0.73]); in Books it is large at 1.17 (−4.00 vs −5.17; p&lt;.001; d=1.65; 95% CI [0.97, 1.37]); and in Subtitles it is 0.34 (−4.45 vs −4.79; p&lt;.001; d=0.68; 95% CI [0.20, 0.48]). The violins for Web and Books show a clear upward shift and tighter mass for basic adjectives, while Subtitles displays greater overlap and a smaller but still noticeable upward trend for basic vocabulary.</p>
<p>The same pattern holds for verbs, though with slightly smaller gaps than. Web shows a gap of 0.60 (−3.78 vs −4.39; p&lt;.001; d=1.23; 95% CI [0.47, 0.74]); Books 0.85 (−3.67 vs −4.52; p&lt;.001; d=1.01; 95% CI [0.61, 1.09]); and Subtitles 0.28 (−4.05 vs −4.34; p=.00044; d=0.51; 95% CI [0.13, 0.44]). The violins show basic verbs concentrated higher on the log scale, with non-basic verbs exhibiting broader, lower-frequency tails; overlap is greatest in Subtitles, again matching the smaller effect size.</p>
<p>Across all corpora, basic-noun definitions use higher-frequency vocabulary than those of non-basic nouns. In Web, the mean log-frequency gap is 0.66 (−3.91 vs −4.57; p&lt;.001; d=1.31; 95% CI [0.52, 0.80]); in Books, the gap is 0.64 (−3.90 vs −4.54; p&lt;.001; d=1.34; 95% CI [0.51, 0.77]); in Subtitles, it is 0.25 (−4.28 vs −4.53; p&lt;.001; d=0.55; 95% CI [0.13, 0.38]). The violins mirror this pattern – basic distributions are shifted upward with heavier mass at higher frequencies, while non-basic nouns show broader spread and a long rare-word tail, especially for Web and Books; Subtitles shows more overlap and a smaller shift.</p>
<p>Adverbs show the strongest contrasts. Web shows a gap of 1.07 (−4.03 vs −5.10; p&lt;.001; d=2.05; 95% CI [0.92, 1.21]); Books 1.18 (−3.89 vs −5.07; p&lt;.001; d=1.99; 95% CI [1.01, 1.34]); and Subtitles 0.61 (−4.12 vs −4.73; p&lt;.001; d=0.98; 95% CI [0.43, 0.78]). The violins show basic adverbs with distributions concentrated at much higher frequencies and non-basic adverbs with long rare-word tails; Subtitles again shows more overlap but still a clear upward shift for basic vocabulary.</p>
<p>Overall, for every part of speech and in every corpus, basic words are defined using systematically more frequent vocabulary, with large effects in Web and Books and more moderate effects in Subtitles.</p>
</section>
<section id="analysis-length-of-definitions-1" class="level4" data-number="5.2.2">
<h4 data-number="5.2.2" class="anchored" data-anchor-id="analysis-length-of-definitions-1"><span class="header-section-number">5.2.2</span> Analysis: length of definitions</h4>
<div id="6e77b370" class="cell" data-execution_count="184">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb41"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb41-1"><a href="#cb41-1" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> pos <span class="kw">in</span> {wn.NOUN, wn.VERB, wn.ADJ, wn.ADV}:</span>
<span id="cb41-2"><a href="#cb41-2" aria-hidden="true" tabindex="-1"></a>    pos_full <span class="op">=</span> <span class="st">"Nouns"</span> <span class="cf">if</span> pos <span class="op">==</span> <span class="st">'n'</span> <span class="cf">else</span> <span class="st">"Verbs"</span> <span class="cf">if</span> pos <span class="op">==</span> <span class="st">'v'</span> <span class="cf">else</span> <span class="st">"Adjectives"</span> <span class="cf">if</span> pos <span class="op">==</span> <span class="st">'a'</span> <span class="cf">else</span> <span class="st">"Adverbs"</span> <span class="cf">if</span> pos <span class="op">==</span> <span class="st">'r'</span> <span class="cf">else</span> <span class="va">None</span></span>
<span id="cb41-3"><a href="#cb41-3" aria-hidden="true" tabindex="-1"></a>    plot_violin_pairs(</span>
<span id="cb41-4"><a href="#cb41-4" aria-hidden="true" tabindex="-1"></a>        pairs<span class="op">=</span>[</span>
<span id="cb41-5"><a href="#cb41-5" aria-hidden="true" tabindex="-1"></a>            (<span class="ss">f"./results/wordnet/web/</span><span class="sc">{</span>pos<span class="sc">}</span><span class="ss">/top100_defs_with_freq.csv"</span>, <span class="ss">f"./results/wordnet/web/</span><span class="sc">{</span>pos<span class="sc">}</span><span class="ss">/bottom100_defs_with_freq.csv"</span>),</span>
<span id="cb41-6"><a href="#cb41-6" aria-hidden="true" tabindex="-1"></a>            (<span class="ss">f"./results/wordnet/books/</span><span class="sc">{</span>pos<span class="sc">}</span><span class="ss">/top100_defs_with_freq.csv"</span>, <span class="ss">f"./results/wordnet/books/</span><span class="sc">{</span>pos<span class="sc">}</span><span class="ss">/bottom100_defs_with_freq.csv"</span>),</span>
<span id="cb41-7"><a href="#cb41-7" aria-hidden="true" tabindex="-1"></a>            (<span class="ss">f"./results/wordnet/subtitles/</span><span class="sc">{</span>pos<span class="sc">}</span><span class="ss">/top100_defs_with_freq.csv"</span>, <span class="ss">f"./results/wordnet/subtitles/</span><span class="sc">{</span>pos<span class="sc">}</span><span class="ss">/bottom100_defs_with_freq.csv"</span>),</span>
<span id="cb41-8"><a href="#cb41-8" aria-hidden="true" tabindex="-1"></a>        ],    </span>
<span id="cb41-9"><a href="#cb41-9" aria-hidden="true" tabindex="-1"></a>        value_col<span class="op">=</span><span class="st">"len_tokens"</span>,</span>
<span id="cb41-10"><a href="#cb41-10" aria-hidden="true" tabindex="-1"></a>        pair_labels<span class="op">=</span>[<span class="st">"Web"</span>,<span class="st">"Books"</span>,<span class="st">"Subs"</span>],</span>
<span id="cb41-11"><a href="#cb41-11" aria-hidden="true" tabindex="-1"></a>        transform<span class="op">=</span><span class="va">None</span>,</span>
<span id="cb41-12"><a href="#cb41-12" aria-hidden="true" tabindex="-1"></a>        yscale<span class="op">=</span><span class="st">"linear"</span>,</span>
<span id="cb41-13"><a href="#cb41-13" aria-hidden="true" tabindex="-1"></a>        title<span class="op">=</span><span class="ss">f"Definition length, </span><span class="sc">{</span>pos_full<span class="sc">}</span><span class="ss">"</span></span>
<span id="cb41-14"><a href="#cb41-14" aria-hidden="true" tabindex="-1"></a>    )</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="basic_words_project_files/figure-html/cell-15-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="basic_words_project_files/figure-html/cell-15-output-2.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="basic_words_project_files/figure-html/cell-15-output-3.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="basic_words_project_files/figure-html/cell-15-output-4.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>Basic-word definitions for adjectives are consistently longer across all corpora. In Web, the mean difference is +1.95 tokens (5.76 vs 3.81; p&lt;.001; d=0.83), in Books +2.39 (5.72 vs 3.33; p&lt;.001; d=1.08), and in Subtitles +1.64 (5.64 vs 4.00; p&lt;.001; d=0.73). The violins match this pattern: the basic curves are higher with thicker central mass, while non-basic are found lower and have shorter upper tails.</p>
<p>Differences in word definition lengths for basic and non-basic verbs are smaller and corpus-dependent. Web shows a modest increase for basic definitions (+0.80 tokens; 4.32 vs 3.52; p=.0029; d=0.43), Books a medium one (+1.01; 4.37 vs 3.36; p&lt;.001; d=0.54), whereas for Subtitles the difference is not significant (4.09 vs 3.85; p=.39; d=0.12). Correspondingly, the violins for Web/Books show a slight upward shift of the basic distributions (with an overlap nonetheless), but Subtitles exhibits heavy overlap.</p>
<p>For nouns, the situation is different. Web shows no reliable difference (6.08 vs 5.68; p=.369; d=0.13). In Books, non-basic definitions are longer (+1.35; 6.82 vs 5.47; p=.0096; d=-0.37). In Subtitles, the non-basic definitions seem to be longer as well, but p-value of &gt;.05 shows that the difference is not statistically significant (5.26 vs 6.08; p=.098; d=-0.24). The violins echo this: for Books, the non-basic curves have higher centers and fuller upper bodies, whereas for Web and Subtitles they are nearly aligned.</p>
<p>For adverbs, basic definitions are systematically longer in every corpus. For Web, the increase is +1.17 tokens (3.58 vs 2.41; p&lt;.001; d=0.61). For Books, +1.32 (3.59 vs 2.27; p&lt;.001; d=0.73). For Subtitles, +1.20 (3.45 vs 2.25; p&lt;.001; d=0.66). The violins show clear upward shift and a broader middle section for the basic group, with the non-basic group concentrated at shorter lengths.</p>
<p>In general, the results for definition length vary by part of speech: adjectives and adverbs have substantially longer basic-word definitions (medium to large effects), verbs show modest length increases for basic words (not found in Subtitles corpus), and nouns are an exception, with non-basic noun definitions as long or longer (notably in Books corpus).</p>
</section>
</section>
<section id="alternarive-approach-wiktionary-definitions" class="level3" data-number="5.3">
<h3 data-number="5.3" class="anchored" data-anchor-id="alternarive-approach-wiktionary-definitions"><span class="header-section-number">5.3</span> Alternarive approach: Wiktionary definitions</h3>
<p>It is important to note that definitions from Wiktionary, as well as definitions from WordNet, are far from being perfect. For example, less common words are often defined through their synonyms (“creashak” defined as “The bearberry”) or fixed expressions including words of the same root (“talkily” being defined as “In a talky way”), skewing the results.</p>
<section id="web-based-corpus-1" class="level4" data-number="5.3.1">
<h4 data-number="5.3.1" class="anchored" data-anchor-id="web-based-corpus-1"><span class="header-section-number">5.3.1</span> Web-based corpus</h4>
<div id="d291f83c" class="cell" data-execution_count="236">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb42"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb42-1"><a href="#cb42-1" aria-hidden="true" tabindex="-1"></a>captured_output <span class="op">=</span> capture_function_output(full_run, top_n, bottom_n, file_name<span class="op">=</span><span class="st">"./data/unigram_freq_web.csv"</span>, folder<span class="op">=</span><span class="st">"./results/wiktionary/web/all_100"</span>, pos<span class="op">=</span><span class="va">None</span>, source<span class="op">=</span><span class="st">"wiktionary"</span>)</span>
<span id="cb42-2"><a href="#cb42-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-3"><a href="#cb42-3" aria-hidden="true" tabindex="-1"></a>display(HTML(collapsible_content.replace(<span class="st">"</span><span class="sc">{captured_output}</span><span class="st">"</span>, captured_output)))</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stderr">
<pre><code>Lemmatizing: 100%|██████████| 333333/333333 [00:20&lt;00:00, 15926.14it/s]
Getting top frequency words: 196it [01:52,  1.74it/s]
Getting bottom frequency words:   2%|▏         | 6150/315568 [02:04&lt;1:44:36, 49.30it/s]
Obtaining definitions: 100it [00:43,  2.32it/s]
Obtaining definitions: 100it [00:57,  1.75it/s]</code></pre>
</div>
<div class="cell-output cell-output-display">

<details>
<summary>Click to view analysis output</summary>
<pre>
Log-10 mean frequency of tokens in definitions
Basic=-4.0128 (+-0.5068), nonbasic=-4.7576(+-0.7200)

Welch's t-test (unequal variances)
t = 8.429, p = 1.21e-14

Cohen's d = 1.197  (positive ⇒ basic &gt; nonbasic)

Mann-Whitney U (robustness)
U = 8075, p = 1.45e-14

95% CI for (basic - nonbasic) mean difference: [0.5704, 0.9191]


Log-10 median frequency of tokens in definitions
Basic=-3.9380 (+-0.5428), nonbasic=-4.6395(+-0.8524)

Welch's t-test (unequal variances)
t = 6.917, p = 9.54e-11

Cohen's d = 0.983  (positive ⇒ basic &gt; nonbasic)

Mann-Whitney U (robustness)
U = 7374, p = 2.43e-09

95% CI for (basic - nonbasic) mean difference: [0.5013, 0.9017]


Length of definition in tokens
Basic=6.0100 (+-3.9785), nonbasic=4.7600(+-3.8980)

Welch's t-test (unequal variances)
t = 2.244, p = 0.0259

Cohen's d = 0.317  (positive ⇒ basic &gt; nonbasic)

Mann-Whitney U (robustness)
U = 6167, p = 0.00406

95% CI for (basic - nonbasic) mean difference: [0.1516, 2.3484]
</pre>
</details>
</div>
</div>
</section>
<section id="subtitles-based-corpus-1" class="level4" data-number="5.3.2">
<h4 data-number="5.3.2" class="anchored" data-anchor-id="subtitles-based-corpus-1"><span class="header-section-number">5.3.2</span> Subtitles-based corpus</h4>
<div id="05f7f82f" class="cell" data-execution_count="242">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb44"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb44-1"><a href="#cb44-1" aria-hidden="true" tabindex="-1"></a>captured_output <span class="op">=</span> capture_function_output(full_run, top_n, bottom_n, file_name<span class="op">=</span><span class="st">"./data/unigram_freq_subtitles.csv"</span>, folder<span class="op">=</span><span class="st">"./results/wiktionary/subtitles/all_100"</span>, pos<span class="op">=</span><span class="va">None</span>, source<span class="op">=</span><span class="st">"wiktionary"</span>)</span>
<span id="cb44-2"><a href="#cb44-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-3"><a href="#cb44-3" aria-hidden="true" tabindex="-1"></a>display(HTML(collapsible_content.replace(<span class="st">"</span><span class="sc">{captured_output}</span><span class="st">"</span>, captured_output)))</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stderr">
<pre><code>Lemmatizing: 100%|██████████| 30000/30000 [00:07&lt;00:00, 4267.74it/s]
Getting top frequency words: 219it [00:56,  3.85it/s]
Getting bottom frequency words:   1%|          | 228/26052 [00:51&lt;1:36:54,  4.44it/s]
Obtaining definitions: 100it [00:28,  3.45it/s]
Obtaining definitions: 100it [00:43,  2.32it/s]</code></pre>
</div>
<div class="cell-output cell-output-display">

<details>
<summary>Click to view analysis output</summary>
<pre>
Log-10 mean frequency of tokens in definitions
Basic=-4.2638 (+-0.5501), nonbasic=-4.6249(+-0.5456)

Welch's t-test (unequal variances)
t = 4.626, p = 6.77e-06

Cohen's d = 0.659  (positive ⇒ basic &gt; nonbasic)

Mann-Whitney U (robustness)
U = 6795, p = 1.19e-06

95% CI for (basic - nonbasic) mean difference: [0.2072, 0.5151]


Log-10 median frequency of tokens in definitions
Basic=-4.2311 (+-0.6002), nonbasic=-4.6247(+-0.5896)

Welch's t-test (unequal variances)
t = 4.643, p = 6.29e-06

Cohen's d = 0.662  (positive ⇒ basic &gt; nonbasic)

Mann-Whitney U (robustness)
U = 6752, p = 2.03e-06

95% CI for (basic - nonbasic) mean difference: [0.2264, 0.5608]


Length of definition in tokens
Basic=4.8000 (+-3.2287), nonbasic=5.8300(+-4.1635)

Welch's t-test (unequal variances)
t = -1.955, p = 0.0521

Cohen's d = -0.276  (positive ⇒ basic &gt; nonbasic)

Mann-Whitney U (robustness)
U = 4188, p = 0.0461

95% CI for (basic - nonbasic) mean difference: [-2.0694, 0.0094]
</pre>
</details>
</div>
</div>
</section>
<section id="book-based-corpus-1" class="level4" data-number="5.3.3">
<h4 data-number="5.3.3" class="anchored" data-anchor-id="book-based-corpus-1"><span class="header-section-number">5.3.3</span> Book-based corpus</h4>
<div id="639c9628" class="cell" data-execution_count="243">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb46"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb46-1"><a href="#cb46-1" aria-hidden="true" tabindex="-1"></a>captured_output <span class="op">=</span> capture_function_output(full_run, top_n, bottom_n, file_name<span class="op">=</span><span class="st">"./data/unigram_freq_books.txt"</span>, folder<span class="op">=</span><span class="st">"./results/wiktionary/books/all_100"</span>, pos<span class="op">=</span><span class="va">None</span>, source<span class="op">=</span><span class="st">"wiktionary"</span>)</span>
<span id="cb46-2"><a href="#cb46-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-3"><a href="#cb46-3" aria-hidden="true" tabindex="-1"></a>display(HTML(collapsible_content.replace(<span class="st">"</span><span class="sc">{captured_output}</span><span class="st">"</span>, captured_output)))</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stderr">
<pre><code>Lemmatizing: 100%|██████████| 397430/397430 [00:24&lt;00:00, 16515.26it/s]
Getting top frequency words: 215it [00:48,  4.42it/s]
Getting bottom frequency words:   8%|▊         | 27307/360685 [02:33&lt;31:11, 178.15it/s] 
Obtaining definitions: 100it [00:28,  3.47it/s]
Obtaining definitions: 100it [00:47,  2.12it/s]</code></pre>
</div>
<div class="cell-output cell-output-display">

<details>
<summary>Click to view analysis output</summary>
<pre>
Log-10 mean frequency of tokens in definitions
Basic=-3.9472 (+-0.4301), nonbasic=-5.3351(+-1.3476)

Welch's t-test (unequal variances)
t = 9.722, p = 1.05e-16

Cohen's d = 1.393  (positive ⇒ basic &gt; nonbasic)

Mann-Whitney U (robustness)
U = 8656, p = 1.21e-20

95% CI for (basic - nonbasic) mean difference: [1.1052, 1.6707]


Log-10 median frequency of tokens in definitions
Basic=-3.8802 (+-0.4540), nonbasic=-5.2760(+-1.4125)

Welch's t-test (unequal variances)
t = 9.322, p = 8.92e-16

Cohen's d = 1.336  (positive ⇒ basic &gt; nonbasic)

Mann-Whitney U (robustness)
U = 8394, p = 4.43e-18

95% CI for (basic - nonbasic) mean difference: [1.0993, 1.6924]


Length of definition in tokens
Basic=5.9300 (+-4.3514), nonbasic=3.0500(+-2.3884)

Welch's t-test (unequal variances)
t = 5.802, p = 3.62e-08

Cohen's d = 0.821  (positive ⇒ basic &gt; nonbasic)

Mann-Whitney U (robustness)
U = 7418, p = 2.37e-09

95% CI for (basic - nonbasic) mean difference: [1.8994, 3.8606]
</pre>
</details>
</div>
</div>
</section>
<section id="analysis-token-frequencies-in-definitions-2" class="level4" data-number="5.3.4">
<h4 data-number="5.3.4" class="anchored" data-anchor-id="analysis-token-frequencies-in-definitions-2"><span class="header-section-number">5.3.4</span> Analysis: token frequencies in definitions</h4>
<div id="c2a614fd" class="cell" data-execution_count="261">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb48"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb48-1"><a href="#cb48-1" aria-hidden="true" tabindex="-1"></a>plot_violin_pairs(</span>
<span id="cb48-2"><a href="#cb48-2" aria-hidden="true" tabindex="-1"></a>    pairs<span class="op">=</span>[</span>
<span id="cb48-3"><a href="#cb48-3" aria-hidden="true" tabindex="-1"></a>        (<span class="st">"./results/wiktionary/web/all_100/top100_defs_with_freq.csv"</span>, <span class="ss">f"./results/wiktionary/web/all_100/bottom100_defs_with_freq.csv"</span>),</span>
<span id="cb48-4"><a href="#cb48-4" aria-hidden="true" tabindex="-1"></a>        (<span class="st">"./results/wiktionary/books/all_100/top100_defs_with_freq.csv"</span>, <span class="ss">f"./results/wiktionary/books/all_100/bottom100_defs_with_freq.csv"</span>),</span>
<span id="cb48-5"><a href="#cb48-5" aria-hidden="true" tabindex="-1"></a>        (<span class="st">"./results/wiktionary/subtitles/all_100/top100_defs_with_freq.csv"</span>, <span class="ss">f"./results/wiktionary/subtitles/all_100/bottom100_defs_with_freq.csv"</span>),</span>
<span id="cb48-6"><a href="#cb48-6" aria-hidden="true" tabindex="-1"></a>    ],    </span>
<span id="cb48-7"><a href="#cb48-7" aria-hidden="true" tabindex="-1"></a>    value_col<span class="op">=</span><span class="st">"mean_log_freq"</span>,</span>
<span id="cb48-8"><a href="#cb48-8" aria-hidden="true" tabindex="-1"></a>    pair_labels<span class="op">=</span>[<span class="st">"Web"</span>,<span class="st">"Books"</span>,<span class="st">"Subs"</span>],</span>
<span id="cb48-9"><a href="#cb48-9" aria-hidden="true" tabindex="-1"></a>    transform<span class="op">=</span><span class="st">"yes"</span>,</span>
<span id="cb48-10"><a href="#cb48-10" aria-hidden="true" tabindex="-1"></a>    yscale<span class="op">=</span><span class="st">"log"</span>,</span>
<span id="cb48-11"><a href="#cb48-11" aria-hidden="true" tabindex="-1"></a>    title<span class="op">=</span><span class="st">"Token frequencies per million"</span></span>
<span id="cb48-12"><a href="#cb48-12" aria-hidden="true" tabindex="-1"></a>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="basic_words_project_files/figure-html/cell-19-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>As for Wiktionary definition data, definitions of basic words also use higher-frequency vocabulary than those of non-basic words, confirming that more fundamental concepts are explained using simpler and more common vocabulary.</p>
<p>In the Web corpus, the mean log-frequency difference of words used in definitions is 0.74 (−4.01 vs −4.76; p&lt;.001; d=1.20; 95 % CI [0.57, 0.92]), corresponding to roughly 97 vs 17 occurrences per million tokens (~5.7× more frequent wording). For Subtitles, the difference is smaller but still statistically significant: 0.36 (−4.26 vs −4.62; p&lt;.001; d=0.66; 95 % CI [0.21, 0.52]), corresponding to ~55 vs 24 per million, or ~2.3× more frequent vocabulary. In Books, the contrast is most noticeable: 1.39 (−3.95 vs −5.34; p&lt;.001; d=1.39; 95 % CI [1.11, 1.67]), ~113 vs 4.6 per million, or ~25× more frequent language. The effect sizes are large for Web and Books (d&gt;.8) and moderate for Subtitles (.5&gt;d&gt;.2).</p>
<p>The violin plots in Figure visualize the same trend: for every corpus, the basic distributions are shifted upward on the log scale (compared to the non-basic ones), with higher medians and geometric means than their non-basic counterparts. The differences in distribution shape are (again) especially notable for the Books corpus, where non-basic words show a broad, heavy lower tail of rare, specialized vocabulary.</p>
<p>Yet again, the data contradicts the alleged “basic-word paradox”: in fact, definitions of conceptually simple words are not lexically complex but rather rely on frequent, high-utility words typical of defining vocabularies.</p>
</section>
<section id="analysis-length-of-definitions-2" class="level4" data-number="5.3.5">
<h4 data-number="5.3.5" class="anchored" data-anchor-id="analysis-length-of-definitions-2"><span class="header-section-number">5.3.5</span> Analysis: length of definitions</h4>
<div id="9d28499d" class="cell" data-execution_count="262">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb49"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb49-1"><a href="#cb49-1" aria-hidden="true" tabindex="-1"></a>plot_violin_pairs(</span>
<span id="cb49-2"><a href="#cb49-2" aria-hidden="true" tabindex="-1"></a>    pairs<span class="op">=</span>[</span>
<span id="cb49-3"><a href="#cb49-3" aria-hidden="true" tabindex="-1"></a>        (<span class="st">"./results/wiktionary/web/all_100/top100_defs_with_freq.csv"</span>, <span class="ss">f"./results/wiktionary/web/all_100/bottom100_defs_with_freq.csv"</span>),</span>
<span id="cb49-4"><a href="#cb49-4" aria-hidden="true" tabindex="-1"></a>        (<span class="st">"./results/wiktionary/books/all_100/top100_defs_with_freq.csv"</span>, <span class="ss">f"./results/wiktionary/books/all_100/bottom100_defs_with_freq.csv"</span>),</span>
<span id="cb49-5"><a href="#cb49-5" aria-hidden="true" tabindex="-1"></a>        (<span class="st">"./results/wiktionary/subtitles/all_100/top100_defs_with_freq.csv"</span>, <span class="ss">f"./results/wiktionary/subtitles/all_100/bottom100_defs_with_freq.csv"</span>),</span>
<span id="cb49-6"><a href="#cb49-6" aria-hidden="true" tabindex="-1"></a>    ],</span>
<span id="cb49-7"><a href="#cb49-7" aria-hidden="true" tabindex="-1"></a>    value_col<span class="op">=</span><span class="st">"len_tokens"</span>,</span>
<span id="cb49-8"><a href="#cb49-8" aria-hidden="true" tabindex="-1"></a>    pair_labels<span class="op">=</span>[<span class="st">"Web"</span>,<span class="st">"Books"</span>,<span class="st">"Subs"</span>],</span>
<span id="cb49-9"><a href="#cb49-9" aria-hidden="true" tabindex="-1"></a>    transform<span class="op">=</span><span class="va">None</span>,</span>
<span id="cb49-10"><a href="#cb49-10" aria-hidden="true" tabindex="-1"></a>    yscale<span class="op">=</span><span class="st">"linear"</span>,</span>
<span id="cb49-11"><a href="#cb49-11" aria-hidden="true" tabindex="-1"></a>    title<span class="op">=</span><span class="st">"Definition length"</span></span>
<span id="cb49-12"><a href="#cb49-12" aria-hidden="true" tabindex="-1"></a>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="basic_words_project_files/figure-html/cell-20-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>As for definition length, basic words tend to have slightly longer definitions, but the effect varies by domain. In the Web corpus, the mean length is 6.01 vs 4.76 tokens (p=.026; d=0.32; 95 % CI [0.15, 2.35]), suggesting modestly longer and more elaborated phrasing for basic items. The effect is much stronger in Books, where definitions average 5.93 vs 3.05 tokens (p&lt;.001; d=0.82; 95 % CI [1.90, 3.86])—a large difference of nearly two tokens. In Subtitles, the direction reverses slightly (4.80 vs 5.83; p=.052; d=−0.28; 95 % CI [−2.07, 0.01]), but the effect is marginal and small in magnitude. Together, these results indicate that while definitional wording for basic terms is lexically simpler (cf.&nbsp;Section X.X), it can also be somewhat more expansive in length—particularly in written corpora like Books, where definitions may include brief usage clarifications or additional paraphrases.</p>
<p>The violin plots in visualize this pattern. In both Web and Books, the basic distributions are centered slightly higher, with longer medians (blue lines) and broader upper ranges, while non-basic definitions cluster around shorter spans. The Subtitles corpus shows wider overlap and greater dispersion, probably due to its more conversational register (there are fewer technical terms that can be defined using words of the same family in set idiomatic expressions, such as “cylindricalness” being defined as “The state of being cylindrical” in the Books dataset).</p>
<p>Overall, the results suggest that basic terms, though explained through simpler words, often receive slightly loner definitions, probably due to their broader conceptual scope.</p>
</section>
</section>
</section>
<section id="conclusion-and-future-work" class="level2" data-number="6">
<h2 data-number="6" class="anchored" data-anchor-id="conclusion-and-future-work"><span class="header-section-number">6</span> Conclusion and future work</h2>
<p>In this project, I investigated whether basic words – in this case, defined by higher corpus frequency – are easier to define by studying the length and lexical simplicity of their dictionary definitions.</p>
<p>Across both WordNet and Wiktionary, the results for definitional vocabulary frequency show a consistent pattern: definitions of basic words systematically use more frequent, (i.e.&nbsp;basic) words than those of “non-basic” ones. This pattern holds across all corpora and parts of speech, with especially strong effects in the written corpora of Web and Books and more moderate ones in the spoken corpus of Subtitles. <strong>The finding directly contradicts the alleged “Basic-Word Paradox” and supports the view that fundamental concepts are explained using the high-frequency core lexicon.</strong></p>
<p>Differences in definition length were modest and dependent on corpus and part of speech. In WordNet, basic terms were slightly longer in Web and Books but not in Subtitles, with clear length increases limited to adjectives and adverbs, modest for verbs, and reversed for nouns. In Wiktionary, definitions of basic words were also significantly longer in Web and Books but not in Subtitles, where the difference was small and nonsignificant. <strong>Overall, definition length shows no uniform trend: while basic words tend to have marginally longer definitions in written corpora, the effect is weak and variable.</strong></p>
<p>Then I can answer the suggested research question as follows: <strong>Frequency-defined “basic” words are easier to define lexically, as their definitions consistently rely on higher-frequency, more common vocabulary across corpora and parts of speech. However, this simplicity in wording is not directly related to shorter definitions – their length varies across datasets and grammatical categories, with definitions for basic words being unexpectedly longer in many cases.</strong></p>
<p>In the future, my analysis could be expanded on by: 1. Using a larger and more balanced word sample; 2. Including additional lexical resources such as learner dictionaries to test consistency across sources; 3. Taking into account potential polysemanticity of some words (limiting the analysis to the first definition, as I did, seems to be an oversimplification); 4. Examining dictionary data in languages other than English to determine whether the relationship between frequency and definitional simplicity is universally or language-specific.</p>
</section>
<section id="references" class="level2" data-number="7">
<h2 data-number="7" class="anchored" data-anchor-id="references"><span class="header-section-number">7</span> References</h2>
<p>Brysbaert, M., Warriner, A. B., &amp; Kuperman, V. (2014). Concreteness ratings for 40,000 English words. Behavior Research Methods, 46 (3), 904–911. https://link.springer.com/article/10.3758/s13428-013-0403-5</p>
<p>Kamiński, M. P. (2021). Defining with simple vocabulary in English dictionaries. Lexicography Studies. https://academic.oup.com/ijl/article/36/2/227/7114903</p>
<p>Mandera, P., Keuleers, E., &amp; Brysbaert, M. (2017). Explaining human performance in psycholinguistic tasks with models of semantic similarity based on prediction and counting: A review and empirical validation. Journal of Memory and Language, 92, 57–78. https://www.sciencedirect.com/science/article/pii/S0749596X16300079</p>
<p>Rosch, E., Mervis, C. B., Gray, W. D., Johnson, D. M., &amp; Boyes-Braem, P. (1976). Basic objects in natural categories. Cognitive Psychology, 8 (3), 382–439. https://www.sciencedirect.com/science/article/pii/001002857690013X</p>
<p>Steyvers, M., &amp; Tenenbaum, J. B. (2005). The large-scale structure of semantic networks: Statistical analyses and a model of semantic growth. Cognitive Science, 29 (1), 41–78. https://pubmed.ncbi.nlm.nih.gov/21702767/</p>
<p>Swadesh, M. (1952). Lexico-statistic dating of prehistoric ethnic contacts: with special reference to North American Indians and Eskimos. Proceedings of the American Philosophical Society, 96 (4), 452–463. https://www.jstor.org/stable/3143802</p>
</section>

</main>
<!-- /main column -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
      const outerScaffold = trigger.parentElement.cloneNode(true);
      const codeEl = outerScaffold.querySelector('code');
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
</div> <!-- /content -->




</body></html>